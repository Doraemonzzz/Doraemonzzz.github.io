<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Deep Learning Systems HW0 | Doraemonzzz</title><meta name="keywords" content="深度学习系统"><meta name="author" content="Doraemonzzz"><meta name="copyright" content="Doraemonzzz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="这里回顾HW0，主要是熟悉代码和复现一些基本的反传。 课程主页：  https:&#x2F;&#x2F;dlsyscourse.org&#x2F; https:&#x2F;&#x2F;forum.dlsyscourse.org&#x2F; https:&#x2F;&#x2F;mugrade-online.dlsyscourse.org&#x2F;  参考资料：  https:&#x2F;&#x2F;github.com&#x2F;hsjeong5&#x2F;MNIST-for-Numpy&#x2F;blob&#x2F;master&#x2F;mnist.">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning Systems HW0">
<meta property="og:url" content="http://www.doraemonzzz.com/2022/10/17/2022-10-17-Deep-Learning-Systems-HW0/index.html">
<meta property="og:site_name" content="Doraemonzzz">
<meta property="og:description" content="这里回顾HW0，主要是熟悉代码和复现一些基本的反传。 课程主页：  https:&#x2F;&#x2F;dlsyscourse.org&#x2F; https:&#x2F;&#x2F;forum.dlsyscourse.org&#x2F; https:&#x2F;&#x2F;mugrade-online.dlsyscourse.org&#x2F;  参考资料：  https:&#x2F;&#x2F;github.com&#x2F;hsjeong5&#x2F;MNIST-for-Numpy&#x2F;blob&#x2F;master&#x2F;mnist.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2022-10-17T14:19:00.000Z">
<meta property="article:modified_time" content="2022-10-17T15:15:10.960Z">
<meta property="article:author" content="Doraemonzzz">
<meta property="article:tag" content="深度学习系统">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true"><link rel="canonical" href="http://www.doraemonzzz.com/2022/10/17/2022-10-17-Deep-Learning-Systems-HW0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6f00f37f957f0608abb8c571105456f0";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-G-RE4B1LKRZD"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-G-RE4B1LKRZD');
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"距离上次更新已经","messageNext":"天了，文章内容可能已经过时。"},
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Deep Learning Systems HW0',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-10-17 23:15:10'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/bilibili.css" media="defer" onload="this.media='all'"><meta name="google-site-verification" content="c4v-NmuUZRgl3cvtg9GKswryK1YLaPztd_5M-df5VNI" /><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Doraemonzzz" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src= "/img/loading.gif" data-lazy-src="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">822</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">81</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">34</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-chart-pie"></i><span> 博客统计</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/charts/"><i class="fa-fw far fa-chart-bar"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-area"></i><span> 访问统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/top/"><i class="fa-fw fab fa-hotjar"></i><span> 阅读排行榜</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Doraemonzzz</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-chart-pie"></i><span> 博客统计</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/charts/"><i class="fa-fw far fa-chart-bar"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-area"></i><span> 访问统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/top/"><i class="fa-fw fab fa-hotjar"></i><span> 阅读排行榜</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Deep Learning Systems HW0</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-10-17T14:19:00.000Z" title="发表于 2022-10-17 22:19:00">2022-10-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-10-17T15:15:10.960Z" title="更新于 2022-10-17 23:15:10">2022-10-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/">深度学习系统</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/2022/10/17/2022-10-17-Deep-Learning-Systems-HW0/" data-flag-title="Deep Learning Systems HW0"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span class="leancloud-visitors-count"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2022/10/17/2022-10-17-Deep-Learning-Systems-HW0/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2022/10/17/2022-10-17-Deep-Learning-Systems-HW0/" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>这里回顾HW0，主要是熟悉代码和复现一些基本的反传。</p>
<p>课程主页：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://dlsyscourse.org/">https://dlsyscourse.org/</a></li>
<li><a target="_blank" rel="noopener" href="https://forum.dlsyscourse.org/">https://forum.dlsyscourse.org/</a></li>
<li><a target="_blank" rel="noopener" href="https://mugrade-online.dlsyscourse.org/">https://mugrade-online.dlsyscourse.org/</a></li>
</ul>
<p>参考资料：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/hsjeong5/MNIST-for-Numpy/blob/master/mnist.py">https://github.com/hsjeong5/MNIST-for-Numpy/blob/master/mnist.py</a></li>
<li><a target="_blank" rel="noopener" href="https://mattpetersen.github.io/load-mnist-with-numpy">https://mattpetersen.github.io/load-mnist-with-numpy</a></li>
</ul>
<span id="more"></span>
<h2 id="Question-1-A-basic-add-function-and-testing-autograding-basics"><a href="#Question-1-A-basic-add-function-and-testing-autograding-basics" class="headerlink" title="Question 1: A basic add function, and testing/autograding basics"></a>Question 1: A basic <code>add</code> function, and testing/autograding basics</h2><p>这部分没有难度，主要是熟悉代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">add</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" A trivial 'add' function you should implement to get used to the
    autograder and submission system.  The solution to this problem is in the
    the homework notebook.

    Args:
        x (Python number or numpy array)
        y (Python number or numpy array)

    Return:
        Sum of x + y
    """</span>
    <span class="token comment">### BEGIN YOUR CODE</span>
    <span class="token keyword">return</span> x <span class="token operator">+</span> y
    <span class="token comment">### END YOUR CODE</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>Question 2: Loading MNIST data</p>
<p>根据参考资料，可以给出如下代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">parse_mnist</span><span class="token punctuation">(</span>image_filename<span class="token punctuation">,</span> label_filename<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Read an images and labels file in MNIST format.  See this page:
    http://yann.lecun.com/exdb/mnist/ for a description of the file format.

    Args:
        image_filename (str): name of gzipped images file in MNIST format
        label_filename (str): name of gzipped labels file in MNIST format

    Returns:
        Tuple (X,y):
            X (numpy.ndarray[np.float32]): 2D numpy array containing the loaded 
                data.  The dimensionality of the data should be 
                (num_examples x input_dim) where 'input_dim' is the full 
                dimension of the data, e.g., since MNIST images are 28x28, it 
                will be 784.  Values should be of type np.float32, and the data 
                should be normalized to have a minimum value of 0.0 and a 
                maximum value of 1.0. The normalization should be applied uniformly
                across the whole dataset, _not_ individual images.

            y (numpy.ndarray[dtype=np.uint8]): 1D numpy array containing the
                labels of the examples.  Values should be of type np.uint8 and
                for MNIST will contain the values 0-9.
    """</span>
    <span class="token comment">### BEGIN YOUR CODE</span>
    <span class="token keyword">with</span> gzip<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>image_filename<span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        X <span class="token operator">=</span> np<span class="token punctuation">.</span>frombuffer<span class="token punctuation">(</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>uint8<span class="token punctuation">,</span> offset<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255</span>
    <span class="token keyword">with</span> gzip<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>label_filename<span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        y <span class="token operator">=</span> np<span class="token punctuation">.</span>frombuffer<span class="token punctuation">(</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>uint8<span class="token punctuation">,</span> offset<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> X<span class="token punctuation">,</span> y
    <span class="token comment">### END YOUR CODE</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Question-3-Softmax-loss"><a href="#Question-3-Softmax-loss" class="headerlink" title="Question 3: Softmax loss"></a>Question 3: Softmax loss</h2><p>根据如下公式实现即可：</p>
<script type="math/tex; mode=display">
\ell_{\mathrm{softmax}}(z, y) = \log\sum_{i=1}^k \exp z_i - z_y.</script><p>代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">softmax_loss</span><span class="token punctuation">(</span>Z<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Return softmax loss.  Note that for the purposes of this assignment,
    you don't need to worry about "nicely" scaling the numerical properties
    of the log-sum-exp computation, but can just compute this directly.

    Args:
        Z (np.ndarray[np.float32]): 2D numpy array of shape
            (batch_size, num_classes), containing the logit predictions for
            each class.
        y (np.ndarray[np.int8]): 1D numpy array of shape (batch_size, )
            containing the true label of each example.

    Returns:
        Average softmax loss over the sample.
    """</span>
    <span class="token comment">### BEGIN YOUR CODE</span>
    exp_sum_z <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>Z<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    b <span class="token operator">=</span> Z<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    z_y <span class="token operator">=</span> Z<span class="token punctuation">[</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">]</span>
    loss <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>exp_sum_z<span class="token punctuation">)</span> <span class="token operator">-</span> z_y<span class="token punctuation">)</span>

    <span class="token keyword">return</span> loss
    <span class="token comment">### END YOUR CODE</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Question-4-Stochastic-gradient-descent-for-softmax-regression"><a href="#Question-4-Stochastic-gradient-descent-for-softmax-regression" class="headerlink" title="Question 4: Stochastic gradient descent for softmax regression"></a>Question 4: Stochastic gradient descent for softmax regression</h2><p>根据如下公式计算梯度即可：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_\Theta \ell_{\mathrm{softmax}}(X \Theta, y) &= \frac{1}{m} X^T (Z - I_y)\\
Z &= \mathrm{normalize}(\exp(X \Theta)) \quad \mbox{(normalization applied row-wise)}
\end{aligned}</script><p>代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x1 <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

    <span class="token keyword">return</span> x1 <span class="token operator">/</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>x1<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">softmax_regression_epoch</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> theta<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">,</span> batch<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Run a single epoch of SGD for softmax regression on the data, using
    the step size lr and specified batch size.  This function should modify the
    theta matrix in place, and you should iterate through batches in X _without_
    randomizing the order.

    Args:
        X (np.ndarray[np.float32]): 2D input array of size
            (num_examples x input_dim).
        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)
        theta (np.ndarrray[np.float32]): 2D array of softmax regression
            parameters, of shape (input_dim, num_classes)
        lr (float): step size (learning rate) for SGD
        batch (int): size of SGD minibatch

    Returns:
        None
    """</span>
    <span class="token comment">### BEGIN YOUR CODE</span>
    n <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    step <span class="token operator">=</span> n <span class="token operator">//</span> batch
    index <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>step <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        start <span class="token operator">=</span> i <span class="token operator">*</span> batch
        end <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>start <span class="token operator">+</span> batch<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
        <span class="token keyword">if</span> start <span class="token operator">==</span> end<span class="token punctuation">:</span>
            <span class="token keyword">break</span>
        x1 <span class="token operator">=</span> X<span class="token punctuation">[</span>start<span class="token punctuation">:</span> end<span class="token punctuation">]</span>
        y1 <span class="token operator">=</span> y<span class="token punctuation">[</span>start<span class="token punctuation">:</span> end<span class="token punctuation">]</span>
        z <span class="token operator">=</span> softmax<span class="token punctuation">(</span>np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x1<span class="token punctuation">,</span> theta<span class="token punctuation">)</span><span class="token punctuation">)</span>
        z<span class="token punctuation">[</span>index<span class="token punctuation">,</span> y1<span class="token punctuation">]</span> <span class="token operator">-=</span> <span class="token number">1</span>
        grad <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x1<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> z<span class="token punctuation">)</span> <span class="token operator">/</span> batch
        theta <span class="token operator">-=</span> lr <span class="token operator">*</span> grad
    <span class="token comment">### END YOUR CODE</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Question-5-SGD-for-a-two-layer-neural-network"><a href="#Question-5-SGD-for-a-two-layer-neural-network" class="headerlink" title="Question 5: SGD for a two-layer neural network"></a>Question 5: SGD for a two-layer neural network</h2><p>我们需要求解的问题为：</p>
<script type="math/tex; mode=display">
\mathrm{minimize}_{W_1, W_2}\ell_{\mathrm{softmax}}(\mathrm{ReLU}(X W_1) W_2, y)</script><p>记：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Z_1 \in \mathbb{R}^{m \times d} & = \mathrm{ReLU}(X W_1) \\
G_2 \in \mathbb{R}^{m \times k} & = \mathrm{normalize}(\exp(Z_1 W_2)) - I_y \\
G_1 \in \mathbb{R}^{m \times d} & = \mathrm{1}\{Z_1 > 0\} \circ (G_2 W_2^T)
\end{aligned}</script><p>梯度为：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\nabla_{W_1} \ell_{\mathrm{softmax}}(\mathrm{ReLU}(X W_1) W_2, y) & = \frac{1}{m} X^T G_1  \\
\nabla_{W_2} \ell_{\mathrm{softmax}}(\mathrm{ReLU}(X W_1) W_2, y) & = \frac{1}{m} Z_1^T G_2
\end{aligned}
\end{equation}</script><p>代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">nn_epoch</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">,</span> batch<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Run a single epoch of SGD for a two-layer neural network defined by the
    weights W1 and W2 (with no bias terms):
        logits = ReLU(X * W1) * W2
    The function should use the step size lr, and the specified batch size (and
    again, without randomizing the order of X).  It should modify the
    W1 and W2 matrices in place.

    Args:
        X (np.ndarray[np.float32]): 2D input array of size
            (num_examples x input_dim).
        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)
        W1 (np.ndarray[np.float32]): 2D array of first layer weights, of shape
            (input_dim, hidden_dim)
        W2 (np.ndarray[np.float32]): 2D array of second layer weights, of shape
            (hidden_dim, num_classes)
        lr (float): step size (learning rate) for SGD
        batch (int): size of SGD minibatch

    Returns:
        None
    """</span>
    <span class="token comment">### BEGIN YOUR CODE</span>
    n <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    step <span class="token operator">=</span> n <span class="token operator">//</span> batch
    index <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>step <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        start <span class="token operator">=</span> i <span class="token operator">*</span> batch
        end <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>start <span class="token operator">+</span> batch<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
        <span class="token keyword">if</span> start <span class="token operator">==</span> end<span class="token punctuation">:</span>
            <span class="token keyword">break</span>
        x1 <span class="token operator">=</span> X<span class="token punctuation">[</span>start<span class="token punctuation">:</span> end<span class="token punctuation">]</span>
        y1 <span class="token operator">=</span> y<span class="token punctuation">[</span>start<span class="token punctuation">:</span> end<span class="token punctuation">]</span>
        <span class="token comment"># ReLU</span>
        Z1 <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x1<span class="token punctuation">,</span> W1<span class="token punctuation">)</span>
        Z1<span class="token punctuation">[</span>Z1 <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token comment"># G2</span>
        G2 <span class="token operator">=</span> softmax<span class="token punctuation">(</span>np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Z1<span class="token punctuation">,</span> W2<span class="token punctuation">)</span><span class="token punctuation">)</span>
        G2<span class="token punctuation">[</span>index<span class="token punctuation">,</span> y1<span class="token punctuation">]</span> <span class="token operator">-=</span> <span class="token number">1</span>
        <span class="token comment"># G1</span>
        G1 <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>G2<span class="token punctuation">,</span> W2<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        G1<span class="token punctuation">[</span>Z1 <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token comment"># grad</span>
        W1_grad <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x1<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> G1<span class="token punctuation">)</span> <span class="token operator">/</span> batch
        W2_grad <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Z1<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> G2<span class="token punctuation">)</span> <span class="token operator">/</span> batch
        W1 <span class="token operator">-=</span> lr <span class="token operator">*</span> W1_grad
        W2 <span class="token operator">-=</span> lr <span class="token operator">*</span> W2_grad
    <span class="token comment">### END YOUR CODE</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Question-6-Softmax-regression-in-C"><a href="#Question-6-Softmax-regression-in-C" class="headerlink" title="Question 6: Softmax regression in C++"></a>Question 6: Softmax regression in C++</h2><p>利用C++实现反传，主要难点是这里矩阵使用一维数组表示：</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">float* matmul(float *x, float *y, int m, int d, int n) &#123;
    float *z &#x3D; new float[m * n];
    for (int i &#x3D; 0; i &lt; m; i++) &#123;
        for (int j &#x3D; 0; j &lt; n; j++) &#123;
            int ij &#x3D; i * n + j;
            z[ij] &#x3D; 0;
            for (int k &#x3D; 0; k &lt; d; k++) &#123;
                int ik &#x3D; i * d + k;
                int kj &#x3D; k * n + j;
                z[ij] +&#x3D; x[ik] * y[kj];
            &#125;
        &#125;
    &#125;

    return z;
&#125;

float* slice(const float *x, int start_row, int end_row, int d) &#123;
    int n &#x3D; (end_row - start_row) * d;
    int start &#x3D; start_row * d;
    float* z &#x3D; new float[n];
    for (int i &#x3D; 0; i &lt; n; i++) &#123;
        z[i] &#x3D; x[start + i];
    &#125;

    return z;
&#125;

unsigned char* slice(const unsigned char *x, int start_row, int end_row, int d) &#123;
    int n &#x3D; (end_row - start_row) * d;
    int start &#x3D; start_row * d;
    unsigned char* z &#x3D; new unsigned char[n];
    for (int i &#x3D; 0; i &lt; n; i++) &#123;
        z[i] &#x3D; x[start + i];
    &#125;

    return z;
&#125;

float* softmax(float *x, int m, int n) &#123;
    float* res &#x3D; new float[m * n];
    for (int i &#x3D; 0; i &lt; m; i++) &#123;
        float s &#x3D; 0;
        for (int j &#x3D; 0; j &lt; n; j++) &#123;
            int index &#x3D; i * n + j;
            res[index] &#x3D; exp(x[index]);
            s +&#x3D; res[index];
        &#125;
        for (int j &#x3D; 0; j &lt; n; j++) &#123;
            int index &#x3D; i * n + j;
            res[index] &#x2F;&#x3D; s;
        &#125;
    &#125;

    return res;
&#125;

float* transpose(float *x, int m, int n) &#123;
    float *y &#x3D; new float[m * n];
    for (int i &#x3D; 0; i &lt; m; i++) &#123;
        for (int j &#x3D; 0; j &lt; n; j++) &#123;
            y[j * m + i] &#x3D; x[i * n + j];
        &#125;
    &#125;

    return y;
&#125;

void minus(float *x, float *y, int m, int n) &#123;
    for (int i &#x3D; 0; i &lt; m; i++) &#123;
        for (int j &#x3D; 0; j &lt; n; j++) &#123;
            int index &#x3D; i * n + j;
            x[index] -&#x3D; y[index];
        &#125;
    &#125;
&#125;

void mul(float *x, float a, int m, int n) &#123;
    for (int i &#x3D; 0; i &lt; m; i++) &#123;
        for (int j &#x3D; 0; j &lt; n; j++) &#123;
            int index &#x3D; i * n + j;
            x[index] *&#x3D; a;
        &#125;
    &#125;
&#125;

void softmax_regression_epoch_cpp(const float *X, const unsigned char *y,
								  float *theta, size_t m, size_t n, size_t k,
								  float lr, size_t batch)
&#123;
    &#x2F;**
     * A C++ version of the softmax regression epoch code.  This should run a
     * single epoch over the data defined by X and y (and sizes m,n,k), and
     * modify theta in place.  Your function will probably want to allocate
     * (and then delete) some helper arrays to store the logits and gradients.
     *
     * Args:
     *     X (const float *): pointer to X data, of size m*n, stored in row
     *          major (C) format
     *     y (const unsigned char *): pointer to y data, of size m
     *     theta (float *): pointer to theta data, of size n*k, stored in row
     *          major (C) format
     *     m (size_t): number of examples
     *     n (size_t): input dimension
     *     k (size_t): number of classes
     *     lr (float): learning rate &#x2F; SGD step size
     *     batch (int): SGD minibatch size
     *
     * Returns:
     *     (None)
     *&#x2F;

    &#x2F;&#x2F;&#x2F; BEGIN YOUR CODE
    int step &#x3D; m &#x2F; batch;
    &#x2F;&#x2F; int e &#x3D; 1;
    for (int i &#x3D; 0; i &lt; step + 1; i++) &#123;
        int start &#x3D; i * batch;
        int end &#x3D; std::min(start + batch, m);
        if (start &#x3D;&#x3D; end) &#123;
            break;
        &#125;
        &#x2F;&#x2F; row number of x1, y1
        int l &#x3D; end - start;
        &#x2F;&#x2F; shape: l, n
        float *x1 &#x3D; slice(X, start, end, n);
        &#x2F;&#x2F; shape: l, 1
        unsigned char *y1 &#x3D; slice(y, start, end, 1);
        &#x2F;&#x2F; shape: l, k
        float *score &#x3D; matmul(x1, theta, l, n, k);
        &#x2F;&#x2F; shape: l, k
        float *z &#x3D; softmax(score, l, k);
        &#x2F;&#x2F; z - Iy, shape: l, k
        for (int i &#x3D; 0; i &lt; l; i++) &#123;
            int j &#x3D; y1[i];
            int index &#x3D; i * k + j;
            z[index] -&#x3D; 1;
        &#125;
        &#x2F;&#x2F; grad
        &#x2F;&#x2F; shape: n, l
        float *x1_transpose &#x3D; transpose(x1, l, n);
        &#x2F;&#x2F; shape: n, k
        float *grad &#x3D; matmul(x1_transpose, z, n, l, k);
        &#x2F;&#x2F; update
        mul(grad, 1.0 * lr &#x2F; batch, n, k);
        &#x2F;&#x2F; shape: n, k
        minus(theta, grad, n, k);
    &#125;
    &#x2F;&#x2F;&#x2F; END YOUR CODE
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Doraemonzzz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://www.doraemonzzz.com/2022/10/17/2022-10-17-Deep-Learning-Systems-HW0/">http://www.doraemonzzz.com/2022/10/17/2022-10-17-Deep-Learning-Systems-HW0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.doraemonzzz.com" target="_blank">Doraemonzzz</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/">深度学习系统</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/10/17/2022-10-17-Deep-Learning-Systems-HW1/"><img class="prev-cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Deep Learning Systems HW1</div></div></a></div><div class="next-post pull-right"><a href="/2022/10/16/2022-10-16-Deep-Learning-Systems-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"><img class="next-cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Deep Learning Systems 开发环境配置</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/10/16/2022-10-16-Deep-Learning-Systems-开发环境配置/" title="Deep Learning Systems 开发环境配置"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-16</div><div class="title">Deep Learning Systems 开发环境配置</div></div></a></div><div><a href="/2023/01/15/2023-1-15-Deep-Learning-Systems-Lecture-1-Introduction-and-Logistics/" title="Deep Learning Systems Lecture 1 Introduction and Logistics"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-15</div><div class="title">Deep Learning Systems Lecture 1 Introduction and Logistics</div></div></a></div><div><a href="/2023/01/18/2023-1-18-Deep-Learning-Systems-Lecture-3-Manual-Neural-Networks-and-Backprop/" title="Deep Learning Systems Lecture 3 Manual Neural Networks and Backprop"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-18</div><div class="title">Deep Learning Systems Lecture 3 Manual Neural Networks and Backprop</div></div></a></div><div><a href="/2023/01/16/2023-1-16-Deep-Learning-Systems-Lecture-2-ML-Refresher-and-Softmax-Regression/" title="Deep Learning Systems Lecture 2 ML Refresher and Softmax Regression"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-16</div><div class="title">Deep Learning Systems Lecture 2 ML Refresher and Softmax Regression</div></div></a></div><div><a href="/2023/01/20/2023-1-20-Deep-Learning-Systems-Lecture-4-Automatic-Differentiation/" title="Deep Learning Systems Lecture 4 Automatic Differentiation"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-20</div><div class="title">Deep Learning Systems Lecture 4 Automatic Differentiation</div></div></a></div><div><a href="/2023/01/26/2023-1-26-Deep-Learning-Systems-Lecture-6-Fully-connected-networks-and-optimization/" title="Deep Learning Systems Lecture 6 Fully connected networks and optimization"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-26</div><div class="title">Deep Learning Systems Lecture 6 Fully connected networks and optimization</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Livere</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNDcxOS8xMTI1Ng=="></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src= "/img/loading.gif" data-lazy-src="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Doraemonzzz</div><div class="author-info__description">个人博客，主要记录有关机器学习，数学以及计算机科学的笔记</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">822</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">81</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">34</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Doraemonzzz"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Doraemonzzz" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/doraemon_zzz@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://space.bilibili.com/291079982" target="_blank" title="Bilibili"><i class="iconfont icon-bilibili"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title=""><i class="fa fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">暂无公告</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Question-1-A-basic-add-function-and-testing-autograding-basics"><span class="toc-number">1.</span> <span class="toc-text">Question 1: A basic add function, and testing&#x2F;autograding basics</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Question-3-Softmax-loss"><span class="toc-number">2.</span> <span class="toc-text">Question 3: Softmax loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Question-4-Stochastic-gradient-descent-for-softmax-regression"><span class="toc-number">3.</span> <span class="toc-text">Question 4: Stochastic gradient descent for softmax regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Question-5-SGD-for-a-two-layer-neural-network"><span class="toc-number">4.</span> <span class="toc-text">Question 5: SGD for a two-layer neural network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Question-6-Softmax-regression-in-C"><span class="toc-number">5.</span> <span class="toc-text">Question 6: Softmax regression in C++</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/11/18/2024-11-18-Xmixers(1)-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%AF%B9%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BD%B1%E5%93%8D/" title="Xmixers(1) 初始化对于模型的影响"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Xmixers(1) 初始化对于模型的影响"/></a><div class="content"><a class="title" href="/2024/11/18/2024-11-18-Xmixers(1)-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%AF%B9%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BD%B1%E5%93%8D/" title="Xmixers(1) 初始化对于模型的影响">Xmixers(1) 初始化对于模型的影响</a><time datetime="2024-11-18T04:24:00.000Z" title="发表于 2024-11-18 12:24:00">2024-11-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/18/2024-11-18-Xmixers(0)-Introduction-to-the-Xmixers-Project/" title="Xmixers(0) Introduction to the Xmixers Project"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Xmixers(0) Introduction to the Xmixers Project"/></a><div class="content"><a class="title" href="/2024/11/18/2024-11-18-Xmixers(0)-Introduction-to-the-Xmixers-Project/" title="Xmixers(0) Introduction to the Xmixers Project">Xmixers(0) Introduction to the Xmixers Project</a><time datetime="2024-11-18T03:20:00.000Z" title="发表于 2024-11-18 11:20:00">2024-11-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/18/2024-11-18-Xmixers(0)-xmixers%E9%A1%B9%E7%9B%AE%E7%AE%80%E4%BB%8B/" title="Xmixers(0) xmixers项目简介"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Xmixers(0) xmixers项目简介"/></a><div class="content"><a class="title" href="/2024/11/18/2024-11-18-Xmixers(0)-xmixers%E9%A1%B9%E7%9B%AE%E7%AE%80%E4%BB%8B/" title="Xmixers(0) xmixers项目简介">Xmixers(0) xmixers项目简介</a><time datetime="2024-11-18T03:19:00.000Z" title="发表于 2024-11-18 11:19:00">2024-11-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/30/2023-12-30-23%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/" title="23年终总结"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="23年终总结"/></a><div class="content"><a class="title" href="/2023/12/30/2023-12-30-23%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/" title="23年终总结">23年终总结</a><time datetime="2023-12-30T15:18:00.000Z" title="发表于 2023-12-30 23:18:00">2023-12-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/20/2023-2-20-Deep-Learning-Systems-Lecture-24-Model-Deployment/" title="Deep Learning Systems Lecture 24 Model Deployment"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Deep Learning Systems Lecture 24 Model Deployment"/></a><div class="content"><a class="title" href="/2023/02/20/2023-2-20-Deep-Learning-Systems-Lecture-24-Model-Deployment/" title="Deep Learning Systems Lecture 24 Model Deployment">Deep Learning Systems Lecture 24 Model Deployment</a><time datetime="2023-02-20T07:30:00.000Z" title="发表于 2023-02-20 15:30:00">2023-02-20</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2024 By Doraemonzzz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.25
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'IpnmxCW9CvYWIXbol5QXsegX-MdYXbMMI',
      appKey: 'w57DVCdbxcyB1TYYagMIMJIU',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Valine' === 'Livere' || !false) {
  if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>