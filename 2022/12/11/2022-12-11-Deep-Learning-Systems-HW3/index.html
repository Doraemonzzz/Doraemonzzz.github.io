<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Deep Learning Systems HW3 | Doraemonzzz</title><meta name="keywords" content="深度学习系统"><meta name="author" content="Doraemonzzz"><meta name="copyright" content="Doraemonzzz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="这里回顾HW3，这次的主要内容是实现一个简单的线性代数库。 课程主页：  https:&#x2F;&#x2F;dlsyscourse.org&#x2F; https:&#x2F;&#x2F;forum.dlsyscourse.org&#x2F; https:&#x2F;&#x2F;mugrade-online.dlsyscourse.org&#x2F;  参考资料：  https:&#x2F;&#x2F;github.com&#x2F;YuanchengFang&#x2F;dlsys_solution&#x2F;blob&#x2F;master">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning Systems HW3">
<meta property="og:url" content="http://www.doraemonzzz.com/2022/12/11/2022-12-11-Deep-Learning-Systems-HW3/index.html">
<meta property="og:site_name" content="Doraemonzzz">
<meta property="og:description" content="这里回顾HW3，这次的主要内容是实现一个简单的线性代数库。 课程主页：  https:&#x2F;&#x2F;dlsyscourse.org&#x2F; https:&#x2F;&#x2F;forum.dlsyscourse.org&#x2F; https:&#x2F;&#x2F;mugrade-online.dlsyscourse.org&#x2F;  参考资料：  https:&#x2F;&#x2F;github.com&#x2F;YuanchengFang&#x2F;dlsys_solution&#x2F;blob&#x2F;master">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2022-12-11T04:41:00.000Z">
<meta property="article:modified_time" content="2022-12-14T01:31:01.631Z">
<meta property="article:author" content="Doraemonzzz">
<meta property="article:tag" content="深度学习系统">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true"><link rel="canonical" href="http://www.doraemonzzz.com/2022/12/11/2022-12-11-Deep-Learning-Systems-HW3/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6f00f37f957f0608abb8c571105456f0";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-G-RE4B1LKRZD"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-G-RE4B1LKRZD');
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"距离上次更新已经","messageNext":"天了，文章内容可能已经过时。"},
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Deep Learning Systems HW3',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-12-14 09:31:01'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/bilibili.css" media="defer" onload="this.media='all'"><meta name="google-site-verification" content="c4v-NmuUZRgl3cvtg9GKswryK1YLaPztd_5M-df5VNI" /><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Doraemonzzz" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src= "/img/loading.gif" data-lazy-src="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">786</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">79</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">32</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-chart-pie"></i><span> 博客统计</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/charts/"><i class="fa-fw far fa-chart-bar"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-area"></i><span> 访问统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/top/"><i class="fa-fw fab fa-hotjar"></i><span> 阅读排行榜</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Doraemonzzz</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-chart-pie"></i><span> 博客统计</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/charts/"><i class="fa-fw far fa-chart-bar"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-area"></i><span> 访问统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/top/"><i class="fa-fw fab fa-hotjar"></i><span> 阅读排行榜</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Deep Learning Systems HW3</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-11T04:41:00.000Z" title="发表于 2022-12-11 12:41:00">2022-12-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-14T01:31:01.631Z" title="更新于 2022-12-14 09:31:01">2022-12-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/">深度学习系统</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>36分钟</span></span><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/2022/12/11/2022-12-11-Deep-Learning-Systems-HW3/" data-flag-title="Deep Learning Systems HW3"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span class="leancloud-visitors-count"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2022/12/11/2022-12-11-Deep-Learning-Systems-HW3/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2022/12/11/2022-12-11-Deep-Learning-Systems-HW3/" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>这里回顾HW3，这次的主要内容是实现一个简单的线性代数库。</p>
<p>课程主页：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://dlsyscourse.org/">https://dlsyscourse.org/</a></li>
<li><a target="_blank" rel="noopener" href="https://forum.dlsyscourse.org/">https://forum.dlsyscourse.org/</a></li>
<li><a target="_blank" rel="noopener" href="https://mugrade-online.dlsyscourse.org/">https://mugrade-online.dlsyscourse.org/</a></li>
</ul>
<p>参考资料：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/YuanchengFang/dlsys_solution/blob/master/hw3/src/ndarray_backend_cpu.cc">https://github.com/YuanchengFang/dlsys_solution/blob/master/hw3/src/ndarray_backend_cpu.cc</a></li>
<li><a target="_blank" rel="noopener" href="https://www.runoob.com/cplusplus/cpp-templates.html">https://www.runoob.com/cplusplus/cpp-templates.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/2f67a83da503">https://www.jianshu.com/p/2f67a83da503</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/zh-cn/cpp/cpp/lambda-expressions-in-cpp?view=msvc-170">https://learn.microsoft.com/zh-cn/cpp/cpp/lambda-expressions-in-cpp?view=msvc-170</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/db079eeb4b15">https://www.jianshu.com/p/db079eeb4b15</a></li>
<li><a target="_blank" rel="noopener" href="https://forum.dlsyscourse.org/t/q6-setitem-in-gpu-version/2641">https://forum.dlsyscourse.org/t/q6-setitem-in-gpu-version/2641</a></li>
</ul>
<span id="more"></span>
<h2 id="重点回顾"><a href="#重点回顾" class="headerlink" title="重点回顾"></a>重点回顾</h2><p>利用stride描述一般的内存访问：</p>
<pre class="line-numbers language-none"><code class="language-none">A[i, j] &#x3D;&gt; Adata[i * A.strides[0] + j * A.strides[1]] <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>通过变换stride，就可以实现reshape的功能，例如形状(1, 2)变成(1, 1, 2)可以增加stride=0。</p>
<h2 id="Part-1-Python-array-operations"><a href="#Part-1-Python-array-operations" class="headerlink" title="Part 1: Python array operations"></a>Part 1: Python array operations</h2><p>实现<code>reshape, permute, broadcast_to, __getitem__</code>函数，主要基于stride。</p>
<p>stride：</p>
<p>高维数组在内存中的存储方式依然是一维，按照行优先可以表示为：</p>
<pre class="line-numbers language-none"><code class="language-none">A[i, j] &#x3D;&gt; Adata[i * A.shape[1] + j] <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>按照列优先则可以表示为：</p>
<pre class="line-numbers language-none"><code class="language-none">A[i, j] &#x3D;&gt; Adata[j * A.shape[0] + i] <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>更一般的情形，需要引入stride：</p>
<pre class="line-numbers language-none"><code class="language-none">A[i, j] &#x3D;&gt; Adata[i * A.strides[0] + j * A.strides[1]] <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>stride的引入可以让reshape更加方便，如形状(1, 2)变成(1, 1, 2)可以增加stride=0：</p>
<pre class="line-numbers language-none"><code class="language-none">A[i, j, k] &#x3D; Adata[i * A.strides[0] + j * 0 + k * A.strides[2]] <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>更一般的情况是增加offset，即初始位置的偏移：</p>
<pre class="line-numbers language-none"><code class="language-none">A[i, j] &#x3D;&gt; Adata[offset + i * A.strides[0] + j * A.strides[1]] <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>其他注意点在于需要计算输出形状，整体代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">reshape</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> new_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Reshape the matrix without copying memory.  This will return a matrix
    that corresponds to a reshaped array but points to the same memory as
    the original array.
    Raises:
        ValueError if product of current shape is not equal to the product
        of the new shape, or if the matrix is not compact.
    Args:
        new_shape (tuple): new shape of the array
    Returns:
        NDArray : reshaped array; this will point to the same memory as the original NDArray.
    """</span>

    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    origin_size <span class="token operator">=</span> self<span class="token punctuation">.</span>size
    new_size <span class="token operator">=</span> prod<span class="token punctuation">(</span>new_shape<span class="token punctuation">)</span>
    <span class="token keyword">if</span> origin_size <span class="token operator">!=</span> new_size<span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError
    new_strides <span class="token operator">=</span> self<span class="token punctuation">.</span>compact_strides<span class="token punctuation">(</span>new_shape<span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> NDArray<span class="token punctuation">.</span>make<span class="token punctuation">(</span>
        shape<span class="token operator">=</span>new_shape<span class="token punctuation">,</span> 
        strides<span class="token operator">=</span>new_strides<span class="token punctuation">,</span> 
        device<span class="token operator">=</span>self<span class="token punctuation">.</span>_device<span class="token punctuation">,</span> 
        handle<span class="token operator">=</span>self<span class="token punctuation">.</span>_handle<span class="token punctuation">,</span>
        offset<span class="token operator">=</span>self<span class="token punctuation">.</span>_offset<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    <span class="token comment">### END YOUR SOLUTION</span>

<span class="token keyword">def</span> <span class="token function">permute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> new_axes<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Permute order of the dimensions.  new_axes describes a permutation of the
    existing axes, so e.g.:
        - If we have an array with dimension "BHWC" then .permute((0,3,1,2))
        would convert this to "BCHW" order.
        - For a 2D array, .permute((1,0)) would transpose the array.
    Like reshape, this operation should not copy memory, but achieves the
    permuting by just adjusting the shape/strides of the array.  That is,
    it returns a new array that has the dimensions permuted as desired, but
    which points to the same memory as the original array.
    Args:
        new_axes (tuple): permutation order of the dimensions
    Returns:
        NDarray : new NDArray object with permuted dimensions, pointing
        to the same memory as the original NDArray (i.e., just shape and
        strides changed).
    """</span>
    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    new_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>ndim<span class="token punctuation">)</span><span class="token punctuation">]</span>
    new_strides <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>ndim<span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> j <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>new_axes<span class="token punctuation">)</span><span class="token punctuation">:</span>
        new_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>_shape<span class="token punctuation">[</span>j<span class="token punctuation">]</span>
        new_strides<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>_strides<span class="token punctuation">[</span>j<span class="token punctuation">]</span>
    
    <span class="token keyword">return</span> NDArray<span class="token punctuation">.</span>make<span class="token punctuation">(</span>
        shape<span class="token operator">=</span>new_shape<span class="token punctuation">,</span> 
        strides<span class="token operator">=</span><span class="token builtin">tuple</span><span class="token punctuation">(</span>new_strides<span class="token punctuation">)</span><span class="token punctuation">,</span> 
        device<span class="token operator">=</span>self<span class="token punctuation">.</span>_device<span class="token punctuation">,</span> 
        handle<span class="token operator">=</span>self<span class="token punctuation">.</span>_handle<span class="token punctuation">,</span>
        offset<span class="token operator">=</span>self<span class="token punctuation">.</span>_offset<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    <span class="token comment">### END YOUR SOLUTION</span>

<span class="token keyword">def</span> <span class="token function">broadcast_to</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> new_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Broadcast an array to a new shape.  new_shape's elements must be the
    same as the original shape, except for dimensions in the self where
    the size = 1 (which can then be broadcast to any size).  As with the
    previous calls, this will not copy memory, and just achieves
    broadcasting by manipulating the strides.
    Raises:
        assertion error if new_shape[i] != shape[i] for all i where
        shape[i] != 1
    Args:
        new_shape (tuple): shape to broadcast to
    Returns:
        NDArray: the new NDArray object with the new broadcast shape; should
        point to the same memory as the original array.
    """</span>

    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>new_shape<span class="token punctuation">)</span> <span class="token operator">!=</span> self<span class="token punctuation">.</span>ndim<span class="token punctuation">:</span>
        <span class="token keyword">raise</span> AssertionError
    new_strides <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>ndim<span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>ndim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> new_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span>
            new_strides<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>_strides<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            new_strides<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> AssertionError
    
    <span class="token keyword">return</span> NDArray<span class="token punctuation">.</span>make<span class="token punctuation">(</span>
        shape<span class="token operator">=</span>new_shape<span class="token punctuation">,</span> 
        strides<span class="token operator">=</span><span class="token builtin">tuple</span><span class="token punctuation">(</span>new_strides<span class="token punctuation">)</span><span class="token punctuation">,</span> 
        device<span class="token operator">=</span>self<span class="token punctuation">.</span>_device<span class="token punctuation">,</span> 
        handle<span class="token operator">=</span>self<span class="token punctuation">.</span>_handle<span class="token punctuation">,</span>
        offset<span class="token operator">=</span>self<span class="token punctuation">.</span>_offset<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    <span class="token comment">### END YOUR SOLUTION</span>
    
<span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idxs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    The __getitem__ operator in Python allows us to access elements of our
    array.  When passed notation such as a[1:5,:-1:2,4,:] etc, Python will
    convert this to a tuple of slices and integers (for singletons like the
    '4' in this example).  Slices can be a bit odd to work with (they have
    three elements .start .stop .step), which can be None or have negative
    entries, so for simplicity we wrote the code for you to convert these
    to always be a tuple of slices, one of each dimension.
    For this tuple of slices, return an array that subsets the desired
    elements.  As before, this can be done entirely through compute a new
    shape, stride, and offset for the new "view" into the original array,
    pointing to the same memory
    Raises:
        AssertionError if a slice has negative size or step, or if number
        of slices is not equal to the number of dimension (the stub code
        already raises all these errors.
    Args:
        idxs tuple: (after stub code processes), a tuple of slice elements
        corresponding to the subset of the matrix to get
    Returns:
        NDArray: a new NDArray object corresponding to the selected
        subset of elements.  As before, this should not copy memory but just
        manipulate the shape/strides/offset of the new array, referencing
        the same array as the original one.
    """</span>

    <span class="token comment"># handle singleton as tuple, everything as slices</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>idxs<span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        idxs <span class="token operator">=</span> <span class="token punctuation">(</span>idxs<span class="token punctuation">,</span><span class="token punctuation">)</span>
    idxs <span class="token operator">=</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>
        <span class="token punctuation">[</span>
            self<span class="token punctuation">.</span>process_slice<span class="token punctuation">(</span>s<span class="token punctuation">,</span> i<span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> <span class="token builtin">slice</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token builtin">slice</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> s <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> i<span class="token punctuation">,</span> s <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>idxs<span class="token punctuation">)</span>
        <span class="token punctuation">]</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>idxs<span class="token punctuation">)</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>ndim<span class="token punctuation">,</span> <span class="token string">"Need indexes equal to number of dimensions"</span>

    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    new_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>ndim<span class="token punctuation">)</span><span class="token punctuation">]</span>
    new_strides <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>ndim<span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>ndim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 不整除情形</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>idxs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>stop <span class="token operator">-</span> idxs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>start<span class="token punctuation">)</span> <span class="token operator">%</span> idxs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>step<span class="token punctuation">:</span>
            new_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>
        new_strides<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>strides<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> idxs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>step
    new_offset <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>ndim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        new_offset <span class="token operator">+=</span> self<span class="token punctuation">.</span>_strides<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> idxs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>start
    
    <span class="token keyword">return</span> NDArray<span class="token punctuation">.</span>make<span class="token punctuation">(</span>
        shape<span class="token operator">=</span>new_shape<span class="token punctuation">,</span> 
        strides<span class="token operator">=</span><span class="token builtin">tuple</span><span class="token punctuation">(</span>new_strides<span class="token punctuation">)</span><span class="token punctuation">,</span> 
        device<span class="token operator">=</span>self<span class="token punctuation">.</span>_device<span class="token punctuation">,</span> 
        handle<span class="token operator">=</span>self<span class="token punctuation">.</span>_handle<span class="token punctuation">,</span>
        offset<span class="token operator">=</span>new_offset<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-2-CPU-Backend-Compact-and-setitem"><a href="#Part-2-CPU-Backend-Compact-and-setitem" class="headerlink" title="Part 2: CPU Backend - Compact and setitem"></a>Part 2: CPU Backend - Compact and setitem</h2><p>主要难点就是如何用一个循环遍历$n$个维度，核心思想类似于大整数加法，将第$i$维的坐标理解为大整数的第$i$位：</p>
<pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">Compact</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">,</span> std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> shape<span class="token punctuation">,</span>
             std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> strides<span class="token punctuation">,</span> size_t offset<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Compact an array in memory
   *
   * Args:
   *   a: non-compact representation of the array, given as input
   *   out: compact version of the array to be written
   *   shape: shapes of each dimension for a and out
   *   strides: strides of the *a* array (not out, which has compact strides)
   *   offset: offset of the *a* array (not out, which has zero offset, being compact)
   *
   * Returns:
   *  void (you need to modify out directly, rather than returning anything; this is true for all the
   *  function will implement here, so we won't repeat this note.)
   */</span>
  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  <span class="token comment">// out不需要申请</span>
  <span class="token keyword">uint32_t</span> size <span class="token operator">=</span> out<span class="token operator">-></span>size<span class="token punctuation">;</span>
  <span class="token keyword">int</span> n <span class="token operator">=</span> shape<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> <span class="token function">indexs</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token comment">// 从末位到第一位</span>
  <span class="token keyword">int</span> i <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> size<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> k <span class="token operator">=</span> offset<span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> l <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> l <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> l<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      k <span class="token operator">+=</span> indexs<span class="token punctuation">[</span>l<span class="token punctuation">]</span> <span class="token operator">*</span> strides<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
    out<span class="token operator">-></span>ptr<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token comment">// update</span>
    indexs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">++</span><span class="token punctuation">;</span>
    <span class="token comment">// 进位</span>
    <span class="token keyword">int</span> carry <span class="token operator">=</span> indexs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">/</span> shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">while</span> <span class="token punctuation">(</span>carry<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      indexs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> indexs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">%</span> shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
      <span class="token keyword">int</span> i1 <span class="token operator">=</span> <span class="token punctuation">(</span>i <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">+</span> n<span class="token punctuation">)</span> <span class="token operator">%</span> n<span class="token punctuation">;</span>
      <span class="token keyword">if</span> <span class="token punctuation">(</span>i1 <span class="token operator">&lt;</span> i<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        indexs<span class="token punctuation">[</span>i1<span class="token punctuation">]</span> <span class="token operator">+=</span> carry<span class="token punctuation">;</span>
        carry <span class="token operator">=</span> indexs<span class="token punctuation">[</span>i1<span class="token punctuation">]</span> <span class="token operator">/</span> shape<span class="token punctuation">[</span>i1<span class="token punctuation">]</span><span class="token punctuation">;</span>
        i <span class="token operator">=</span> i1<span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span> <span class="token keyword">else</span> <span class="token punctuation">&#123;</span>
        <span class="token keyword">break</span><span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span>
    <span class="token punctuation">&#125;</span>
    <span class="token comment">// 回到最低位</span>
    i <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseSetitem</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">,</span> std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> shape<span class="token punctuation">,</span>
                  std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> strides<span class="token punctuation">,</span> size_t offset<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Set items in a (non-compact) array
   *
   * Args:
   *   a: _compact_ array whose items will be written to out
   *   out: non-compact array whose items are to be written
   *   shape: shapes of each dimension for a and out
   *   strides: strides of the *out* array (not a, which has compact strides)
   *   offset: offset of the *out* array (not a, which has zero offset, being compact)
   */</span>
  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  <span class="token comment">// out不需要申请</span>
  <span class="token keyword">uint32_t</span> size <span class="token operator">=</span> a<span class="token punctuation">.</span>size<span class="token punctuation">;</span>
  <span class="token keyword">int</span> n <span class="token operator">=</span> shape<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> <span class="token function">indexs</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token comment">// 从末位到第一位</span>
  <span class="token keyword">int</span> i <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> size<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> k <span class="token operator">=</span> offset<span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> l <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> l <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> l<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      k <span class="token operator">+=</span> indexs<span class="token punctuation">[</span>l<span class="token punctuation">]</span> <span class="token operator">*</span> strides<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
    out<span class="token operator">-></span>ptr<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token comment">// update</span>
    indexs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">++</span><span class="token punctuation">;</span>
    <span class="token comment">// 进位</span>
    <span class="token keyword">int</span> carry <span class="token operator">=</span> indexs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">/</span> shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">while</span> <span class="token punctuation">(</span>carry<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      indexs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> indexs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">%</span> shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
      <span class="token keyword">int</span> i1 <span class="token operator">=</span> <span class="token punctuation">(</span>i <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">+</span> n<span class="token punctuation">)</span> <span class="token operator">%</span> n<span class="token punctuation">;</span>
      <span class="token keyword">if</span> <span class="token punctuation">(</span>i1 <span class="token operator">&lt;</span> i<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        indexs<span class="token punctuation">[</span>i1<span class="token punctuation">]</span> <span class="token operator">+=</span> carry<span class="token punctuation">;</span>
        carry <span class="token operator">=</span> indexs<span class="token punctuation">[</span>i1<span class="token punctuation">]</span> <span class="token operator">/</span> shape<span class="token punctuation">[</span>i1<span class="token punctuation">]</span><span class="token punctuation">;</span>
        i <span class="token operator">=</span> i1<span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span> <span class="token keyword">else</span> <span class="token punctuation">&#123;</span>
        <span class="token keyword">break</span><span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span>
    <span class="token punctuation">&#125;</span>
    <span class="token comment">// 回到最低位</span>
    i <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarSetitem</span><span class="token punctuation">(</span><span class="token keyword">const</span> size_t size<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">,</span> std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> shape<span class="token punctuation">,</span>
                   std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> strides<span class="token punctuation">,</span> size_t offset<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Set items is a (non-compact) array
   *
   * Args:
   *   size: number of elements to write in out array (note that this will note be the same as
   *         out.size, because out is a non-compact subset array);  it _will_ be the same as the
   *         product of items in shape, but convenient to just pass it here.
   *   val: scalar value to write to
   *   out: non-compact array whose items are to be written
   *   shape: shapes of each dimension of out
   *   strides: strides of the out array
   *   offset: offset of the out array
   */</span>

  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  <span class="token keyword">int</span> n <span class="token operator">=</span> shape<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> <span class="token function">indexs</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token comment">// 从末位到第一位</span>
  <span class="token keyword">int</span> i <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> size<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> k <span class="token operator">=</span> offset<span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> l <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> l <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> l<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      k <span class="token operator">+=</span> indexs<span class="token punctuation">[</span>l<span class="token punctuation">]</span> <span class="token operator">*</span> strides<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
    out<span class="token operator">-></span>ptr<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> val<span class="token punctuation">;</span>
    <span class="token comment">// update</span>
    indexs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">++</span><span class="token punctuation">;</span>
    <span class="token comment">// 进位</span>
    <span class="token keyword">int</span> carry <span class="token operator">=</span> indexs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">/</span> shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">while</span> <span class="token punctuation">(</span>carry<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      indexs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> indexs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">%</span> shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
      <span class="token keyword">int</span> i1 <span class="token operator">=</span> <span class="token punctuation">(</span>i <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">+</span> n<span class="token punctuation">)</span> <span class="token operator">%</span> n<span class="token punctuation">;</span>
      <span class="token keyword">if</span> <span class="token punctuation">(</span>i1 <span class="token operator">&lt;</span> i<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        indexs<span class="token punctuation">[</span>i1<span class="token punctuation">]</span> <span class="token operator">+=</span> carry<span class="token punctuation">;</span>
        carry <span class="token operator">=</span> indexs<span class="token punctuation">[</span>i1<span class="token punctuation">]</span> <span class="token operator">/</span> shape<span class="token punctuation">[</span>i1<span class="token punctuation">]</span><span class="token punctuation">;</span>
        i <span class="token operator">=</span> i1<span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span> <span class="token keyword">else</span> <span class="token punctuation">&#123;</span>
        <span class="token keyword">break</span><span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span>
    <span class="token punctuation">&#125;</span>
    <span class="token comment">// 回到最低位</span>
    i <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-3-CPU-Backend-Elementwise-and-scalar-operations"><a href="#Part-3-CPU-Backend-Elementwise-and-scalar-operations" class="headerlink" title="Part 3: CPU Backend - Elementwise and scalar operations"></a>Part 3: CPU Backend - Elementwise and scalar operations</h2><p>这部分没啥难度，主要是使用函数模板：</p>
<pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token comment">/// BEGIN YOUR SOLUTION</span>
<span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">F</span><span class="token operator">></span>
<span class="token keyword">void</span> <span class="token function">EwiseFun</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">,</span> F f<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span>size_t i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> a<span class="token punctuation">.</span>size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token operator">-></span>ptr<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">f</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">F</span><span class="token operator">></span>
<span class="token keyword">void</span> <span class="token function">ScalarFun</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">,</span> F f<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span>size_t i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> a<span class="token punctuation">.</span>size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token operator">-></span>ptr<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">f</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> val<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">F</span><span class="token operator">></span>
<span class="token keyword">void</span> <span class="token function">SingleEwiseFun</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">,</span> F f<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span>size_t i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> a<span class="token punctuation">.</span>size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token operator">-></span>ptr<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">f</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseMul, ScalarMul</span>
scalar_t <span class="token function">Mul</span><span class="token punctuation">(</span>scalar_t a<span class="token punctuation">,</span> scalar_t b<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> a <span class="token operator">*</span> b<span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseMul</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">EwiseFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Mul<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarMul</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">ScalarFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> val<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Mul<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseDiv, ScalarDiv</span>
scalar_t <span class="token function">Div</span><span class="token punctuation">(</span>scalar_t a<span class="token punctuation">,</span> scalar_t b<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> a <span class="token operator">/</span> b<span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseDiv</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">EwiseFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Div<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarDiv</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">ScalarFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> val<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Div<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// ScalarPower</span>
scalar_t <span class="token function">Power</span><span class="token punctuation">(</span>scalar_t a<span class="token punctuation">,</span> scalar_t b<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">pow</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarPower</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">ScalarFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> val<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Power<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseMaximum, ScalarMaximum</span>
scalar_t <span class="token function">Maximum</span><span class="token punctuation">(</span>scalar_t a<span class="token punctuation">,</span> scalar_t b<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> std<span class="token operator">::</span><span class="token function">max</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseMaximum</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">EwiseFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Maximum<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarMaximum</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">ScalarFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> val<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Maximum<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseEq, ScalarEq</span>
scalar_t <span class="token function">Eq</span><span class="token punctuation">(</span>scalar_t a<span class="token punctuation">,</span> scalar_t b<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> std<span class="token operator">::</span><span class="token function">fabs</span><span class="token punctuation">(</span>a <span class="token operator">-</span> b<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">1e-6</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseEq</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">EwiseFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Eq<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarEq</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">ScalarFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> val<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Eq<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseGe, ScalarGe</span>
scalar_t <span class="token function">Ge</span><span class="token punctuation">(</span>scalar_t a<span class="token punctuation">,</span> scalar_t b<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> a <span class="token operator">>=</span> b<span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseGe</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">EwiseFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Ge<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarGe</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">ScalarFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> val<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Ge<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseLog</span>
scalar_t <span class="token function">Log</span><span class="token punctuation">(</span>scalar_t a<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">log</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseLog</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">SingleEwiseFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Log<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseExp</span>
scalar_t <span class="token function">Exp</span><span class="token punctuation">(</span>scalar_t a<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">exp</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseExp</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">SingleEwiseFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Exp<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseTanh</span>
scalar_t <span class="token function">Tanh</span><span class="token punctuation">(</span>scalar_t a<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">tanh</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseTanh</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> <span class="token function">SingleEwiseFun</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> out<span class="token punctuation">,</span> Tanh<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">/// END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-4-CPU-Backend-Reductions"><a href="#Part-4-CPU-Backend-Reductions" class="headerlink" title="Part 4: CPU Backend - Reductions"></a>Part 4: CPU Backend - Reductions</h2><p>依旧难度不大，转换坐标即可：</p>
<pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">ReduceMax</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t reduce_size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Reduce by taking maximum over `reduce_size` contiguous blocks.
   *
   * Args:
   *   a: compact array of size a.size = out.size * reduce_size to reduce over
   *   out: compact array to write into
   *   reduce_size: size of the dimension to reduce over
   */</span>

  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  size_t n <span class="token operator">=</span> out<span class="token operator">-></span>size<span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> j <span class="token operator">=</span> i <span class="token operator">*</span> reduce_size<span class="token punctuation">;</span>
    scalar_t res <span class="token operator">=</span> a<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> reduce_size<span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      res <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">max</span><span class="token punctuation">(</span>res<span class="token punctuation">,</span> a<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>j <span class="token operator">+</span> k<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
    out<span class="token operator">-></span>ptr<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> res<span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ReduceSum</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t reduce_size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Reduce by taking sum over `reduce_size` contiguous blocks.
   *
   * Args:
   *   a: compact array of size a.size = out.size * reduce_size to reduce over
   *   out: compact array to write into
   *   reduce_size: size of the dimension to reduce over
   */</span>

  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  size_t n <span class="token operator">=</span> out<span class="token operator">-></span>size<span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> j <span class="token operator">=</span> i <span class="token operator">*</span> reduce_size<span class="token punctuation">;</span>
    scalar_t res <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> reduce_size<span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      res <span class="token operator">+=</span> a<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>j <span class="token operator">+</span> k<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
    out<span class="token operator">-></span>ptr<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> res<span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-5-CPU-Backend-Matrix-multiplication"><a href="#Part-5-CPU-Backend-Matrix-multiplication" class="headerlink" title="Part 5: CPU Backend - Matrix multiplication"></a>Part 5: CPU Backend - Matrix multiplication</h2><p>矩阵乘法和分块矩阵乘法：</p>
<pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">void</span> <span class="token function">Matmul</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">,</span> <span class="token keyword">uint32_t</span> m<span class="token punctuation">,</span> <span class="token keyword">uint32_t</span> n<span class="token punctuation">,</span>
            <span class="token keyword">uint32_t</span> p<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Multiply two (compact) matrices into an output (also compact) matrix.  For this implementation
   * you can use the "naive" three-loop algorithm.
   *
   * Args:
   *   a: compact 2D array of size m x n
   *   b: compact 2D array of size n x p
   *   out: compact 2D array of size m x p to write the output to
   *   m: rows of a / out
   *   n: columns of a / rows of b
   *   p: columns of b / out
   */</span>

  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> m<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> p<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      <span class="token keyword">int</span> r <span class="token operator">=</span> i <span class="token operator">*</span> p <span class="token operator">+</span> j<span class="token punctuation">;</span>
      out<span class="token operator">-></span>ptr<span class="token punctuation">[</span>r<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        <span class="token keyword">int</span> s <span class="token operator">=</span> i <span class="token operator">*</span> n <span class="token operator">+</span> k<span class="token punctuation">;</span>
        <span class="token keyword">int</span> t <span class="token operator">=</span> k <span class="token operator">*</span> p <span class="token operator">+</span> j<span class="token punctuation">;</span>
        out<span class="token operator">-></span>ptr<span class="token punctuation">[</span>r<span class="token punctuation">]</span> <span class="token operator">+=</span> a<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>s<span class="token punctuation">]</span> <span class="token operator">*</span> b<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">inline</span> <span class="token keyword">void</span> <span class="token function">AlignedDot</span><span class="token punctuation">(</span><span class="token keyword">const</span> <span class="token keyword">float</span><span class="token operator">*</span> __restrict__ a<span class="token punctuation">,</span>
                       <span class="token keyword">const</span> <span class="token keyword">float</span><span class="token operator">*</span> __restrict__ b<span class="token punctuation">,</span>
                       <span class="token keyword">float</span><span class="token operator">*</span> __restrict__ out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>

  <span class="token comment">/**
   * Multiply together two TILE x TILE matrices, and _add _the result to out (it is important to add
   * the result to the existing out, which you should not set to zero beforehand).  We are including
   * the compiler flags here that enable the compile to properly use vector operators to implement
   * this function.  Specifically, the __restrict__ keyword indicates to the compile that a, b, and
   * out don't have any overlapping memory (which is necessary in order for vector operations to be
   * equivalent to their non-vectorized counterparts (imagine what could happen otherwise if a, b,
   * and out had overlapping memory).  Similarly the __builtin_assume_aligned keyword tells the
   * compiler that the input array will be aligned to the appropriate blocks in memory, which also
   * helps the compiler vectorize the code.
   *
   * Args:
   *   a: compact 2D array of size TILE x TILE
   *   b: compact 2D array of size TILE x TILE
   *   out: compact 2D array of size TILE x TILE to write to
   */</span>

  a <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">const</span> <span class="token keyword">float</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">__builtin_assume_aligned</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> TILE <span class="token operator">*</span> ELEM_SIZE<span class="token punctuation">)</span><span class="token punctuation">;</span>
  b <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">const</span> <span class="token keyword">float</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">__builtin_assume_aligned</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> TILE <span class="token operator">*</span> ELEM_SIZE<span class="token punctuation">)</span><span class="token punctuation">;</span>
  out <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">__builtin_assume_aligned</span><span class="token punctuation">(</span>out<span class="token punctuation">,</span> TILE <span class="token operator">*</span> ELEM_SIZE<span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> TILE<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> TILE<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      <span class="token keyword">int</span> r <span class="token operator">=</span> i <span class="token operator">*</span> TILE <span class="token operator">+</span> j<span class="token punctuation">;</span>
      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> TILE<span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        <span class="token keyword">int</span> s <span class="token operator">=</span> i <span class="token operator">*</span> TILE <span class="token operator">+</span> k<span class="token punctuation">;</span>
        <span class="token keyword">int</span> t <span class="token operator">=</span> k <span class="token operator">*</span> TILE <span class="token operator">+</span> j<span class="token punctuation">;</span>
        out<span class="token punctuation">[</span>r<span class="token punctuation">]</span> <span class="token operator">+=</span> a<span class="token punctuation">[</span>s<span class="token punctuation">]</span> <span class="token operator">*</span> b<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">MatmulTiled</span><span class="token punctuation">(</span><span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> AlignedArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> AlignedArray<span class="token operator">*</span> out<span class="token punctuation">,</span> <span class="token keyword">uint32_t</span> m<span class="token punctuation">,</span>
                 <span class="token keyword">uint32_t</span> n<span class="token punctuation">,</span> <span class="token keyword">uint32_t</span> p<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Matrix multiplication on tiled representations of array.  In this setting, a, b, and out
   * are all *4D* compact arrays of the appropriate size, e.g. a is an array of size
   *   a[m/TILE][n/TILE][TILE][TILE]
   * You should do the multiplication tile-by-tile to improve performance of the array (i.e., this
   * function should call `AlignedDot()` implemented above).
   *
   * Note that this function will only be called when m, n, p are all multiples of TILE, so you can
   * assume that this division happens without any remainder.
   *
   * Args:
   *   a: compact 4D array of size m/TILE x n/TILE x TILE x TILE
   *   b: compact 4D array of size n/TILE x p/TILE x TILE x TILE
   *   out: compact 4D array of size m/TILE x p/TILE x TILE x TILE to write to
   *   m: rows of a / out
   *   n: columns of a / rows of b
   *   p: columns of b / out
   *
   */</span>
  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  size_t m1 <span class="token operator">=</span> m <span class="token operator">/</span> TILE<span class="token punctuation">;</span>
  size_t n1 <span class="token operator">=</span> n <span class="token operator">/</span> TILE<span class="token punctuation">;</span>
  size_t p1 <span class="token operator">=</span> p <span class="token operator">/</span> TILE<span class="token punctuation">;</span>
  size_t TILE2 <span class="token operator">=</span> TILE <span class="token operator">*</span> TILE<span class="token punctuation">;</span>
  <span class="token keyword">float</span> <span class="token operator">*</span>a1 <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token keyword">float</span><span class="token punctuation">[</span>TILE2<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token keyword">float</span> <span class="token operator">*</span>b1 <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token keyword">float</span><span class="token punctuation">[</span>TILE2<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token keyword">float</span> <span class="token operator">*</span>out1 <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token keyword">float</span><span class="token punctuation">[</span>TILE2<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> m1<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> p1<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      <span class="token comment">// 初始化为0</span>
      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> u <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> u <span class="token operator">&lt;</span> TILE2<span class="token punctuation">;</span> u<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        out1<span class="token punctuation">[</span>u<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span>
      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> n1<span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        <span class="token keyword">int</span> s <span class="token operator">=</span> <span class="token punctuation">(</span>i <span class="token operator">*</span> n1 <span class="token operator">+</span> k<span class="token punctuation">)</span> <span class="token operator">*</span> TILE2<span class="token punctuation">;</span>
        <span class="token keyword">int</span> t <span class="token operator">=</span> <span class="token punctuation">(</span>k <span class="token operator">*</span> p1 <span class="token operator">+</span> j<span class="token punctuation">)</span> <span class="token operator">*</span> TILE2<span class="token punctuation">;</span>
        <span class="token comment">// 复制到a, b中</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> u <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> u <span class="token operator">&lt;</span> TILE2<span class="token punctuation">;</span> u<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
          a1<span class="token punctuation">[</span>u<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>s <span class="token operator">+</span> u<span class="token punctuation">]</span><span class="token punctuation">;</span>
          b1<span class="token punctuation">[</span>u<span class="token punctuation">]</span> <span class="token operator">=</span> b<span class="token punctuation">.</span>ptr<span class="token punctuation">[</span>t <span class="token operator">+</span> u<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token punctuation">&#125;</span>
        <span class="token function">AlignedDot</span><span class="token punctuation">(</span>a1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> out1<span class="token punctuation">)</span><span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span>
      <span class="token comment">// 复制回out</span>
      <span class="token keyword">int</span> r <span class="token operator">=</span> <span class="token punctuation">(</span>i <span class="token operator">*</span> p1 <span class="token operator">+</span> j<span class="token punctuation">)</span> <span class="token operator">*</span> TILE2<span class="token punctuation">;</span>
      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> u <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> u <span class="token operator">&lt;</span> TILE2<span class="token punctuation">;</span> u<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        out<span class="token operator">-></span>ptr<span class="token punctuation">[</span>r <span class="token operator">+</span> u<span class="token punctuation">]</span> <span class="token operator">=</span> out1<span class="token punctuation">[</span>u<span class="token punctuation">]</span><span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-6-CUDA-Backend-Compact-and-setitem"><a href="#Part-6-CUDA-Backend-Compact-and-setitem" class="headerlink" title="Part 6: CUDA Backend - Compact and setitem"></a>Part 6: CUDA Backend - Compact and setitem</h2><p>实现cuda版本，需要参考课件中cuda的使用方式：</p>
<pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">__global__ <span class="token keyword">void</span> <span class="token function">CompactKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">,</span> CudaVec shape<span class="token punctuation">,</span>
                              CudaVec strides<span class="token punctuation">,</span> size_t offset<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * The CUDA kernel for the compact opeation.  This should effectively map a single entry in the 
   * non-compact input a, to the corresponding item (at location gid) in the compact array out.
   * 
   * Args:
   *   a: CUDA pointer to a array
   *   out: CUDA point to out array
   *   size: size of out array
   *   shape: vector of shapes of a and out arrays (of type CudaVec, for past passing to CUDA kernel)
   *   strides: vector of strides of out array
   *   offset: offset of out array
   */</span>
  <span class="token comment">// out的索引</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>

  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> n <span class="token operator">=</span> shape<span class="token punctuation">.</span>size<span class="token punctuation">;</span>
    <span class="token keyword">int</span> i1 <span class="token operator">=</span> offset<span class="token punctuation">;</span>
    <span class="token keyword">int</span> j1 <span class="token operator">=</span> gid<span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">--</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      i1 <span class="token operator">+=</span> <span class="token punctuation">(</span>j1 <span class="token operator">%</span> shape<span class="token punctuation">.</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> strides<span class="token punctuation">.</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
      j1 <span class="token operator">/=</span> shape<span class="token punctuation">.</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>i1<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">Compact</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">,</span> std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> shape<span class="token punctuation">,</span>
             std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> strides<span class="token punctuation">,</span> size_t offset<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Compact an array in memory.  Unlike the C++ version, in CUDA this will primarily call the 
   * relevant CUDA kernel.  In this case, we illustrate how you should set this up (i.e., we give 
   * you the code for this fuction, and also the prototype for the CompactKernel() function).  For
   * the functions after this, however, you'll need to define these kernels as you see fit to 
   * execute the underlying function.
   * 
   * Args:
   *   a: non-compact represntation of the array, given as input
   *   out: compact version of the array to be written
   *   shape: shapes of each dimension for a and out
   *   strides: strides of the *a* array (not out, which has compact strides)
   *   offset: offset of the *a* array (not out, which has zero offset, being compact)
   */</span>

  <span class="token comment">// Nothing needs to be added here</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  CompactKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">,</span> <span class="token function">VecToCuda</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                         <span class="token function">VecToCuda</span><span class="token punctuation">(</span>strides<span class="token punctuation">)</span><span class="token punctuation">,</span> offset<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>


__global__ <span class="token keyword">void</span> <span class="token function">EwiseSetitemKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">,</span> CudaVec shape<span class="token punctuation">,</span>
                              CudaVec strides<span class="token punctuation">,</span> size_t offset<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * The CUDA kernel for the ewiseSetitem opeation.  This should effectively map a single entry in the 
   * non-compact input a, to the corresponding item (at location gid) in the compact array out.
   * 
   * Args:
   *   a: CUDA pointer to a array
   *   out: CUDA point to out array
   *   size: size of out array
   *   shape: vector of shapes of a and out arrays (of type CudaVec, for past passing to CUDA kernel)
   *   strides: vector of strides of out array
   *   offset: offset of out array
   */</span>
  <span class="token comment">// a的索引</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>

  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> n <span class="token operator">=</span> shape<span class="token punctuation">.</span>size<span class="token punctuation">;</span>
    <span class="token keyword">int</span> i1 <span class="token operator">=</span> offset<span class="token punctuation">;</span>
    <span class="token keyword">int</span> j1 <span class="token operator">=</span> gid<span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">--</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      i1 <span class="token operator">+=</span> <span class="token punctuation">(</span>j1 <span class="token operator">%</span> shape<span class="token punctuation">.</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> strides<span class="token punctuation">.</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
      j1 <span class="token operator">/=</span> shape<span class="token punctuation">.</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
    out<span class="token punctuation">[</span>i1<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseSetitem</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">,</span> std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> shape<span class="token punctuation">,</span>
                  std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> strides<span class="token punctuation">,</span> size_t offset<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Set items in a (non-compact) array using CUDA.  You will most likely want to implement a
   * EwiseSetitemKernel() function, similar to those above, that will do the actual work.
   * 
   * Args:
   *   a: _compact_ array whose items will be written to out
   *   out: non-compact array whose items are to be written
   *   shape: shapes of each dimension for a and out
   *   strides: strides of the *out* array (not a, which has compact strides)
   *   offset: offset of the *out* array (not a, which has zero offset, being compact)
   */</span>
  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  EwiseSetitemKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> a<span class="token punctuation">.</span>size<span class="token punctuation">,</span> <span class="token function">VecToCuda</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                         <span class="token function">VecToCuda</span><span class="token punctuation">(</span>strides<span class="token punctuation">)</span><span class="token punctuation">,</span> offset<span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span>


__global__ <span class="token keyword">void</span> <span class="token function">ScalarSetitemKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t val<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">,</span> CudaVec shape<span class="token punctuation">,</span>
                              CudaVec strides<span class="token punctuation">,</span> size_t offset<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * The CUDA kernel for the ewiseSetitem opeation.  This should effectively map a single entry in the 
   * non-compact input a, to the corresponding item (at location gid) in the compact array out.
   * 
   * Args:
   *   a: CUDA pointer to a array
   *   out: CUDA point to out array
   *   size: size of out array
   *   shape: vector of shapes of a and out arrays (of type CudaVec, for past passing to CUDA kernel)
   *   strides: vector of strides of out array
   *   offset: offset of out array
   */</span>
  <span class="token comment">// a的索引</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>

  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> n <span class="token operator">=</span> shape<span class="token punctuation">.</span>size<span class="token punctuation">;</span>
    <span class="token keyword">int</span> i1 <span class="token operator">=</span> offset<span class="token punctuation">;</span>
    <span class="token keyword">int</span> j1 <span class="token operator">=</span> gid<span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">--</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      i1 <span class="token operator">+=</span> <span class="token punctuation">(</span>j1 <span class="token operator">%</span> shape<span class="token punctuation">.</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> strides<span class="token punctuation">.</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
      j1 <span class="token operator">/=</span> shape<span class="token punctuation">.</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
    out<span class="token punctuation">[</span>i1<span class="token punctuation">]</span> <span class="token operator">=</span> val<span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarSetitem</span><span class="token punctuation">(</span>size_t size<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">,</span> std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> shape<span class="token punctuation">,</span>
                   std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token operator">></span> strides<span class="token punctuation">,</span> size_t offset<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Set items is a (non-compact) array
   * 
   * Args:
   *   size: number of elements to write in out array (note that this will note be the same as
   *         out.size, because out is a non-compact subset array);  it _will_ be the same as the 
   *         product of items in shape, but covenient to just pass it here.
   *   val: scalar value to write to
   *   out: non-compact array whose items are to be written
   *   shape: shapes of each dimension of out
   *   strides: strides of the out array
   *   offset: offset of the out array
   */</span>
  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  ScalarSetitemKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>val<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">,</span> <span class="token function">VecToCuda</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                         <span class="token function">VecToCuda</span><span class="token punctuation">(</span>strides<span class="token punctuation">)</span><span class="token punctuation">,</span> offset<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-7-CUDA-Backend-Elementwise-and-scalar-operations"><a href="#Part-7-CUDA-Backend-Elementwise-and-scalar-operations" class="headerlink" title="Part 7: CUDA Backend - Elementwise and scalar operations"></a>Part 7: CUDA Backend - Elementwise and scalar operations</h2><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token comment">/**
 * In the code the follows, use the above template to create analogous elementise
 * and and scalar operators for the following functions.  See the numpy backend for
 * examples of how they should work.
 *   - EwiseMul, ScalarMul
 *   - EwiseDiv, ScalarDiv
 *   - ScalarPower
 *   - EwiseMaximum, ScalarMaximum
 *   - EwiseEq, ScalarEq
 *   - EwiseGe, ScalarGe
 *   - EwiseLog
 *   - EwiseExp
 *   - EwiseTanh
 *
 * If you implement all these naively, there will be a lot of repeated code, so
 * you are welcome (but not required), to use macros or templates to define these
 * functions (however you want to do so, as long as the functions match the proper)
 * signatures above.
 */</span>

<span class="token comment">/// BEGIN YOUR SOLUTION</span>
<span class="token comment">// EwiseMul, ScalarMul</span>
__global__ <span class="token keyword">void</span> <span class="token function">EwiseMulKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> scalar_t<span class="token operator">*</span> b<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">*</span> b<span class="token punctuation">[</span>gid<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseMul</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  EwiseMulKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> b<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

__global__ <span class="token keyword">void</span> <span class="token function">ScalarMulKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">*</span> val<span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarMul</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  ScalarMulKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> val<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseDiv, ScalarDiv</span>
__global__ <span class="token keyword">void</span> <span class="token function">EwiseDivKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> scalar_t<span class="token operator">*</span> b<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">/</span> b<span class="token punctuation">[</span>gid<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseDiv</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  EwiseDivKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> b<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

__global__ <span class="token keyword">void</span> <span class="token function">ScalarDivKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">/</span> val<span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarDiv</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  ScalarDivKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> val<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// ScalarPower</span>
__global__ <span class="token keyword">void</span> <span class="token function">ScalarPowerKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">pow</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span><span class="token punctuation">,</span> val<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarPower</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  ScalarPowerKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> val<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseMaximum, ScalarMaximum</span>
__global__ <span class="token keyword">void</span> <span class="token function">EwiseMaximumKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> scalar_t<span class="token operator">*</span> b<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">&lt;</span> b<span class="token punctuation">[</span>gid<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> b<span class="token punctuation">[</span>gid<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span> <span class="token keyword">else</span> <span class="token punctuation">&#123;</span>
      out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseMaximum</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  EwiseMaximumKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> b<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

__global__ <span class="token keyword">void</span> <span class="token function">ScalarMaximumKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">&lt;</span> val<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> val<span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span> <span class="token keyword">else</span> <span class="token punctuation">&#123;</span>
      out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarMaximum</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  ScalarMaximumKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> val<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseEq, ScalarEq</span>
__global__ <span class="token keyword">void</span> <span class="token function">EwiseEqKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> scalar_t<span class="token operator">*</span> b<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">scalar_t</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">-</span> b<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">1e-6</span> <span class="token operator">&amp;&amp;</span> a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">-</span> b<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">></span> <span class="token operator">-</span><span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseEq</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  EwiseEqKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> b<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

__global__ <span class="token keyword">void</span> <span class="token function">ScalarEqKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">scalar_t</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">-</span> val <span class="token operator">&lt;</span> <span class="token number">1e-6</span> <span class="token operator">&amp;&amp;</span> a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">-</span> val <span class="token operator">></span> <span class="token operator">-</span><span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarEq</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  ScalarEqKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> val<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseGe, ScalarGe</span>
__global__ <span class="token keyword">void</span> <span class="token function">EwiseGeKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> scalar_t<span class="token operator">*</span> b<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">>=</span> b<span class="token punctuation">[</span>gid<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseGe</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  EwiseGeKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> b<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

__global__ <span class="token keyword">void</span> <span class="token function">ScalarGeKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">>=</span> val<span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ScalarGe</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> scalar_t val<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  ScalarGeKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> val<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseLog</span>
__global__ <span class="token keyword">void</span> <span class="token function">SingleEwiseLogKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">log</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseLog</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  SingleEwiseLogKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseExp</span>
__global__ <span class="token keyword">void</span> <span class="token function">SingleEwiseExpKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">exp</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseExp</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  SingleEwiseExpKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token comment">// EwiseTanh</span>
__global__ <span class="token keyword">void</span> <span class="token function">SingleEwiseTanhKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">tanh</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>gid<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">EwiseTanh</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  SingleEwiseTanhKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>
<span class="token comment">/// END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-8-CUDA-Backend-Reductions"><a href="#Part-8-CUDA-Backend-Reductions" class="headerlink" title="Part 8: CUDA Backend - Reductions"></a>Part 8: CUDA Backend - Reductions</h2><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token comment">////////////////////////////////////////////////////////////////////////////////</span>
<span class="token comment">// Max and sum reductions</span>
<span class="token comment">////////////////////////////////////////////////////////////////////////////////</span>
__global__ <span class="token keyword">void</span> <span class="token function">ReduceMaxKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">,</span> size_t reduce_size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> j <span class="token operator">=</span> gid <span class="token operator">*</span> reduce_size<span class="token punctuation">;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> reduce_size<span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      <span class="token keyword">if</span> <span class="token punctuation">(</span>out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">&lt;</span> a<span class="token punctuation">[</span>j <span class="token operator">+</span> k<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>j <span class="token operator">+</span> k<span class="token punctuation">]</span><span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ReduceMax</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t reduce_size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Reduce by taking maximum over `reduce_size` contiguous blocks.  Even though it is inefficient,
   * for simplicity you can perform each reduction in a single CUDA thread.
   * 
   * Args:
   *   a: compact array of size a.size = out.size * reduce_size to reduce over
   *   out: compact array to write into
   *   redice_size: size of the dimension to reduce over
   */</span>
  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  ReduceMaxKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">,</span> reduce_size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span>


__global__ <span class="token keyword">void</span> <span class="token function">ReduceSumKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t size<span class="token punctuation">,</span> size_t reduce_size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> j <span class="token operator">=</span> gid <span class="token operator">*</span> reduce_size<span class="token punctuation">;</span>
    out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> reduce_size<span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      out<span class="token punctuation">[</span>gid<span class="token punctuation">]</span> <span class="token operator">+=</span> a<span class="token punctuation">[</span>j <span class="token operator">+</span> k<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">ReduceSum</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">,</span> size_t reduce_size<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Reduce by taking summation over `reduce_size` contiguous blocks.  Again, for simplicity you 
   * can perform each reduction in a single CUDA thread.
   * 
   * Args:
   *   a: compact array of size a.size = out.size * reduce_size to reduce over
   *   out: compact array to write into
   *   redice_size: size of the dimension to reduce over
   */</span>
  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>out<span class="token operator">-></span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  ReduceSumKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>size<span class="token punctuation">,</span> reduce_size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-9-CUDA-Backend-Matrix-multiplication"><a href="#Part-9-CUDA-Backend-Matrix-multiplication" class="headerlink" title="Part 9: CUDA Backend - Matrix multiplication"></a>Part 9: CUDA Backend - Matrix multiplication</h2><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token comment">////////////////////////////////////////////////////////////////////////////////</span>
<span class="token comment">// Elementwise and scalar operations</span>
<span class="token comment">////////////////////////////////////////////////////////////////////////////////</span>

__global__ <span class="token keyword">void</span> <span class="token function">MatmulKernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> scalar_t<span class="token operator">*</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> scalar_t<span class="token operator">*</span> b<span class="token punctuation">,</span> scalar_t<span class="token operator">*</span> out<span class="token punctuation">,</span> <span class="token keyword">uint32_t</span> M<span class="token punctuation">,</span> <span class="token keyword">uint32_t</span> N<span class="token punctuation">,</span>
            <span class="token keyword">uint32_t</span> P<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  size_t gid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>gid <span class="token operator">&lt;</span> M<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> P<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
      <span class="token keyword">int</span> r <span class="token operator">=</span> gid <span class="token operator">*</span> P <span class="token operator">+</span> j<span class="token punctuation">;</span>
      out<span class="token punctuation">[</span>r<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        <span class="token keyword">int</span> s <span class="token operator">=</span> gid <span class="token operator">*</span> N <span class="token operator">+</span> k<span class="token punctuation">;</span>
        <span class="token keyword">int</span> t <span class="token operator">=</span> k <span class="token operator">*</span> P <span class="token operator">+</span> j<span class="token punctuation">;</span>
        out<span class="token punctuation">[</span>r<span class="token punctuation">]</span> <span class="token operator">+=</span> a<span class="token punctuation">[</span>s<span class="token punctuation">]</span> <span class="token operator">*</span> b<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">;</span>
      <span class="token punctuation">&#125;</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">Matmul</span><span class="token punctuation">(</span><span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> a<span class="token punctuation">,</span> <span class="token keyword">const</span> CudaArray<span class="token operator">&amp;</span> b<span class="token punctuation">,</span> CudaArray<span class="token operator">*</span> out<span class="token punctuation">,</span> <span class="token keyword">uint32_t</span> M<span class="token punctuation">,</span> <span class="token keyword">uint32_t</span> N<span class="token punctuation">,</span>
            <span class="token keyword">uint32_t</span> P<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token comment">/**
   * Multiply two (compact) matrices into an output (also comapct) matrix.  You will want to look
   * at the lecture and notes on GPU-based linear algebra to see how to do this.  Since ultimately
   * mugrade is just evaluating correctness, you _can_ implement a version that simply parallelizes
   * over (i,j) entries in the output array.  However, to really get the full benefit of this
   * problem, we would encourage you to use cooperative fetching, shared memory register tiling, 
   * and other ideas covered in the class notes.  Note that unlike the tiled matmul function in
   * the CPU backend, here you should implement a single function that works across all size
   * matrices, whether or not they are a multiple of a tile size.  As with previous CUDA
   * implementations, this function here will largely just set up the kernel call, and you should
   * implement the logic in a separate MatmulKernel() call.
   * 
   *
   * Args:
   *   a: compact 2D array of size m x n
   *   b: comapct 2D array of size n x p
   *   out: compact 2D array of size m x p to write the output to
   *   M: rows of a / out
   *   N: columns of a / rows of b
   *   P: columns of b / out
   */</span>

  <span class="token comment">/// BEGIN YOUR SOLUTION</span>
  CudaDims dim <span class="token operator">=</span> <span class="token function">CudaOneDim</span><span class="token punctuation">(</span>M<span class="token punctuation">)</span><span class="token punctuation">;</span>
  MatmulKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dim<span class="token punctuation">.</span>grid<span class="token punctuation">,</span> dim<span class="token punctuation">.</span>block<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> b<span class="token punctuation">.</span>ptr<span class="token punctuation">,</span> out<span class="token operator">-></span>ptr<span class="token punctuation">,</span> M<span class="token punctuation">,</span> N<span class="token punctuation">,</span> P<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token comment">/// END YOUR SOLUTION</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Doraemonzzz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://www.doraemonzzz.com/2022/12/11/2022-12-11-Deep-Learning-Systems-HW3/">http://www.doraemonzzz.com/2022/12/11/2022-12-11-Deep-Learning-Systems-HW3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.doraemonzzz.com" target="_blank">Doraemonzzz</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/">深度学习系统</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/12/12/2022-12-12-Deep-Learning-Systems-HW4/"><img class="prev-cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Deep Learning Systems HW4</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/08/2022-12-8-GAMES101-HW8/"><img class="next-cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">GAMES101 HW8</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/10/16/2022-10-16-Deep-Learning-Systems-开发环境配置/" title="Deep Learning Systems 开发环境配置"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-16</div><div class="title">Deep Learning Systems 开发环境配置</div></div></a></div><div><a href="/2022/10/17/2022-10-17-Deep-Learning-Systems-HW0/" title="Deep Learning Systems HW0"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-17</div><div class="title">Deep Learning Systems HW0</div></div></a></div><div><a href="/2022/10/17/2022-10-17-Deep-Learning-Systems-HW1/" title="Deep Learning Systems HW1"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-17</div><div class="title">Deep Learning Systems HW1</div></div></a></div><div><a href="/2022/11/01/2022-11-1-Deep-Learning-Systems-HW2/" title="Deep Learning Systems HW2"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-01</div><div class="title">Deep Learning Systems HW2</div></div></a></div><div><a href="/2022/12/12/2022-12-12-Deep-Learning-Systems-HW4/" title="Deep Learning Systems HW4"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-12</div><div class="title">Deep Learning Systems HW4</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Livere</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNDcxOS8xMTI1Ng=="></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src= "/img/loading.gif" data-lazy-src="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Doraemonzzz</div><div class="author-info__description">个人博客，主要记录有关机器学习，数学以及计算机科学的笔记</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">786</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">79</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">32</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Doraemonzzz"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Doraemonzzz" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/doraemon_zzz@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://space.bilibili.com/291079982" target="_blank" title="Bilibili"><i class="iconfont icon-bilibili"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title=""><i class="fa fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">暂无公告</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E7%82%B9%E5%9B%9E%E9%A1%BE"><span class="toc-number">1.</span> <span class="toc-text">重点回顾</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-1-Python-array-operations"><span class="toc-number">2.</span> <span class="toc-text">Part 1: Python array operations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-2-CPU-Backend-Compact-and-setitem"><span class="toc-number">3.</span> <span class="toc-text">Part 2: CPU Backend - Compact and setitem</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-3-CPU-Backend-Elementwise-and-scalar-operations"><span class="toc-number">4.</span> <span class="toc-text">Part 3: CPU Backend - Elementwise and scalar operations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-4-CPU-Backend-Reductions"><span class="toc-number">5.</span> <span class="toc-text">Part 4: CPU Backend - Reductions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-5-CPU-Backend-Matrix-multiplication"><span class="toc-number">6.</span> <span class="toc-text">Part 5: CPU Backend - Matrix multiplication</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-6-CUDA-Backend-Compact-and-setitem"><span class="toc-number">7.</span> <span class="toc-text">Part 6: CUDA Backend - Compact and setitem</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-7-CUDA-Backend-Elementwise-and-scalar-operations"><span class="toc-number">8.</span> <span class="toc-text">Part 7: CUDA Backend - Elementwise and scalar operations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-8-CUDA-Backend-Reductions"><span class="toc-number">9.</span> <span class="toc-text">Part 8: CUDA Backend - Reductions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-9-CUDA-Backend-Matrix-multiplication"><span class="toc-number">10.</span> <span class="toc-text">Part 9: CUDA Backend - Matrix multiplication</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/01/06/2023-1-6-ECE408-Lecture-6-Generalized-Tiling-and-DRAM-Bandwidth/" title="ECE408 Lecture 6 Generalized Tiling and DRAM Bandwidth"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 6 Generalized Tiling and DRAM Bandwidth"/></a><div class="content"><a class="title" href="/2023/01/06/2023-1-6-ECE408-Lecture-6-Generalized-Tiling-and-DRAM-Bandwidth/" title="ECE408 Lecture 6 Generalized Tiling and DRAM Bandwidth">ECE408 Lecture 6 Generalized Tiling and DRAM Bandwidth</a><time datetime="2023-01-06T06:30:00.000Z" title="发表于 2023-01-06 14:30:00">2023-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/06/2023-1-6-ECE408-Lecture-4-CUDA-Memory-Model/" title="ECE408 Lecture 4 CUDA Memory Model"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 4 CUDA Memory Model"/></a><div class="content"><a class="title" href="/2023/01/06/2023-1-6-ECE408-Lecture-4-CUDA-Memory-Model/" title="ECE408 Lecture 4 CUDA Memory Model">ECE408 Lecture 4 CUDA Memory Model</a><time datetime="2023-01-06T03:07:00.000Z" title="发表于 2023-01-06 11:07:00">2023-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/06/2023-1-6-ECE408-Lecture-5-Locality-and-Tiled-Matrix-Multiply/" title="ECE408 Lecture 5 Locality and Tiled Matrix Multiply"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 5 Locality and Tiled Matrix Multiply"/></a><div class="content"><a class="title" href="/2023/01/06/2023-1-6-ECE408-Lecture-5-Locality-and-Tiled-Matrix-Multiply/" title="ECE408 Lecture 5 Locality and Tiled Matrix Multiply">ECE408 Lecture 5 Locality and Tiled Matrix Multiply</a><time datetime="2023-01-06T03:07:00.000Z" title="发表于 2023-01-06 11:07:00">2023-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/05/2023-1-5-ECE408-Lecture-3-CUDA-Parallel-Execution-Model/" title="ECE408 Lecture 3 CUDA Parallel Execution Model"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 3 CUDA Parallel Execution Model"/></a><div class="content"><a class="title" href="/2023/01/05/2023-1-5-ECE408-Lecture-3-CUDA-Parallel-Execution-Model/" title="ECE408 Lecture 3 CUDA Parallel Execution Model">ECE408 Lecture 3 CUDA Parallel Execution Model</a><time datetime="2023-01-05T09:03:00.000Z" title="发表于 2023-01-05 17:03:00">2023-01-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/05/2023-1-5-ECE408-Lecture-2-Introduction-to-Parallel-Computing-and-CUDA/" title="ECE408 Lecture 2 Introduction to Parallel Computing and CUDA"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 2 Introduction to Parallel Computing and CUDA"/></a><div class="content"><a class="title" href="/2023/01/05/2023-1-5-ECE408-Lecture-2-Introduction-to-Parallel-Computing-and-CUDA/" title="ECE408 Lecture 2 Introduction to Parallel Computing and CUDA">ECE408 Lecture 2 Introduction to Parallel Computing and CUDA</a><time datetime="2023-01-05T03:53:00.000Z" title="发表于 2023-01-05 11:53:00">2023-01-05</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2023 By Doraemonzzz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.25
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'IpnmxCW9CvYWIXbol5QXsegX-MdYXbMMI',
      appKey: 'w57DVCdbxcyB1TYYagMIMJIU',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Valine' === 'Livere' || !false) {
  if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>