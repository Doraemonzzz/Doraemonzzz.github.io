<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Deep Learning Systems HW4 | Doraemonzzz</title><meta name="keywords" content="深度学习系统"><meta name="author" content="Doraemonzzz"><meta name="copyright" content="Doraemonzzz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="这里回顾HW4，这次的主要内容是利用needle实现cnn和rnn。 课程主页：  https:&#x2F;&#x2F;dlsyscourse.org&#x2F; https:&#x2F;&#x2F;forum.dlsyscourse.org&#x2F; https:&#x2F;&#x2F;mugrade-online.dlsyscourse.org&#x2F;  参考资料：  https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~kriz&#x2F;cifar.html https:&#x2F;&#x2F;fo">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning Systems HW4">
<meta property="og:url" content="http://www.doraemonzzz.com/2022/12/12/2022-12-12-Deep-Learning-Systems-HW4/index.html">
<meta property="og:site_name" content="Doraemonzzz">
<meta property="og:description" content="这里回顾HW4，这次的主要内容是利用needle实现cnn和rnn。 课程主页：  https:&#x2F;&#x2F;dlsyscourse.org&#x2F; https:&#x2F;&#x2F;forum.dlsyscourse.org&#x2F; https:&#x2F;&#x2F;mugrade-online.dlsyscourse.org&#x2F;  参考资料：  https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~kriz&#x2F;cifar.html https:&#x2F;&#x2F;fo">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2022-12-12T14:05:00.000Z">
<meta property="article:modified_time" content="2022-12-14T11:59:52.197Z">
<meta property="article:author" content="Doraemonzzz">
<meta property="article:tag" content="深度学习系统">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true"><link rel="canonical" href="http://www.doraemonzzz.com/2022/12/12/2022-12-12-Deep-Learning-Systems-HW4/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6f00f37f957f0608abb8c571105456f0";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-G-RE4B1LKRZD"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-G-RE4B1LKRZD');
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"距离上次更新已经","messageNext":"天了，文章内容可能已经过时。"},
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Deep Learning Systems HW4',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-12-14 19:59:52'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/bilibili.css" media="defer" onload="this.media='all'"><meta name="google-site-verification" content="c4v-NmuUZRgl3cvtg9GKswryK1YLaPztd_5M-df5VNI" /><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Doraemonzzz" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src= "/img/loading.gif" data-lazy-src="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">786</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">79</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">32</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-chart-pie"></i><span> 博客统计</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/charts/"><i class="fa-fw far fa-chart-bar"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-area"></i><span> 访问统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/top/"><i class="fa-fw fab fa-hotjar"></i><span> 阅读排行榜</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Doraemonzzz</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-chart-pie"></i><span> 博客统计</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/charts/"><i class="fa-fw far fa-chart-bar"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-area"></i><span> 访问统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/top/"><i class="fa-fw fab fa-hotjar"></i><span> 阅读排行榜</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Deep Learning Systems HW4</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-12T14:05:00.000Z" title="发表于 2022-12-12 22:05:00">2022-12-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-14T11:59:52.197Z" title="更新于 2022-12-14 19:59:52">2022-12-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/">深度学习系统</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>43分钟</span></span><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/2022/12/12/2022-12-12-Deep-Learning-Systems-HW4/" data-flag-title="Deep Learning Systems HW4"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span class="leancloud-visitors-count"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2022/12/12/2022-12-12-Deep-Learning-Systems-HW4/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2022/12/12/2022-12-12-Deep-Learning-Systems-HW4/" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>这里回顾HW4，这次的主要内容是利用needle实现cnn和rnn。</p>
<p>课程主页：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://dlsyscourse.org/">https://dlsyscourse.org/</a></li>
<li><a target="_blank" rel="noopener" href="https://forum.dlsyscourse.org/">https://forum.dlsyscourse.org/</a></li>
<li><a target="_blank" rel="noopener" href="https://mugrade-online.dlsyscourse.org/">https://mugrade-online.dlsyscourse.org/</a></li>
</ul>
<p>参考资料：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></li>
<li><a target="_blank" rel="noopener" href="https://forum.dlsyscourse.org/t/test-ndarray-backend-with-test-optim-py-hw2/2821">https://forum.dlsyscourse.org/t/test-ndarray-backend-with-test-optim-py-hw2/2821</a></li>
<li><a target="_blank" rel="noopener" href="https://hideyukiinada.github.io/cnn_backprop_strides2.html">https://hideyukiinada.github.io/cnn_backprop_strides2.html</a></li>
<li><a target="_blank" rel="noopener" href="https://deeplearning.cs.cmu.edu/F21/document/recitation/Recitation5/CNN_Backprop_Recitation_5_F21.pdf">https://deeplearning.cs.cmu.edu/F21/document/recitation/Recitation5/CNN_Backprop_Recitation_5_F21.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html">https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html</a></li>
</ul>
<span id="more"></span>
<h2 id="Part-1-ND-Backend"><a href="#Part-1-ND-Backend" class="headerlink" title="Part 1: ND Backend"></a>Part 1: ND Backend</h2><p>注意点：</p>
<ul>
<li>需要将之前numpy部分删去；</li>
<li>broadcast_to的特殊处理；</li>
<li>summation的多维版本支持；</li>
<li>变换形状之前需要compact；</li>
</ul>
<p>代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">
<span class="token keyword">class</span> <span class="token class-name">PowerScalar</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Op raise a tensor to an (integer) power."""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> scalar<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>scalar <span class="token operator">=</span> scalar

    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">:</span> NDArray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> NDArray<span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token comment"># 避免标量情形</span>
        <span class="token keyword">return</span> array_api<span class="token punctuation">.</span>power<span class="token punctuation">(</span>a<span class="token punctuation">,</span> self<span class="token punctuation">.</span>scalar<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>inputs
        <span class="token comment"># nx^(n-1)</span>
        <span class="token keyword">return</span> out_grad <span class="token operator">*</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>scalar <span class="token operator">*</span> array_api<span class="token punctuation">.</span>power<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>scalar <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">power_scalar</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> scalar<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> PowerScalar<span class="token punctuation">(</span>scalar<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">EWiseDiv</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Op to element-wise divide two nodes."""</span>

    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> a <span class="token operator">/</span> b
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        lhs<span class="token punctuation">,</span> rhs <span class="token operator">=</span> node<span class="token punctuation">.</span>inputs
        <span class="token keyword">return</span> out_grad <span class="token operator">/</span> rhs<span class="token punctuation">,</span> <span class="token operator">-</span>out_grad <span class="token operator">*</span> lhs <span class="token operator">/</span> rhs <span class="token operator">/</span> rhs
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">divide</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> EWiseDiv<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">DivScalar</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> scalar<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>scalar <span class="token operator">=</span> scalar

    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token comment"># to keep dtype</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>a <span class="token operator">/</span> self<span class="token punctuation">.</span>scalar<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> out_grad <span class="token operator">/</span> self<span class="token punctuation">.</span>scalar
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">divide_scalar</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> scalar<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> DivScalar<span class="token punctuation">(</span>scalar<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">Transpose</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> axes<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>axes<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            axes <span class="token operator">=</span> <span class="token punctuation">(</span>axes<span class="token punctuation">,</span> <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>axes <span class="token operator">=</span> axes

    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">:</span>
            x<span class="token punctuation">,</span> y <span class="token operator">=</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            x<span class="token punctuation">,</span> y <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span>
        shape<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">,</span> shape<span class="token punctuation">[</span>y<span class="token punctuation">]</span> <span class="token operator">=</span> shape<span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">,</span> shape<span class="token punctuation">[</span>x<span class="token punctuation">]</span>
        
        <span class="token keyword">return</span> array_api<span class="token punctuation">.</span>permute<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">:</span>
            x<span class="token punctuation">,</span> y <span class="token operator">=</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            x<span class="token punctuation">,</span> y <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span>
        <span class="token keyword">return</span> transpose<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">transpose</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> Transpose<span class="token punctuation">(</span>axes<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">Reshape</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>shape <span class="token operator">=</span> shape

    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> array_api<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>a<span class="token punctuation">,</span> self<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>inputs
        <span class="token keyword">return</span> reshape<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">reshape</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> Reshape<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>

<span class="token comment">##### batch size = 1特殊处理</span>
<span class="token keyword">class</span> <span class="token class-name">BroadcastTo</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>shape <span class="token operator">=</span> shape

    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> a<span class="token punctuation">.</span>shape <span class="token operator">==</span> self<span class="token punctuation">.</span>shape<span class="token punctuation">:</span>
            <span class="token keyword">return</span> a
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> array_api<span class="token punctuation">.</span>broadcast_to<span class="token punctuation">(</span>a<span class="token punctuation">,</span> self<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>inputs
        <span class="token keyword">if</span> <span class="token builtin">input</span><span class="token punctuation">.</span>shape <span class="token operator">==</span> self<span class="token punctuation">.</span>shape<span class="token punctuation">:</span>
            <span class="token keyword">return</span> out_grad
        <span class="token comment"># 找到广播的维度</span>
        <span class="token comment"># input: scalar</span>
        n1 <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        n2 <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token comment"># 计算系数</span>
        c <span class="token operator">=</span> <span class="token number">1</span>
        <span class="token comment"># 扩充为相同维度大小</span>
        shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>n2 <span class="token operator">-</span> n1<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        axes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">reversed</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>n2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 不相等或填充为0</span>
            <span class="token keyword">if</span> shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">!=</span> self<span class="token punctuation">.</span>shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">or</span> i <span class="token operator">&lt;</span> n2 <span class="token operator">-</span> n1<span class="token punctuation">:</span>
                axes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i<span class="token punctuation">)</span>

        <span class="token keyword">return</span> reshape<span class="token punctuation">(</span>summation<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token builtin">tuple</span><span class="token punctuation">(</span>axes<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token comment">## END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">broadcast_to</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> BroadcastTo<span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    
<span class="token keyword">class</span> <span class="token class-name">Summation</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> axes<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>axes<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            axes <span class="token operator">=</span> <span class="token punctuation">(</span>axes<span class="token punctuation">,</span> <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>axes <span class="token operator">=</span> axes
        
    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        axes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 处理多维度求和</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>axes<span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            ori_axes <span class="token operator">=</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">,</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            ori_axes <span class="token operator">=</span> self<span class="token punctuation">.</span>axes
        <span class="token keyword">for</span> axis <span class="token keyword">in</span> ori_axes<span class="token punctuation">:</span>
            <span class="token comment"># 处理负数情形</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>axis<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> axis <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    axes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>axis <span class="token operator">+</span> n<span class="token punctuation">)</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    axes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>axis<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                axes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>axis<span class="token punctuation">)</span>
        <span class="token comment"># 降序排列</span>
        axes <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>axes<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> axis <span class="token keyword">in</span> axes<span class="token punctuation">:</span>
            a <span class="token operator">=</span> array_api<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> axis<span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> a
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>inputs
        <span class="token comment"># 使坐标为正并且从小到大排列</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>axes <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            axes <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">.</span>shape
            grad_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            axes <span class="token operator">=</span> self<span class="token punctuation">.</span>axes
            grad_shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>out_grad<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

        n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        new_axes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> x <span class="token keyword">in</span> axes<span class="token punctuation">:</span>
            <span class="token keyword">if</span> x <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">:</span>
                new_axes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                new_axes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x <span class="token operator">+</span> n<span class="token punctuation">)</span>
        new_axes <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>new_axes<span class="token punctuation">)</span>
        <span class="token comment"># 恢复grad_shape, 使grad_shape的维度和input.shape的维度相同</span>
        <span class="token keyword">for</span> axis <span class="token keyword">in</span> new_axes<span class="token punctuation">:</span>
            grad_shape<span class="token punctuation">.</span>insert<span class="token punctuation">(</span>axis<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> broadcast_to<span class="token punctuation">(</span>reshape<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> grad_shape<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>
        
<span class="token keyword">def</span> <span class="token function">summation</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> Summation<span class="token punctuation">(</span>axes<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">MatMul</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> array_api<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token comment"># (A, a, b), (B, b, c)</span>
        lhs<span class="token punctuation">,</span> rhs <span class="token operator">=</span> nodeclass Summation<span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> axes<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>axes<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            axes <span class="token operator">=</span> <span class="token punctuation">(</span>axes<span class="token punctuation">,</span> <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>axes <span class="token operator">=</span> axes
        
    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        axes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 处理多维度求和</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>axes<span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            ori_axes <span class="token operator">=</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">,</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            ori_axes <span class="token operator">=</span> self<span class="token punctuation">.</span>axes
        <span class="token keyword">for</span> axis <span class="token keyword">in</span> ori_axes<span class="token punctuation">:</span>
            <span class="token comment"># 处理负数情形</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>axis<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> axis <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    axes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>axis <span class="token operator">+</span> n<span class="token punctuation">)</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    axes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>axis<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                axes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>axis<span class="token punctuation">)</span>
        <span class="token comment"># 降序排列</span>
        axes <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>axes<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> axis <span class="token keyword">in</span> axes<span class="token punctuation">:</span>
            a <span class="token operator">=</span> array_api<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> axis<span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> a
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>inputs
        <span class="token comment"># 使坐标为正并且从小到大排列</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>axes <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            axes <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">.</span>shape
            grad_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            axes <span class="token operator">=</span> self<span class="token punctuation">.</span>axes
            grad_shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>out_grad<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

        n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        new_axes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> x <span class="token keyword">in</span> axes<span class="token punctuation">:</span>
            <span class="token keyword">if</span> x <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">:</span>
                new_axes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                new_axes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x <span class="token operator">+</span> n<span class="token punctuation">)</span>
        new_axes <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>new_axes<span class="token punctuation">)</span>
        <span class="token comment"># 恢复grad_shape, 使grad_shape的维度和input.shape的维度相同</span>
        <span class="token keyword">for</span> axis <span class="token keyword">in</span> new_axes<span class="token punctuation">:</span>
            grad_shape<span class="token punctuation">.</span>insert<span class="token punctuation">(</span>axis<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> broadcast_to<span class="token punctuation">(</span>reshape<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> grad_shape<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION.inputs</span>
        <span class="token comment"># out_grad: (C, a, c)</span>
        <span class="token comment"># (C, a, b)</span>
        lhs_grad <span class="token operator">=</span> matmul<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> transpose<span class="token punctuation">(</span>rhs<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># (C, b, c)</span>
        rhs_grad <span class="token operator">=</span> matmul<span class="token punctuation">(</span>transpose<span class="token punctuation">(</span>lhs<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> out_grad<span class="token punctuation">)</span>
        <span class="token comment"># 注意形状</span>
        n1 <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>out_grad<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        n2 <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>lhs<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        n3 <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>rhs<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">if</span> n1 <span class="token operator">></span> n2<span class="token punctuation">:</span>
            lhs_grad <span class="token operator">=</span> summation<span class="token punctuation">(</span>lhs_grad<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token builtin">tuple</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>n1 <span class="token operator">-</span> n2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> n1 <span class="token operator">></span> n3<span class="token punctuation">:</span>
            rhs_grad <span class="token operator">=</span> summation<span class="token punctuation">(</span>rhs_grad<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token builtin">tuple</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>n1 <span class="token operator">-</span> n3<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> lhs_grad<span class="token punctuation">,</span> rhs_grad
        <span class="token comment">### END YOUR SOLUTION</span>
        
 <span class="token keyword">def</span> <span class="token function">matmul</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> MatMul<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">Negate</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> <span class="token operator">-</span>a
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> <span class="token operator">-</span>out_grad
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">negate</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> Negate<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">Log</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> array_api<span class="token punctuation">.</span>log<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>inputs
        <span class="token keyword">return</span> out_grad <span class="token operator">/</span> <span class="token builtin">input</span>
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">log</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> Log<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">Exp</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> array_api<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>inputs
        <span class="token keyword">return</span> out_grad <span class="token operator">*</span> exp<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">exp</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> Exp<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    
<span class="token keyword">class</span> <span class="token class-name">ReLU</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> array_api<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>inputs
        input_relu <span class="token operator">=</span> relu<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">.</span>realize_cached_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> out_grad <span class="token operator">*</span> Tensor<span class="token punctuation">(</span>input_relu <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">,</span> device<span class="token operator">=</span>out_grad<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>out_grad<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">relu</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">LogSumExp</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> axes<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>axes<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            axes <span class="token operator">=</span> <span class="token punctuation">(</span>axes<span class="token punctuation">,</span> <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>axes <span class="token operator">=</span> axes

    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Z<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        z <span class="token operator">=</span> array_api<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>Z<span class="token punctuation">,</span> axis<span class="token operator">=</span>self<span class="token punctuation">.</span>axes<span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        z_broadcast <span class="token operator">=</span> array_api<span class="token punctuation">.</span>broadcast_to<span class="token punctuation">(</span>z<span class="token punctuation">,</span> Z<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        log_sum_exp <span class="token operator">=</span> array_api<span class="token punctuation">.</span>log<span class="token punctuation">(</span>array_api<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>array_api<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>Z <span class="token operator">-</span> z_broadcast<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span>self<span class="token punctuation">.</span>axes<span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> z

        new_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">:</span>
            l <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>Z<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
            <span class="token keyword">for</span> i<span class="token punctuation">,</span> n <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>Z<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>i <span class="token operator">-</span> l<span class="token punctuation">)</span> <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">)</span><span class="token punctuation">:</span>
                    new_shape<span class="token punctuation">.</span>append<span class="token punctuation">(</span>n<span class="token punctuation">)</span>
            log_sum_exp <span class="token operator">=</span> log_sum_exp<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>new_shape<span class="token punctuation">)</span><span class="token comment">#.astype(Z.dtype)</span>

        <span class="token keyword">return</span> log_sum_exp
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>inputs
        data <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">.</span>realize_cached_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
        z <span class="token operator">=</span> array_api<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> axis<span class="token operator">=</span>self<span class="token punctuation">.</span>axes<span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        z <span class="token operator">=</span> array_api<span class="token punctuation">.</span>broadcast_to<span class="token punctuation">(</span>z<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        e <span class="token operator">=</span> array_api<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>data <span class="token operator">-</span> z<span class="token punctuation">)</span>
        e_sum <span class="token operator">=</span> array_api<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>e<span class="token punctuation">,</span> axis<span class="token operator">=</span>self<span class="token punctuation">.</span>axes<span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        e_sum <span class="token operator">=</span> array_api<span class="token punctuation">.</span>broadcast_to<span class="token punctuation">(</span>e_sum<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        prob <span class="token operator">=</span> e <span class="token operator">/</span> e_sum
        new_shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token comment"># (a, b) -> (1, a, 1, b)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">:</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">:</span>
                new_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
            grad <span class="token operator">=</span> reshape<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> new_shape<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            grad <span class="token operator">=</span> out_grad
        
        <span class="token keyword">return</span> broadcast_to<span class="token punctuation">(</span>grad<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">*</span> Tensor<span class="token punctuation">(</span>prob<span class="token punctuation">,</span> dtype<span class="token operator">=</span>grad<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>grad<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">logsumexp</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> LogSumExp<span class="token punctuation">(</span>axes<span class="token operator">=</span>axes<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">Tanh</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> array_api<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> node<span class="token punctuation">.</span>inputs
        tmp <span class="token operator">=</span> tanh<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> out_grad <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> tmp <span class="token operator">*</span> tmp<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>
        
<span class="token keyword">def</span> <span class="token function">tanh</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">getitem</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> axises<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> axis <span class="token keyword">in</span> axises<span class="token punctuation">:</span>
        x <span class="token operator">=</span> make_tuple<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">[</span>axis<span class="token punctuation">]</span>
        
    <span class="token keyword">return</span> x

<span class="token keyword">class</span> <span class="token class-name">Stack</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> axis<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Concatenates a sequence of arrays along a new dimension.
        Parameters:
        axis - dimension to concatenate along
        All arrays need to be of the same size.
        """</span>
        self<span class="token punctuation">.</span>axis <span class="token operator">=</span> axis

    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span>
        shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        arg_shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        shape<span class="token punctuation">.</span>insert<span class="token punctuation">(</span>self<span class="token punctuation">.</span>axis<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
        new_arr <span class="token operator">=</span> array_api<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>shape<span class="token punctuation">,</span> dtype<span class="token operator">=</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        <span class="token comment"># 计算index</span>
        idxes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        m <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>arg_shape<span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
            idxes<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> arg_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        idxes<span class="token punctuation">.</span>insert<span class="token punctuation">(</span>self<span class="token punctuation">.</span>axis<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token comment"># 新形状</span>
        arg_shape<span class="token punctuation">.</span>insert<span class="token punctuation">(</span>self<span class="token punctuation">.</span>axis<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 赋值</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
            idxes<span class="token punctuation">[</span>self<span class="token punctuation">.</span>axis<span class="token punctuation">]</span> <span class="token operator">=</span> i
            new_arr<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">(</span>idxes<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> array_api<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>args<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> arg_shape<span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> new_arr
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> split<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> self<span class="token punctuation">.</span>axis<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">stack</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> axis<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> Stack<span class="token punctuation">(</span>axis<span class="token punctuation">)</span><span class="token punctuation">(</span>make_tuple<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
<span class="token keyword">class</span> <span class="token class-name">Split</span><span class="token punctuation">(</span>TensorTupleOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> axis<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Splits a tensor along an axis into a tuple of tensors.
        (The "inverse" of Stack)
        Parameters:
        axis - dimension to split
        """</span>
        self<span class="token punctuation">.</span>axis <span class="token operator">=</span> axis

    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token comment"># (5, 3, 5) -> [(5, 5), (5, 5), (5, 5)]</span>
        n <span class="token operator">=</span> A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span>self<span class="token punctuation">.</span>axis<span class="token punctuation">]</span>
        arg_shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        new_arr <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 计算index</span>
        idxes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        m <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>arg_shape<span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
            idxes<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> arg_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 新形状</span>
        new_shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">del</span> new_shape<span class="token punctuation">[</span>self<span class="token punctuation">.</span>axis<span class="token punctuation">]</span>

        <span class="token comment"># 赋值</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
            idxes<span class="token punctuation">[</span>self<span class="token punctuation">.</span>axis<span class="token punctuation">]</span> <span class="token operator">=</span> i
            data <span class="token operator">=</span> array_api<span class="token punctuation">.</span>array<span class="token punctuation">(</span>A<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">(</span>idxes<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>A<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>A<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            data <span class="token operator">=</span> array_api<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>data<span class="token punctuation">,</span> new_shape<span class="token punctuation">)</span>
            new_arr<span class="token punctuation">.</span>append<span class="token punctuation">(</span>data<span class="token punctuation">)</span>

        <span class="token keyword">return</span> new_arr
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> stack<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> self<span class="token punctuation">.</span>axis<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">split</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> axis<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> Split<span class="token punctuation">(</span>axis<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-2-CIFAR-10-dataset"><a href="#Part-2-CIFAR-10-dataset" class="headerlink" title="Part 2: CIFAR-10 dataset"></a>Part 2: CIFAR-10 dataset</h2><p>注意点：</p>
<ul>
<li>需要参考cifar10官网；</li>
<li>最后的结果需要除以255；</li>
</ul>
<p>代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">CIFAR10Dataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        base_folder<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span>
        train<span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">,</span>
        p<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.5</span><span class="token punctuation">,</span>
        transforms<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Parameters:
        base_folder - cifar-10-batches-py folder filepath
        train - bool, if True load training dataset, else load test dataset
        Divide pixel values by 255. so that images are in 0-1 range.
        Attributes:
        X - numpy array of images
        y - numpy array of labels
        """</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>transforms<span class="token punctuation">)</span>
        <span class="token keyword">if</span> train<span class="token punctuation">:</span>
            files <span class="token operator">=</span> <span class="token punctuation">[</span>
                <span class="token string">"data_batch_1"</span><span class="token punctuation">,</span>
                <span class="token string">"data_batch_2"</span><span class="token punctuation">,</span>
                <span class="token string">"data_batch_3"</span><span class="token punctuation">,</span>
                <span class="token string">"data_batch_4"</span><span class="token punctuation">,</span>
                <span class="token string">"data_batch_5"</span>
            <span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            files <span class="token operator">=</span> <span class="token punctuation">[</span>
                <span class="token string">"test_batch"</span>
            <span class="token punctuation">]</span>
        X <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> <span class="token builtin">file</span> <span class="token keyword">in</span> files<span class="token punctuation">:</span>
            data<span class="token punctuation">,</span> labels <span class="token operator">=</span> self<span class="token punctuation">.</span>unpickle<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>base_folder<span class="token punctuation">,</span> <span class="token builtin">file</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            X<span class="token punctuation">.</span>append<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
            y<span class="token punctuation">.</span>append<span class="token punctuation">(</span>labels<span class="token punctuation">)</span>
        X <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span>
        y <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>n <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>X <span class="token operator">=</span> X
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> y
        <span class="token comment">### END YOUR SOLUTION</span>
        
    <span class="token keyword">def</span> <span class="token function">unpickle</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">file</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token builtin">file</span><span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> fo<span class="token punctuation">:</span>
            <span class="token builtin">dict</span> <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>fo<span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'bytes'</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token string">b'data'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token string">b'labels'</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">object</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Returns the image, label at given index
        Image should be of shape (3, 32, 32)
        """</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>apply_transforms<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>y<span class="token punctuation">[</span>index<span class="token punctuation">]</span>
        
        <span class="token keyword">return</span> x<span class="token punctuation">,</span> y
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">int</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Returns the total number of examples in the dataset
        """</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>n
        <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-3-Convolutional-neural-network"><a href="#Part-3-Convolutional-neural-network" class="headerlink" title="Part 3: Convolutional neural network"></a>Part 3: Convolutional neural network</h2><h3 id="Padding-ndarrays"><a href="#Padding-ndarrays" class="headerlink" title="Padding ndarrays"></a>Padding ndarrays</h3><p>代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">pad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> axes<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Pad this ndarray by zeros by the specified amount in `axes`,
    which lists for _all_ axes the left and right padding amount, e.g.,
    axes = ( (0, 0), (1, 1), (0, 0)) pads the middle axis with a 0 on the left and right side.
    """</span>
    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    old_shape <span class="token operator">=</span> self<span class="token punctuation">.</span>_shape
    n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>axes<span class="token punctuation">)</span>
    new_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    idxs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        new_shape<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>old_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> axes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> axes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    arr <span class="token operator">=</span> self<span class="token punctuation">.</span>device<span class="token punctuation">.</span>full<span class="token punctuation">(</span>new_shape<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
    <span class="token comment"># 赋值</span>
    offset <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        offset <span class="token operator">+=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>arr<span class="token punctuation">.</span>strides<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> axes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>device<span class="token punctuation">.</span>ewise_setitem<span class="token punctuation">(</span>
        self<span class="token punctuation">.</span>_handle<span class="token punctuation">,</span>
        arr<span class="token punctuation">.</span>_handle<span class="token punctuation">,</span>
        old_shape<span class="token punctuation">,</span>
        arr<span class="token punctuation">.</span>_strides<span class="token punctuation">,</span>
        offset<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> arr
    <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Flipping-ndarrays-amp-FlipOp"><a href="#Flipping-ndarrays-amp-FlipOp" class="headerlink" title="Flipping ndarrays &amp; FlipOp"></a>Flipping ndarrays &amp; FlipOp</h3><p>注意点：</p>
<ul>
<li>通过变换stride以及offset实现功能；</li>
</ul>
<p>代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">flip</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> axes<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Flip this ndarray along the specified axes.
    Note: compact() before returning.
    """</span>
    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_strides<span class="token punctuation">)</span>
    new_strides <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_strides<span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> axes<span class="token punctuation">:</span>
        new_strides<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token operator">-</span><span class="token number">1</span>
    new_offset <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> i <span class="token keyword">in</span> axes<span class="token punctuation">:</span>
            new_offset <span class="token operator">+=</span> self<span class="token punctuation">.</span>_strides<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>
    arr <span class="token operator">=</span> NDArray<span class="token punctuation">.</span>make<span class="token punctuation">(</span>
        shape<span class="token operator">=</span>self<span class="token punctuation">.</span>_shape<span class="token punctuation">,</span> 
        strides<span class="token operator">=</span><span class="token builtin">tuple</span><span class="token punctuation">(</span>new_strides<span class="token punctuation">)</span><span class="token punctuation">,</span> 
        device<span class="token operator">=</span>self<span class="token punctuation">.</span>_device<span class="token punctuation">,</span> 
        handle<span class="token operator">=</span>self<span class="token punctuation">.</span>_handle<span class="token punctuation">,</span>
        offset<span class="token operator">=</span>new_offset<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> arr<span class="token punctuation">.</span>compact<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment">### END YOUR SOLUTION</span>
    
<span class="token keyword">class</span> <span class="token class-name">Flip</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> axes<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>axes <span class="token operator">=</span> axes

    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> array_api<span class="token punctuation">.</span>flip<span class="token punctuation">(</span>a<span class="token punctuation">,</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> flip<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">flip</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> axes<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> Flip<span class="token punctuation">(</span>axes<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Dilation"><a href="#Dilation" class="headerlink" title="Dilation"></a>Dilation</h3><p>注意点：</p>
<ul>
<li>重点是计算stride；</li>
</ul>
<p>代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Dilate</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> axes<span class="token punctuation">:</span> <span class="token builtin">tuple</span><span class="token punctuation">,</span> dilation<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>axes <span class="token operator">=</span> axes
        self<span class="token punctuation">.</span>dilation <span class="token operator">=</span> dilation

    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        old_shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        new_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>old_shape<span class="token punctuation">)</span>
        index <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> i <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">:</span>
                new_shape<span class="token punctuation">.</span>append<span class="token punctuation">(</span>old_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
                index<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">(</span>new_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                new_shape<span class="token punctuation">.</span>append<span class="token punctuation">(</span>old_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>dilation<span class="token punctuation">)</span><span class="token punctuation">)</span>
                index<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> new_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>dilation<span class="token punctuation">)</span><span class="token punctuation">)</span>
                
        res <span class="token operator">=</span> array_api<span class="token punctuation">.</span>full<span class="token punctuation">(</span>new_shape<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>a<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>a<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        res<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">(</span>index<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> a
        
        <span class="token keyword">return</span> res
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> undilate<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dilation<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">dilate</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> axes<span class="token punctuation">,</span> dilation<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> Dilate<span class="token punctuation">(</span>axes<span class="token punctuation">,</span> dilation<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    
<span class="token keyword">class</span> <span class="token class-name">UnDilate</span><span class="token punctuation">(</span>TensorOp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> axes<span class="token punctuation">:</span> <span class="token builtin">tuple</span><span class="token punctuation">,</span> dilation<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>axes <span class="token operator">=</span> axes
        self<span class="token punctuation">.</span>dilation <span class="token operator">=</span> dilation

    <span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        old_shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        new_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>old_shape<span class="token punctuation">)</span>
        index <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> i <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">:</span>
                new_shape<span class="token punctuation">.</span>append<span class="token punctuation">(</span>old_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
                index<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">(</span>new_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                new_shape<span class="token punctuation">.</span>append<span class="token punctuation">(</span>old_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>dilation<span class="token punctuation">)</span><span class="token punctuation">)</span>
                index<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> old_shape<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>dilation<span class="token punctuation">)</span><span class="token punctuation">)</span>
                
        res <span class="token operator">=</span> array_api<span class="token punctuation">.</span>full<span class="token punctuation">(</span>new_shape<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>a<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>a<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        res <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token builtin">tuple</span><span class="token punctuation">(</span>index<span class="token punctuation">)</span><span class="token punctuation">]</span>
        
        <span class="token keyword">return</span> res
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> dilate<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> self<span class="token punctuation">.</span>axes<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dilation<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">undilate</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> axes<span class="token punctuation">,</span> dilation<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> UnDilate<span class="token punctuation">(</span>axes<span class="token punctuation">,</span> dilation<span class="token punctuation">)</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Convolution-forward"><a href="#Convolution-forward" class="headerlink" title="Convolution forward"></a>Convolution forward</h3><p>注意点：</p>
<p>利用老师提供的如下公式即可：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>K<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>K<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">+=</span> Z<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>H<span class="token operator">-</span>K<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>j<span class="token punctuation">:</span>j<span class="token operator">+</span>W<span class="token operator">-</span>K<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> @ weight<span class="token punctuation">[</span>i<span class="token punctuation">,</span>j<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>修改的地方是stride，代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">compute</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> A<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    axes <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>padding<span class="token punctuation">,</span> self<span class="token punctuation">.</span>padding<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>padding<span class="token punctuation">,</span> self<span class="token punctuation">.</span>padding<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    A_pad <span class="token operator">=</span> array_api<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>A<span class="token punctuation">,</span> axes<span class="token punctuation">)</span>
    N<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> C_in <span class="token operator">=</span> A_pad<span class="token punctuation">.</span>shape
    K<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _<span class="token punctuation">,</span> C_out <span class="token operator">=</span> B<span class="token punctuation">.</span>shape
    H_out <span class="token operator">=</span> <span class="token punctuation">(</span>H <span class="token operator">-</span> K<span class="token punctuation">)</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>stride <span class="token operator">+</span> <span class="token number">1</span>
    W_out <span class="token operator">=</span> <span class="token punctuation">(</span>W <span class="token operator">-</span> K<span class="token punctuation">)</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>stride <span class="token operator">+</span> <span class="token number">1</span>
    out <span class="token operator">=</span> array_api<span class="token punctuation">.</span>full<span class="token punctuation">(</span>
        shape<span class="token operator">=</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> H_out<span class="token punctuation">,</span> W_out<span class="token punctuation">,</span> C_out<span class="token punctuation">)</span><span class="token punctuation">,</span>
        fill_value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> 
        dtype<span class="token operator">=</span>A<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> 
        device<span class="token operator">=</span>A<span class="token punctuation">.</span>device
    <span class="token punctuation">)</span>
    <span class="token comment"># for i in range(K):</span>
    <span class="token comment">#     for j in range(K):</span>
    <span class="token comment">#         out += Z[:,i:i+H-K+1,j:j+W-K+1,:] @ weight[i,j]</span>
    batch_index <span class="token operator">=</span> <span class="token builtin">slice</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span>
    feature_index1 <span class="token operator">=</span> <span class="token builtin">slice</span><span class="token punctuation">(</span>C_in<span class="token punctuation">)</span>
    feature_index2 <span class="token operator">=</span> <span class="token builtin">slice</span><span class="token punctuation">(</span>C_out<span class="token punctuation">)</span>
    n <span class="token operator">=</span> N <span class="token operator">*</span> H_out <span class="token operator">*</span> W_out
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>K<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>K<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 不要和dilate搞混</span>
            i_start <span class="token operator">=</span> i
            i_end <span class="token operator">=</span> i_start <span class="token operator">+</span> H_out <span class="token operator">*</span> self<span class="token punctuation">.</span>stride
            h_index <span class="token operator">=</span> <span class="token builtin">slice</span><span class="token punctuation">(</span>i_start<span class="token punctuation">,</span> i_end<span class="token punctuation">,</span> self<span class="token punctuation">.</span>stride<span class="token punctuation">)</span>
            j_start <span class="token operator">=</span> j
            j_end <span class="token operator">=</span> j_start <span class="token operator">+</span> W_out <span class="token operator">*</span> self<span class="token punctuation">.</span>stride
            w_index <span class="token operator">=</span> <span class="token builtin">slice</span><span class="token punctuation">(</span>j_start<span class="token punctuation">,</span> j_end<span class="token punctuation">,</span> self<span class="token punctuation">.</span>stride<span class="token punctuation">)</span>
            A1 <span class="token operator">=</span> A_pad<span class="token punctuation">[</span><span class="token punctuation">(</span>batch_index<span class="token punctuation">,</span> h_index<span class="token punctuation">,</span> w_index<span class="token punctuation">,</span> feature_index1<span class="token punctuation">)</span><span class="token punctuation">]</span>
            A2 <span class="token operator">=</span> array_api<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>A1<span class="token punctuation">,</span> <span class="token punctuation">(</span>n<span class="token punctuation">,</span> C_in<span class="token punctuation">)</span><span class="token punctuation">)</span>
            B1 <span class="token operator">=</span> B<span class="token punctuation">[</span><span class="token builtin">slice</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">slice</span><span class="token punctuation">(</span>j<span class="token punctuation">,</span> j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> feature_index1<span class="token punctuation">,</span> feature_index2<span class="token punctuation">]</span>
            B2 <span class="token operator">=</span> array_api<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B1<span class="token punctuation">,</span> <span class="token punctuation">(</span>C_in<span class="token punctuation">,</span> C_out<span class="token punctuation">)</span><span class="token punctuation">)</span>
            C2 <span class="token operator">=</span> array_api<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>A2<span class="token punctuation">,</span> B2<span class="token punctuation">)</span>
            C3 <span class="token operator">=</span> array_api<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>C2<span class="token punctuation">,</span> <span class="token punctuation">(</span>N<span class="token punctuation">,</span> H_out<span class="token punctuation">,</span> W_out<span class="token punctuation">,</span> C_out<span class="token punctuation">)</span><span class="token punctuation">)</span>
            out <span class="token operator">+=</span> C3

    <span class="token keyword">return</span> out
    <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Convolution-backward"><a href="#Convolution-backward" class="headerlink" title="Convolution backward"></a>Convolution backward</h3><p>反传要困难不少，主要是计算维度，参考了如下资料：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://deeplearning.cs.cmu.edu/F21/document/recitation/Recitation5/CNN_Backprop_Recitation_5_F21.pdf">https://deeplearning.cs.cmu.edu/F21/document/recitation/Recitation5/CNN_Backprop_Recitation_5_F21.pdf</a></li>
</ul>
<p>主要两个步骤：</p>
<ul>
<li>第一步：确定维度；</li>
<li>第二部：确定形状；</li>
</ul>
<p>首先是维度。</p>
<p>假设输入为的维度分别为：</p>
<ul>
<li>$A: (b,h_1,w_1,d), B:(k,k,d,e)$；</li>
</ul>
<p>输出的维度为：</p>
<ul>
<li>$O: (b,h_2,w_2,e)$</li>
</ul>
<p>现在反传的输入维度为：</p>
<ul>
<li>$dO: (b,h_2,w_2,e)$</li>
</ul>
<p>要得到$dA: (b, h_1, w_1, d)$的维度，应该计算$dO\star B.transpose(-1, -2)$，注意实际实现的时候需要对$B$ flip；</p>
<p>要得到$dB:(k, k, d, e)$，需要利用$dO$和$A$，需要调整$b$的位置，通过卷积消掉这个维度：</p>
<p>$(e, h_2, w_2, b), (h_1, w_1, b, d)\to (e, k,k, d)\to (k, k, d, e)$；</p>
<p>其次是形状，注意到这里$h=w$，所以后续讨论只使用$h$：</p>
<p>注意到：</p>
<script type="math/tex; mode=display">
h_2=\lfloor (h_1 -k+ 2p_1) / s\rfloor + 1</script><p>所以为了得到$dA$，第一步是dilate:</p>
<script type="math/tex; mode=display">
h_2 \to sh_2</script><p>然后通过下式子求解$p_2$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
s h_2 -k  + 2p_2 + 1= h_1, p_2 = (h_1 + k - sh_2 - 1) /2

\end{aligned}</script><p>这样就可以得到计算卷积时的padding。</p>
<p>为了得到$dB$，第一步依然是dilate:</p>
<script type="math/tex; mode=display">
h_2 \to sh_2</script><p>然后变换维度：</p>
<script type="math/tex; mode=display">
dA:(b, h_1, w_1, d)\to (h_1, w_1, b, d)\\
dO:(b,h_2,w_2,e)\to (e, h_2, w_2,b)</script><p>我们希望满足如下条件：</p>
<script type="math/tex; mode=display">
h_1+ 2p_3 +1 -sh_2 = k,p_3 = (k+sh_2 -h_1 - 1)//2</script><p>这样就可以得到计算卷积时的padding。</p>
<p>还有一些flip以及维度的细节，参考代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out_grad<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    <span class="token comment"># out_grad: bhwc2</span>
    out_grad_dilate <span class="token operator">=</span> dilate<span class="token punctuation">(</span>out_grad<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>stride <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token comment"># A: bhwc1, B: kkc1c2</span>
    A<span class="token punctuation">,</span> B <span class="token operator">=</span> node<span class="token punctuation">.</span>inputs
    A <span class="token operator">=</span> A<span class="token punctuation">.</span>realize_cached_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
    B <span class="token operator">=</span> B<span class="token punctuation">.</span>realize_cached_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
    b <span class="token operator">=</span> A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    h <span class="token operator">=</span> A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    k <span class="token operator">=</span> B<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token comment"># bhwc1 -> c1hwb</span>
    A <span class="token operator">=</span> array_api<span class="token punctuation">.</span>permute<span class="token punctuation">(</span>A<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># kkc1c2 -> kkc1c2</span>
    B <span class="token operator">=</span> array_api<span class="token punctuation">.</span>flip<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># kkc1c2 -> kkc2c1</span>
    B <span class="token operator">=</span> array_api<span class="token punctuation">.</span>permute<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    tmp <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>h <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>padding <span class="token operator">-</span> k<span class="token punctuation">)</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>stride <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>stride
    <span class="token comment"># pad</span>
    p_B <span class="token operator">=</span> <span class="token punctuation">(</span>h <span class="token operator">+</span> k <span class="token operator">-</span> tmp <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span>        
    p_A <span class="token operator">=</span> <span class="token punctuation">(</span>k <span class="token operator">+</span> tmp <span class="token operator">-</span> h <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span>
    <span class="token comment"># bhwc2, kkc2c1 -> bhwc1</span>
    grad_A <span class="token operator">=</span> conv<span class="token punctuation">(</span>out_grad_dilate<span class="token punctuation">,</span> Tensor<span class="token punctuation">(</span>B<span class="token punctuation">,</span> dtype<span class="token operator">=</span>out_grad<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>out_grad<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span>p_B<span class="token punctuation">)</span>
    <span class="token comment"># bhwc2 -> whbc2 -> hwbc2: out_grad_dilate.transpose((0, 2)).transpose((0, 1))</span>
    <span class="token comment"># c1hwb, hwbc2 -> c1hwc2</span>
    grad_B <span class="token operator">=</span> conv<span class="token punctuation">(</span>Tensor<span class="token punctuation">(</span>A<span class="token punctuation">,</span> dtype<span class="token operator">=</span>out_grad<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>out_grad<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> out_grad_dilate<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span>p_A<span class="token punctuation">)</span>
    <span class="token comment"># c1hwc2 -> whc1c2 -> hwc1c2</span>
    grad_B <span class="token operator">=</span> grad_B<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> grad_A<span class="token punctuation">,</span> grad_B
    <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="nn-Conv"><a href="#nn-Conv" class="headerlink" title="nn.Conv"></a>nn.Conv</h3><p>kaiming_uniform代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">kaiming_uniform</span><span class="token punctuation">(</span>fan_in<span class="token punctuation">,</span> fan_out<span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">assert</span> nonlinearity <span class="token operator">==</span> <span class="token string">"relu"</span><span class="token punctuation">,</span> <span class="token string">"Only relu supported currently"</span>
    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    gain <span class="token operator">=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
    bound <span class="token operator">=</span> gain <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">3</span> <span class="token operator">/</span> fan_in<span class="token punctuation">)</span>

    shape <span class="token operator">=</span> kwargs<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"shape"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> shape <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        shape <span class="token operator">=</span> <span class="token punctuation">(</span>fan_in<span class="token punctuation">,</span> fan_out<span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> rand<span class="token punctuation">(</span><span class="token operator">*</span>shape<span class="token punctuation">,</span> low<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> high<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> bound
    <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>conv代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Conv</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Multi-channel 2D convolutional layer
    IMPORTANT: Accepts inputs in NCHW format, outputs also in NCHW format
    Only supports padding=same
    No grouped convolution or dilation
    Only supports square kernels
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            kernel_size <span class="token operator">=</span> kernel_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>stride<span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            stride <span class="token operator">=</span> stride<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> in_channels
        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> out_channels
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>stride <span class="token operator">=</span> stride

        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        self<span class="token punctuation">.</span>padding <span class="token operator">=</span> self<span class="token punctuation">.</span>kernel_size <span class="token operator">//</span> <span class="token number">2</span>
        <span class="token comment"># kernel</span>
        fan_in <span class="token operator">=</span> self<span class="token punctuation">.</span>in_channels <span class="token operator">*</span> self<span class="token punctuation">.</span>kernel_size <span class="token operator">**</span> <span class="token number">2</span>
        fan_out <span class="token operator">=</span> self<span class="token punctuation">.</span>out_channels <span class="token operator">*</span> self<span class="token punctuation">.</span>kernel_size <span class="token operator">**</span> <span class="token number">2</span>
        shape <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>kernel_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kernel_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>in_channels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>out_channels<span class="token punctuation">)</span>
        weight <span class="token operator">=</span> init<span class="token punctuation">.</span>kaiming_uniform<span class="token punctuation">(</span>fan_in<span class="token punctuation">,</span> fan_out<span class="token punctuation">,</span> shape<span class="token operator">=</span>shape<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>weight<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        <span class="token comment"># bias</span>
        self<span class="token punctuation">.</span>use_bias <span class="token operator">=</span> bias
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_bias<span class="token punctuation">:</span>
            k <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channels <span class="token operator">*</span> self<span class="token punctuation">.</span>kernel_size <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">0.5</span>
            b <span class="token operator">=</span> ops<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>init<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span>self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>bias <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>b<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tensor<span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token comment"># NCHW -> NHCW -> NHWC</span>
        x <span class="token operator">=</span> ops<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> ops<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># NHWC</span>
        output <span class="token operator">=</span> ops<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>stride<span class="token punctuation">,</span> self<span class="token punctuation">.</span>padding<span class="token punctuation">)</span>
        <span class="token comment"># 1C -> 111C -> NHWC</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_bias<span class="token punctuation">:</span>
            bias <span class="token operator">=</span> ops<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">)</span> 
            bias <span class="token operator">=</span> ops<span class="token punctuation">.</span>broadcast_to<span class="token punctuation">(</span>bias<span class="token punctuation">,</span> output<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
            output <span class="token operator">+=</span> bias
        <span class="token comment"># NHWC-> NHCW -> NCHW</span>
        output <span class="token operator">=</span> ops<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>output<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> ops<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>output<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> output
        <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="ResNet9"><a href="#ResNet9" class="headerlink" title="ResNet9"></a>ResNet9</h3><p>代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ConvBN</span><span class="token punctuation">(</span>ndl<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> k<span class="token punctuation">,</span> s<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">### BEGIN YOUR SOLUTION ###</span>
        self<span class="token punctuation">.</span>module <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> k<span class="token punctuation">,</span> s<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>b<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">class</span> <span class="token class-name">ResNet9</span><span class="token punctuation">(</span>ndl<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">### BEGIN YOUR SOLUTION </span>
        self<span class="token punctuation">.</span>ConvBN0 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> 
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ConvBN1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Residual<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
                nn<span class="token punctuation">.</span>Conv<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> 
                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                nn<span class="token punctuation">.</span>Conv<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> 
                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ConvBN2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> 
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> 
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ConvBN3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Residual<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
                nn<span class="token punctuation">.</span>Conv<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> 
                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                nn<span class="token punctuation">.</span>Conv<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> 
                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>Linear0 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> 
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ConvBN0<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ConvBN1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ConvBN2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ConvBN3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>Linear0<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
        <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-4-Recurrent-neural-network"><a href="#Part-4-Recurrent-neural-network" class="headerlink" title="Part 4: Recurrent neural network"></a>Part 4: Recurrent neural network</h2><p>注意点：</p>
<ul>
<li>RNN第一层的input_size；</li>
<li>多层rnn的便利顺序为先遍历层，然后遍历时间；</li>
</ul>
<p>代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">RNNCell</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Applies an RNN cell with tanh or ReLU nonlinearity.

        Parameters:
        input_size: The number of expected features in the input X
        hidden_size: The number of features in the hidden state h
        bias: If False, then the layer does not use bias weights
        nonlinearity: The non-linearity to use. Can be either 'tanh' or 'relu'.

        Variables:
        W_ih: The learnable input-hidden weights of shape (input_size, hidden_size).
        W_hh: The learnable hidden-hidden weights of shape (hidden_size, hidden_size).
        bias_ih: The learnable input-hidden bias of shape (hidden_size,).
        bias_hh: The learnable hidden-hidden bias of shape (hidden_size,).

        Weights and biases are initialized from U(-sqrt(k), sqrt(k)) where k = 1/hidden_size
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        self<span class="token punctuation">.</span>use_bias <span class="token operator">=</span> bias
        self<span class="token punctuation">.</span>input_size <span class="token operator">=</span> input_size
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        k <span class="token operator">=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> hidden_size<span class="token punctuation">)</span>
        w1 <span class="token operator">=</span> init<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_ih <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>w1<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        w2 <span class="token operator">=</span> init<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_hh <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>w2<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_bias<span class="token punctuation">:</span>
            b1 <span class="token operator">=</span> init<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>bias_ih <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>b1<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
            b2 <span class="token operator">=</span> init<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>bias_hh <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>b2<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        <span class="token keyword">if</span> nonlinearity <span class="token operator">==</span> <span class="token string">"tanh"</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>f <span class="token operator">=</span> Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>f <span class="token operator">=</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> h<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Inputs:
        X of shape (bs, input_size): Tensor containing input features
        h of shape (bs, hidden_size): Tensor containing the initial hidden state
            for each element in the batch. Defaults to zero if not provided.

        Outputs:
        h' of shape (bs, hidden_size): Tensor contianing the next hidden state
            for each element in the batch.
        """</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        bs <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> h <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            h <span class="token operator">=</span> Tensor<span class="token punctuation">(</span>init<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>X<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        tmp <span class="token operator">=</span> ops<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W_ih<span class="token punctuation">)</span> <span class="token operator">+</span> ops<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>h<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W_hh<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_bias<span class="token punctuation">:</span>
            tmp <span class="token operator">+=</span> ops<span class="token punctuation">.</span>broadcast_to<span class="token punctuation">(</span>ops<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias_ih<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tmp<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">+</span> \
                   ops<span class="token punctuation">.</span>broadcast_to<span class="token punctuation">(</span>ops<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias_hh<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tmp<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>f<span class="token punctuation">(</span>tmp<span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> h
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">class</span> <span class="token class-name">RNN</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Applies a multi-layer RNN with tanh or ReLU non-linearity to an input sequence.

        Parameters:
        input_size - The number of expected features in the input x
        hidden_size - The number of features in the hidden state h
        num_layers - Number of recurrent layers.
        nonlinearity - The non-linearity to use. Can be either 'tanh' or 'relu'.
        bias - If False, then the layer does not use bias weights.

        Variables:
        rnn_cells[k].W_ih: The learnable input-hidden weights of the k-th layer,
            of shape (input_size, hidden_size) for k=0. Otherwise the shape is
            (hidden_size, hidden_size).
        rnn_cells[k].W_hh: The learnable hidden-hidden weights of the k-th layer,
            of shape (hidden_size, hidden_size).
        rnn_cells[k].bias_ih: The learnable input-hidden bias of the k-th layer,
            of shape (hidden_size,).
        rnn_cells[k].bias_hh: The learnable hidden-hidden bias of the k-th layer,
            of shape (hidden_size,).
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers
        self<span class="token punctuation">.</span>input_size <span class="token operator">=</span> input_size
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        rnn_cells <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                d <span class="token operator">=</span> input_size
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                d <span class="token operator">=</span> hidden_size
            rnn_cells<span class="token punctuation">.</span>append<span class="token punctuation">(</span>RNNCell<span class="token punctuation">(</span>d<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> bias<span class="token punctuation">,</span> nonlinearity<span class="token punctuation">,</span> device<span class="token punctuation">,</span> dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn_cells <span class="token operator">=</span> rnn_cells
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> h0<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Inputs:
        X of shape (seq_len, bs, input_size) containing the features of the input sequence.
        h_0 of shape (num_layers, bs, hidden_size) containing the initial
            hidden state for each element in the batch. Defaults to zeros if not provided.

        Outputs
        output of shape (seq_len, bs, hidden_size) containing the output features
            (h_t) from the last layer of the RNN, for each t.
        h_n of shape (num_layers, bs, hidden_size) containing the final hidden state for each element in the batch.
        """</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        output <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        n <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        bs <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># (l, b, e)</span>
        <span class="token keyword">if</span> h0 <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            h0 <span class="token operator">=</span> Tensor<span class="token punctuation">(</span>init<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> bs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>X<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        <span class="token comment"># (l, b, e) -> [(b, e), ... , (b, e)]</span>
        h <span class="token operator">=</span> ops<span class="token punctuation">.</span>split<span class="token punctuation">(</span>h0<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token comment"># (n, b, e) -> [(b, e), ... , (b, e)]</span>
        X_split <span class="token operator">=</span> ops<span class="token punctuation">.</span>split<span class="token punctuation">(</span>X<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        h_out <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            h_state <span class="token operator">=</span> h<span class="token punctuation">[</span>j<span class="token punctuation">]</span>
            X_state <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
                h_state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn_cells<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">(</span>X_split<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> h_state<span class="token punctuation">)</span>
                X_state<span class="token punctuation">.</span>append<span class="token punctuation">(</span>h_state<span class="token punctuation">)</span>
            h_out<span class="token punctuation">.</span>append<span class="token punctuation">(</span>h_state<span class="token punctuation">)</span>
            X_split <span class="token operator">=</span> X_state
        
        <span class="token keyword">return</span> ops<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>X_split<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> ops<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>h_out<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-5-Long-short-term-memory-network"><a href="#Part-5-Long-short-term-memory-network" class="headerlink" title="Part 5: Long short-term memory network"></a>Part 5: Long short-term memory network</h2><p>注意点：</p>
<ul>
<li>LSTM第一层的input_size；</li>
<li>多层rnn的便利顺序为先遍历层，然后遍历时间；</li>
</ul>
<p>代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Sigmoid</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tensor<span class="token punctuation">:</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token keyword">return</span> ops<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>ops<span class="token punctuation">.</span>log<span class="token punctuation">(</span>ops<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span>ops<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>
        
 <span class="token keyword">class</span> <span class="token class-name">LSTMCell</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        A long short-term memory (LSTM) cell.

        Parameters:
        input_size - The number of expected features in the input X
        hidden_size - The number of features in the hidden state h
        bias - If False, then the layer does not use bias weights

        Variables:
        W_ih - The learnable input-hidden weights, of shape (input_size, 4*hidden_size).
        W_hh - The learnable hidden-hidden weights, of shape (hidden_size, 4*hidden_size).
        bias_ih - The learnable input-hidden bias, of shape (4*hidden_size,).
        bias_hh - The learnable hidden-hidden bias, of shape (4*hidden_size,).

        Weights and biases are initialized from U(-sqrt(k), sqrt(k)) where k = 1/hidden_size
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        self<span class="token punctuation">.</span>use_bias <span class="token operator">=</span> bias
        self<span class="token punctuation">.</span>input_size <span class="token operator">=</span> input_size
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        k <span class="token operator">=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> hidden_size<span class="token punctuation">)</span>
        w1 <span class="token operator">=</span> init<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> <span class="token number">4</span><span class="token operator">*</span>hidden_size<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_ih <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>w1<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        w2 <span class="token operator">=</span> init<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> <span class="token number">4</span><span class="token operator">*</span>hidden_size<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_hh <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>w2<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_bias<span class="token punctuation">:</span>
            b1 <span class="token operator">=</span> init<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token operator">*</span>hidden_size<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>bias_ih <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>b1<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
            b2 <span class="token operator">=</span> init<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token operator">*</span>hidden_size<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>bias_hh <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>b2<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sigma <span class="token operator">=</span> Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f <span class="token operator">=</span> Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>


    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> h<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Inputs: X, h
        X of shape (batch, input_size): Tensor containing input features
        h, tuple of (h0, c0), with
            h0 of shape (bs, hidden_size): Tensor containing the initial hidden state
                for each element in the batch. Defaults to zero if not provided.
            c0 of shape (bs, hidden_size): Tensor containing the initial cell state
                for each element in the batch. Defaults to zero if not provided.

        Outputs: (h', c')
        h' of shape (bs, hidden_size): Tensor containing the next hidden state for each
            element in the batch.
        c' of shape (bs, hidden_size): Tensor containing the next cell state for each
            element in the batch.
        """</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        bs <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> h <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            h0 <span class="token operator">=</span> Tensor<span class="token punctuation">(</span>init<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>X<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
            c0 <span class="token operator">=</span> Tensor<span class="token punctuation">(</span>init<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>X<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            h0<span class="token punctuation">,</span> c0 <span class="token operator">=</span> h
        <span class="token comment"># bs, hidden_size * 4</span>
        tmp <span class="token operator">=</span> ops<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W_ih<span class="token punctuation">)</span> <span class="token operator">+</span> ops<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>h0<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W_hh<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_bias<span class="token punctuation">:</span>
            tmp <span class="token operator">+=</span> ops<span class="token punctuation">.</span>broadcast_to<span class="token punctuation">(</span>ops<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias_ih<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tmp<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">+</span> \
                   ops<span class="token punctuation">.</span>broadcast_to<span class="token punctuation">(</span>ops<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias_hh<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tmp<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        tmp <span class="token operator">=</span> ops<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>tmp<span class="token punctuation">,</span> <span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
        i<span class="token punctuation">,</span> f<span class="token punctuation">,</span> g<span class="token punctuation">,</span> o <span class="token operator">=</span> ops<span class="token punctuation">.</span>split<span class="token punctuation">(</span>tmp<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        i <span class="token operator">=</span> self<span class="token punctuation">.</span>sigma<span class="token punctuation">(</span>i<span class="token punctuation">)</span>
        f <span class="token operator">=</span> self<span class="token punctuation">.</span>sigma<span class="token punctuation">(</span>f<span class="token punctuation">)</span>
        g <span class="token operator">=</span> self<span class="token punctuation">.</span>f<span class="token punctuation">(</span>g<span class="token punctuation">)</span>
        o <span class="token operator">=</span> self<span class="token punctuation">.</span>sigma<span class="token punctuation">(</span>o<span class="token punctuation">)</span>
        
        c <span class="token operator">=</span> f <span class="token operator">*</span> c0 <span class="token operator">+</span> i <span class="token operator">*</span> g
        h <span class="token operator">=</span> o <span class="token operator">*</span> self<span class="token punctuation">.</span>f<span class="token punctuation">(</span>c<span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> h<span class="token punctuation">,</span> c
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">class</span> <span class="token class-name">LSTM</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token triple-quoted-string string">"""
        Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.

        Parameters:
        input_size - The number of expected features in the input x
        hidden_size - The number of features in the hidden state h
        num_layers - Number of recurrent layers.
        bias - If False, then the layer does not use bias weights.

        Variables:
        lstm_cells[k].W_ih: The learnable input-hidden weights of the k-th layer,
            of shape (input_size, 4*hidden_size) for k=0. Otherwise the shape is
            (hidden_size, 4*hidden_size).
        lstm_cells[k].W_hh: The learnable hidden-hidden weights of the k-th layer,
            of shape (hidden_size, 4*hidden_size).
        lstm_cells[k].bias_ih: The learnable input-hidden bias of the k-th layer,
            of shape (4*hidden_size,).
        lstm_cells[k].bias_hh: The learnable hidden-hidden bias of the k-th layer,
            of shape (4*hidden_size,).
        """</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers
        self<span class="token punctuation">.</span>input_size <span class="token operator">=</span> input_size
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        lstm_cells <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                d <span class="token operator">=</span> input_size
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                d <span class="token operator">=</span> hidden_size
            lstm_cells<span class="token punctuation">.</span>append<span class="token punctuation">(</span>LSTMCell<span class="token punctuation">(</span>d<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> bias<span class="token punctuation">,</span> device<span class="token punctuation">,</span> dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lstm_cells <span class="token operator">=</span> lstm_cells
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> h<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Inputs: X, h
        X of shape (seq_len, bs, input_size) containing the features of the input sequence.
        h, tuple of (h0, c0) with
            h_0 of shape (num_layers, bs, hidden_size) containing the initial
                hidden state for each element in the batch. Defaults to zeros if not provided.
            c0 of shape (num_layers, bs, hidden_size) containing the initial
                hidden cell state for each element in the batch. Defaults to zeros if not provided.

        Outputs: (output, (h_n, c_n))
        output of shape (seq_len, bs, hidden_size) containing the output features
            (h_t) from the last layer of the LSTM, for each t.
        tuple of (h_n, c_n) with
            h_n of shape (num_layers, bs, hidden_size) containing the final hidden state for each element in the batch.
            h_n of shape (num_layers, bs, hidden_size) containing the final hidden cell state for each element in the batch.
        """</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        output <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        n <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        bs <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># (l, b, e)</span>
        <span class="token keyword">if</span> h <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            h0 <span class="token operator">=</span> Tensor<span class="token punctuation">(</span>init<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> bs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>X<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
            c0 <span class="token operator">=</span> Tensor<span class="token punctuation">(</span>init<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> bs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>X<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>X<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            h0<span class="token punctuation">,</span> c0 <span class="token operator">=</span> h
        <span class="token comment"># (l, b, e) -> [(b, e), ... , (b, e)]</span>
        h <span class="token operator">=</span> ops<span class="token punctuation">.</span>split<span class="token punctuation">(</span>h0<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        c <span class="token operator">=</span> ops<span class="token punctuation">.</span>split<span class="token punctuation">(</span>c0<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token comment"># (n, b, e) -> [(b, e), ... , (b, e)]</span>
        X_split <span class="token operator">=</span> ops<span class="token punctuation">.</span>split<span class="token punctuation">(</span>X<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        
        h_out <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        c_out <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            h_state <span class="token operator">=</span> h<span class="token punctuation">[</span>j<span class="token punctuation">]</span>
            c_state <span class="token operator">=</span> c<span class="token punctuation">[</span>j<span class="token punctuation">]</span>
            X_state <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
                h_state<span class="token punctuation">,</span> c_state <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm_cells<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">(</span>X_split<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>h_state<span class="token punctuation">,</span> c_state<span class="token punctuation">)</span><span class="token punctuation">)</span>
                X_state<span class="token punctuation">.</span>append<span class="token punctuation">(</span>h_state<span class="token punctuation">)</span>
            h_out<span class="token punctuation">.</span>append<span class="token punctuation">(</span>h_state<span class="token punctuation">)</span>
            c_out<span class="token punctuation">.</span>append<span class="token punctuation">(</span>c_state<span class="token punctuation">)</span>
            X_split <span class="token operator">=</span> X_state
        
        <span class="token keyword">return</span> ops<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>X_split<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>ops<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>h_out<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> ops<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>c_out<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-6-Penn-Treebank-dataset"><a href="#Part-6-Penn-Treebank-dataset" class="headerlink" title="Part 6: Penn Treebank dataset"></a>Part 6: Penn Treebank dataset</h2><p>注意点：</p>
<ul>
<li>每句话最后要添加<code>&lt;eos&gt;</code>；</li>
<li><code>batchify</code>时注意维度；</li>
</ul>
<p>代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Corpus</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Creates corpus from train, and test txt files.
    """</span>
    eos <span class="token operator">=</span> <span class="token string">"&lt;eos>"</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> base_dir<span class="token punctuation">,</span> max_lines<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>dictionary <span class="token operator">=</span> Dictionary<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>train <span class="token operator">=</span> self<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>base_dir<span class="token punctuation">,</span> <span class="token string">'train.txt'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> max_lines<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>test <span class="token operator">=</span> self<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>base_dir<span class="token punctuation">,</span> <span class="token string">'test.txt'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> max_lines<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">tokenize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> path<span class="token punctuation">,</span> max_lines<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Input:
        path - path to text file
        max_lines - maximum number of lines to read in
        Tokenizes a text file, first adding each word in the file to the dictionary,
        and then tokenizing the text file to a list of IDs. When adding words to the
        dictionary (and tokenizing the file content) '&lt;eos>' should be appended to
        the end of each line in order to properly account for the end of the sentence.
        Output:
        ids: List of ids
        """</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>path<span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            <span class="token keyword">for</span> i<span class="token punctuation">,</span> sentence <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> i <span class="token operator">==</span> max_lines<span class="token punctuation">:</span>
                    <span class="token keyword">break</span>
                <span class="token keyword">for</span> word <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token builtin">id</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>dictionary<span class="token punctuation">.</span>add_word<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
                    ids<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">id</span><span class="token punctuation">)</span>
                <span class="token comment"># add &lt;eos></span>
                <span class="token builtin">id</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>dictionary<span class="token punctuation">.</span>add_word<span class="token punctuation">(</span>Corpus<span class="token punctuation">.</span>eos<span class="token punctuation">)</span>
                ids<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">id</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> ids
        <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">batchify</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> device<span class="token punctuation">,</span> dtype<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Starting from sequential data, batchify arranges the dataset into columns.
    For instance, with the alphabet as the sequence and batch size 4, we'd get
    ┌ a g m s ┐
    │ b h n t │
    │ c i o u │
    │ d j p v │
    │ e k q w │
    └ f l r x ┘.
    These columns are treated as independent by the model, which means that the
    dependence of e. g. 'g' on 'f' cannot be learned, but allows more efficient
    batch processing.
    If the data cannot be evenly divided by the batch size, trim off the remainder.
    Returns the data as a numpy array of shape (nbatch, batch_size).
    """</span>
    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>
    m <span class="token operator">=</span> n <span class="token operator">//</span> batch_size <span class="token operator">*</span> batch_size
    data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span>m<span class="token punctuation">]</span>
    <span class="token comment"># [1, 2, 3, 4, 5, 6] -> [[1, 2], [3, 4], [5, 6]] -> [[1, 3, 5], [2, 4, 6]]</span>
    array <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> array
    <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">get_batch</span><span class="token punctuation">(</span>batches<span class="token punctuation">,</span> i<span class="token punctuation">,</span> bptt<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    get_batch subdivides the source data into chunks of length bptt.
    If source is equal to the example output of the batchify function, with
    a bptt-limit of 2, we'd get the following two Variables for i = 0:
    ┌ a g m s ┐ ┌ b h n t ┐
    └ b h n t ┘ └ c i o u ┘
    Note that despite the name of the function, the subdivison of data is not
    done along the batch dimension (i.e. dimension 1), since that was handled
    by the batchify function. The chunks are along dimension 0, corresponding
    to the seq_len dimension in the LSTM or RNN.
    Inputs:
    batches - numpy array returned from batchify function
    i - index
    bptt - Sequence length
    Returns:
    data - Tensor of shape (bptt, bs) with cached data as NDArray
    target - Tensor of shape (bptt*bs,) with cached data as NDArray
    """</span>
    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    data <span class="token operator">=</span> batches<span class="token punctuation">[</span>i<span class="token punctuation">:</span> i <span class="token operator">+</span> bptt<span class="token punctuation">]</span>
    target <span class="token operator">=</span> batches<span class="token punctuation">[</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span> i <span class="token operator">+</span> bptt <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span>
    n <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> target<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span>
    target <span class="token operator">=</span> target<span class="token punctuation">[</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> Tensor<span class="token punctuation">(</span>data<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> Tensor<span class="token punctuation">(</span>target<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
    <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Part-7-Training-a-word-level-language-model"><a href="#Part-7-Training-a-word-level-language-model" class="headerlink" title="Part 7: Training a word-level language model"></a>Part 7: Training a word-level language model</h2><p>代码：</p>
<p>nn.py：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Embedding</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_embeddings<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token triple-quoted-string string">"""
        Maps one-hot word vectors from a dictionary of fixed size to embeddings.

        Parameters:
        num_embeddings (int) - Size of the dictionary
        embedding_dim (int) - The size of each embedding vector

        Variables:
        weight - The learnable weights of shape (num_embeddings, embedding_dim)
            initialized from N(0, 1).
        """</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        self<span class="token punctuation">.</span>num_embeddings <span class="token operator">=</span> num_embeddings
        self<span class="token punctuation">.</span>embedding_dim <span class="token operator">=</span> embedding_dim
        weight <span class="token operator">=</span> init<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>weight<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> device
        self<span class="token punctuation">.</span>dtype <span class="token operator">=</span> dtype
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Maps word indices to one-hot vectors, and projects to embedding vectors

        Input:
        x of shape (seq_len, bs)

        Output:
        output of shape (seq_len, bs, embedding_dim)
        """</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        <span class="token comment"># l, b, m</span>
        x_one_hot <span class="token operator">=</span> init<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_embeddings<span class="token punctuation">,</span> x<span class="token punctuation">,</span> device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        n<span class="token punctuation">,</span> b<span class="token punctuation">,</span> m <span class="token operator">=</span> x_one_hot<span class="token punctuation">.</span>shape
        <span class="token comment"># l, b, m -> l * b, m</span>
        x_one_hot <span class="token operator">=</span> ops<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x_one_hot<span class="token punctuation">,</span> <span class="token punctuation">(</span>n <span class="token operator">*</span> b<span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># l * b, d</span>
        output <span class="token operator">=</span> ops<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x_one_hot<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        <span class="token comment"># l, b, d</span>
        output <span class="token operator">=</span> ops<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>output<span class="token punctuation">,</span> <span class="token punctuation">(</span>n<span class="token punctuation">,</span> b<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> output
        <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>models.py：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LanguageModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embedding_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                 seq_model<span class="token operator">=</span><span class="token string">'rnn'</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Consists of an embedding layer, a sequence model (either RNN or LSTM), and a
        linear layer.
        Parameters:
        output_size: Size of dictionary
        embedding_size: Size of embeddings
        hidden_size: The number of features in the hidden state of LSTM or RNN
        seq_model: 'rnn' or 'lstm', whether to use RNN or LSTM
        num_layers: Number of layers in RNN or LSTM
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LanguageModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>output_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        <span class="token keyword">if</span> seq_model <span class="token operator">==</span> <span class="token string">"rnn"</span><span class="token punctuation">:</span>
            seq_model <span class="token operator">=</span> nn<span class="token punctuation">.</span>RNN
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            seq_model <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM
        self<span class="token punctuation">.</span>seq_model <span class="token operator">=</span> seq_model<span class="token punctuation">(</span>embedding_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>out_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        <span class="token comment">### END YOUR SOLUTION</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> h<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Given sequence (and the previous hidden state if given), returns probabilities of next word
        (along with the last hidden state from the sequence model).
        Inputs:
        x of shape (seq_len, bs)
        h of shape (num_layers, bs, hidden_size) if using RNN,
            else h is tuple of (h0, c0), each of shape (num_layers, bs, hidden_size)
        Returns (out, h)
        out of shape (seq_len*bs, output_size)
        h of shape (num_layers, bs, hidden_size) if using RNN,
            else h is tuple of (h0, c0), each of shape (num_layers, bs, hidden_size)
        """</span>
        <span class="token comment">### BEGIN YOUR SOLUTION</span>
        l<span class="token punctuation">,</span> b <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        <span class="token comment"># l, b -> l, b, d</span>
        embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># l * b, d</span>
        feature<span class="token punctuation">,</span> h <span class="token operator">=</span> self<span class="token punctuation">.</span>seq_model<span class="token punctuation">(</span>embedding<span class="token punctuation">,</span> h<span class="token punctuation">)</span>
        d <span class="token operator">=</span> feature<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># l, b, d -> l * b, d</span>
        feature <span class="token operator">=</span> ndl<span class="token punctuation">.</span>ops<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>feature<span class="token punctuation">,</span> <span class="token punctuation">(</span>l <span class="token operator">*</span> b<span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># l * b, d</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>out_proj<span class="token punctuation">(</span>feature<span class="token punctuation">)</span>
        <span class="token comment"># l * b, d -> l, b, d</span>
        <span class="token comment"># output = ndl.ops.reshape(output, (l, b, d))</span>
        
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> h
        <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>simple_training.py：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">### PTB training ###</span>
<span class="token keyword">def</span> <span class="token function">epoch_general_ptb</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> model<span class="token punctuation">,</span> seq_len<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">,</span> loss_fn<span class="token operator">=</span>nn<span class="token punctuation">.</span>SoftmaxLoss<span class="token punctuation">,</span> opt<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        clip<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Iterates over the data. If optimizer is not None, sets the
    model to train mode, and for each batch updates the model parameters.
    If optimizer is None, sets the model to eval mode, and simply computes
    the loss/accuracy.

    Args:
        data: data of shape (nbatch, batch_size) given from batchify function
        model: LanguageModel instance
        seq_len: i.e. bptt, sequence length
        loss_fn: nn.Module instance
        opt: Optimizer instance (optional)
        clip: max norm of gradients (optional)

    Returns:
        avg_acc: average accuracy over dataset
        avg_loss: average loss over dataset
    """</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>
    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    <span class="token keyword">if</span> opt<span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    f <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span><span class="token punctuation">)</span>
    avg_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    avg_acc <span class="token operator">=</span> <span class="token number">0</span>
    cnt <span class="token operator">=</span> <span class="token number">0</span>
    n <span class="token operator">=</span> data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    i <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">while</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">:</span>
        <span class="token keyword">if</span> opt<span class="token punctuation">:</span>
            opt<span class="token punctuation">.</span>reset_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># (l, b), (l * b, )</span>
        x<span class="token punctuation">,</span> y <span class="token operator">=</span> ndl<span class="token punctuation">.</span>data<span class="token punctuation">.</span>get_batch<span class="token punctuation">(</span>data<span class="token punctuation">,</span> i<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
        b <span class="token operator">=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        y_<span class="token punctuation">,</span> h <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> f<span class="token punctuation">(</span>y_<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
        <span class="token keyword">if</span> opt<span class="token punctuation">:</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        cnt <span class="token operator">+=</span> b
        avg_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> b<span class="token punctuation">)</span>
        avg_acc <span class="token operator">+=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>y_<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        i <span class="token operator">+=</span> seq_len
    
    <span class="token keyword">return</span> avg_acc <span class="token operator">/</span> cnt<span class="token punctuation">,</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>avg_loss<span class="token punctuation">)</span> <span class="token operator">/</span> cnt
    <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">train_ptb</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> data<span class="token punctuation">,</span> seq_len<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">,</span> n_epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span>ndl<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">,</span>
          lr<span class="token operator">=</span><span class="token number">4.0</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> loss_fn<span class="token operator">=</span>nn<span class="token punctuation">.</span>SoftmaxLoss<span class="token punctuation">,</span> clip<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
          device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Performs &#123;n_epochs&#125; epochs of training.

    Args:
        model: LanguageModel instance
        data: data of shape (nbatch, batch_size) given from batchify function
        seq_len: i.e. bptt, sequence length
        n_epochs: number of epochs (int)
        optimizer: Optimizer class
        lr: learning rate (float)
        weight_decay: weight decay (float)
        loss_fn: nn.Module class
        clip: max norm of gradients (optional)

    Returns:
        avg_acc: average accuracy over dataset from last epoch of training
        avg_loss: average loss over dataset from last epoch of training
    """</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>
    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    opt <span class="token operator">=</span> optimizer<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">,</span> weight_decay<span class="token operator">=</span>weight_decay<span class="token punctuation">)</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        avg_acc<span class="token punctuation">,</span> avg_loss <span class="token operator">=</span> epoch_general_ptb<span class="token punctuation">(</span>
            data<span class="token operator">=</span>data<span class="token punctuation">,</span> 
            model<span class="token operator">=</span>model<span class="token punctuation">,</span> 
            seq_len<span class="token operator">=</span>seq_len<span class="token punctuation">,</span> 
            loss_fn<span class="token operator">=</span>loss_fn<span class="token punctuation">,</span> 
            opt<span class="token operator">=</span>opt<span class="token punctuation">,</span>
            clip<span class="token operator">=</span>clip<span class="token punctuation">,</span> 
            device<span class="token operator">=</span>device<span class="token punctuation">,</span> 
            dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">return</span> avg_acc<span class="token punctuation">,</span> avg_loss
    <span class="token comment">### END YOUR SOLUTION</span>


<span class="token keyword">def</span> <span class="token function">evaluate_ptb</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> data<span class="token punctuation">,</span> seq_len<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">,</span> loss_fn<span class="token operator">=</span>nn<span class="token punctuation">.</span>SoftmaxLoss<span class="token punctuation">,</span>
        device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Computes the test accuracy and loss of the model.

    Args:
        model: LanguageModel instance
        data: data of shape (nbatch, batch_size) given from batchify function
        seq_len: i.e. bptt, sequence length
        loss_fn: nn.Module class

    Returns:
        avg_acc: average accuracy over dataset
        avg_loss: average loss over dataset
    """</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>
    <span class="token comment">### BEGIN YOUR SOLUTION</span>
    avg_acc<span class="token punctuation">,</span> avg_loss <span class="token operator">=</span> epoch_general_ptb<span class="token punctuation">(</span>
        data<span class="token operator">=</span>data<span class="token punctuation">,</span> 
        model<span class="token operator">=</span>model<span class="token punctuation">,</span> 
        seq_len<span class="token operator">=</span>seq_len<span class="token punctuation">,</span> 
        loss_fn<span class="token operator">=</span>loss_fn<span class="token punctuation">,</span> 
        opt<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        clip<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
        device<span class="token operator">=</span>device<span class="token punctuation">,</span> 
        dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> avg_acc<span class="token punctuation">,</span> avg_loss
    <span class="token comment">### END YOUR SOLUTION</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Doraemonzzz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://www.doraemonzzz.com/2022/12/12/2022-12-12-Deep-Learning-Systems-HW4/">http://www.doraemonzzz.com/2022/12/12/2022-12-12-Deep-Learning-Systems-HW4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.doraemonzzz.com" target="_blank">Doraemonzzz</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/">深度学习系统</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/12/25/2022-12-25-Softmax%E6%9E%81%E5%80%BC%E9%A1%B9%E5%85%B3%E4%BA%8E%E6%B8%A9%E5%BA%A6%E7%9A%84%E5%AF%BC%E6%95%B0/"><img class="prev-cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Softmax极值项关于温度的导数</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/11/2022-12-11-Deep-Learning-Systems-HW3/"><img class="next-cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Deep Learning Systems HW3</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/10/16/2022-10-16-Deep-Learning-Systems-开发环境配置/" title="Deep Learning Systems 开发环境配置"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-16</div><div class="title">Deep Learning Systems 开发环境配置</div></div></a></div><div><a href="/2022/10/17/2022-10-17-Deep-Learning-Systems-HW0/" title="Deep Learning Systems HW0"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-17</div><div class="title">Deep Learning Systems HW0</div></div></a></div><div><a href="/2022/10/17/2022-10-17-Deep-Learning-Systems-HW1/" title="Deep Learning Systems HW1"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-17</div><div class="title">Deep Learning Systems HW1</div></div></a></div><div><a href="/2022/11/01/2022-11-1-Deep-Learning-Systems-HW2/" title="Deep Learning Systems HW2"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-01</div><div class="title">Deep Learning Systems HW2</div></div></a></div><div><a href="/2022/12/11/2022-12-11-Deep-Learning-Systems-HW3/" title="Deep Learning Systems HW3"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-11</div><div class="title">Deep Learning Systems HW3</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Livere</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNDcxOS8xMTI1Ng=="></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src= "/img/loading.gif" data-lazy-src="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Doraemonzzz</div><div class="author-info__description">个人博客，主要记录有关机器学习，数学以及计算机科学的笔记</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">786</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">79</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">32</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Doraemonzzz"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Doraemonzzz" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/doraemon_zzz@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://space.bilibili.com/291079982" target="_blank" title="Bilibili"><i class="iconfont icon-bilibili"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title=""><i class="fa fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">暂无公告</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-1-ND-Backend"><span class="toc-number">1.</span> <span class="toc-text">Part 1: ND Backend</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-2-CIFAR-10-dataset"><span class="toc-number">2.</span> <span class="toc-text">Part 2: CIFAR-10 dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-3-Convolutional-neural-network"><span class="toc-number">3.</span> <span class="toc-text">Part 3: Convolutional neural network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Padding-ndarrays"><span class="toc-number">3.1.</span> <span class="toc-text">Padding ndarrays</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Flipping-ndarrays-amp-FlipOp"><span class="toc-number">3.2.</span> <span class="toc-text">Flipping ndarrays &amp; FlipOp</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dilation"><span class="toc-number">3.3.</span> <span class="toc-text">Dilation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolution-forward"><span class="toc-number">3.4.</span> <span class="toc-text">Convolution forward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolution-backward"><span class="toc-number">3.5.</span> <span class="toc-text">Convolution backward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn-Conv"><span class="toc-number">3.6.</span> <span class="toc-text">nn.Conv</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet9"><span class="toc-number">3.7.</span> <span class="toc-text">ResNet9</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-4-Recurrent-neural-network"><span class="toc-number">4.</span> <span class="toc-text">Part 4: Recurrent neural network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-5-Long-short-term-memory-network"><span class="toc-number">5.</span> <span class="toc-text">Part 5: Long short-term memory network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-6-Penn-Treebank-dataset"><span class="toc-number">6.</span> <span class="toc-text">Part 6: Penn Treebank dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-7-Training-a-word-level-language-model"><span class="toc-number">7.</span> <span class="toc-text">Part 7: Training a word-level language model</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/01/06/2023-1-6-ECE408-Lecture-6-Generalized-Tiling-and-DRAM-Bandwidth/" title="ECE408 Lecture 6 Generalized Tiling and DRAM Bandwidth"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 6 Generalized Tiling and DRAM Bandwidth"/></a><div class="content"><a class="title" href="/2023/01/06/2023-1-6-ECE408-Lecture-6-Generalized-Tiling-and-DRAM-Bandwidth/" title="ECE408 Lecture 6 Generalized Tiling and DRAM Bandwidth">ECE408 Lecture 6 Generalized Tiling and DRAM Bandwidth</a><time datetime="2023-01-06T06:30:00.000Z" title="发表于 2023-01-06 14:30:00">2023-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/06/2023-1-6-ECE408-Lecture-5-Locality-and-Tiled-Matrix-Multiply/" title="ECE408 Lecture 5 Locality and Tiled Matrix Multiply"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 5 Locality and Tiled Matrix Multiply"/></a><div class="content"><a class="title" href="/2023/01/06/2023-1-6-ECE408-Lecture-5-Locality-and-Tiled-Matrix-Multiply/" title="ECE408 Lecture 5 Locality and Tiled Matrix Multiply">ECE408 Lecture 5 Locality and Tiled Matrix Multiply</a><time datetime="2023-01-06T04:07:00.000Z" title="发表于 2023-01-06 12:07:00">2023-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/06/2023-1-6-ECE408-Lecture-4-CUDA-Memory-Model/" title="ECE408 Lecture 4 CUDA Memory Model"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 4 CUDA Memory Model"/></a><div class="content"><a class="title" href="/2023/01/06/2023-1-6-ECE408-Lecture-4-CUDA-Memory-Model/" title="ECE408 Lecture 4 CUDA Memory Model">ECE408 Lecture 4 CUDA Memory Model</a><time datetime="2023-01-06T03:07:00.000Z" title="发表于 2023-01-06 11:07:00">2023-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/05/2023-1-5-ECE408-Lecture-3-CUDA-Parallel-Execution-Model/" title="ECE408 Lecture 3 CUDA Parallel Execution Model"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 3 CUDA Parallel Execution Model"/></a><div class="content"><a class="title" href="/2023/01/05/2023-1-5-ECE408-Lecture-3-CUDA-Parallel-Execution-Model/" title="ECE408 Lecture 3 CUDA Parallel Execution Model">ECE408 Lecture 3 CUDA Parallel Execution Model</a><time datetime="2023-01-05T09:03:00.000Z" title="发表于 2023-01-05 17:03:00">2023-01-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/05/2023-1-5-ECE408-Lecture-2-Introduction-to-Parallel-Computing-and-CUDA/" title="ECE408 Lecture 2 Introduction to Parallel Computing and CUDA"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 2 Introduction to Parallel Computing and CUDA"/></a><div class="content"><a class="title" href="/2023/01/05/2023-1-5-ECE408-Lecture-2-Introduction-to-Parallel-Computing-and-CUDA/" title="ECE408 Lecture 2 Introduction to Parallel Computing and CUDA">ECE408 Lecture 2 Introduction to Parallel Computing and CUDA</a><time datetime="2023-01-05T03:53:00.000Z" title="发表于 2023-01-05 11:53:00">2023-01-05</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2023 By Doraemonzzz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.25
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'IpnmxCW9CvYWIXbol5QXsegX-MdYXbMMI',
      appKey: 'w57DVCdbxcyB1TYYagMIMJIU',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Valine' === 'Livere' || !false) {
  if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>