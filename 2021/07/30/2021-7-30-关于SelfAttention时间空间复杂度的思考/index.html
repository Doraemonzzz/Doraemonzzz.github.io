<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>关于SelfAttention时间空间复杂度的思考 | Doraemonzzz</title><meta name="keywords" content="transformer优化"><meta name="author" content="Doraemonzzz"><meta name="copyright" content="Doraemonzzz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="参考资料： https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;blob&#x2F;master&#x2F;torch&#x2F;nn&#x2F;functional.py">
<meta property="og:type" content="article">
<meta property="og:title" content="关于SelfAttention时间空间复杂度的思考">
<meta property="og:url" content="http://www.doraemonzzz.com/2021/07/30/2021-7-30-%E5%85%B3%E4%BA%8ESelfAttention%E6%97%B6%E9%97%B4%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E6%80%9D%E8%80%83/index.html">
<meta property="og:site_name" content="Doraemonzzz">
<meta property="og:description" content="参考资料： https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;blob&#x2F;master&#x2F;torch&#x2F;nn&#x2F;functional.py">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2021-07-30T14:20:00.000Z">
<meta property="article:modified_time" content="2021-10-24T07:22:03.085Z">
<meta property="article:author" content="Doraemonzzz">
<meta property="article:tag" content="transformer优化">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true"><link rel="canonical" href="http://www.doraemonzzz.com/2021/07/30/2021-7-30-%E5%85%B3%E4%BA%8ESelfAttention%E6%97%B6%E9%97%B4%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E6%80%9D%E8%80%83/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6f00f37f957f0608abb8c571105456f0";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-G-RE4B1LKRZD"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-G-RE4B1LKRZD');
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"距离上次更新已经","messageNext":"天了，文章内容可能已经过时。"},
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '关于SelfAttention时间空间复杂度的思考',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-10-24 15:22:03'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/bilibili.css" media="defer" onload="this.media='all'"><meta name="google-site-verification" content="c4v-NmuUZRgl3cvtg9GKswryK1YLaPztd_5M-df5VNI" /><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Doraemonzzz" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src= "/img/loading.gif" data-lazy-src="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">789</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">79</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">32</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-chart-pie"></i><span> 博客统计</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/charts/"><i class="fa-fw far fa-chart-bar"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-area"></i><span> 访问统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/top/"><i class="fa-fw fab fa-hotjar"></i><span> 阅读排行榜</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Doraemonzzz</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-chart-pie"></i><span> 博客统计</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/charts/"><i class="fa-fw far fa-chart-bar"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-area"></i><span> 访问统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/top/"><i class="fa-fw fab fa-hotjar"></i><span> 阅读排行榜</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">关于SelfAttention时间空间复杂度的思考</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-07-30T14:20:00.000Z" title="发表于 2021-07-30 22:20:00">2021-07-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-24T07:22:03.085Z" title="更新于 2021-10-24 15:22:03">2021-10-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/transformer%E4%BC%98%E5%8C%96/">transformer优化</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>8分钟</span></span><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/2021/07/30/2021-7-30-%E5%85%B3%E4%BA%8ESelfAttention%E6%97%B6%E9%97%B4%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E6%80%9D%E8%80%83/" data-flag-title="关于SelfAttention时间空间复杂度的思考"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span class="leancloud-visitors-count"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2021/07/30/2021-7-30-%E5%85%B3%E4%BA%8ESelfAttention%E6%97%B6%E9%97%B4%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E6%80%9D%E8%80%83/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2021/07/30/2021-7-30-%E5%85%B3%E4%BA%8ESelfAttention%E6%97%B6%E9%97%B4%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E6%80%9D%E8%80%83/" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>参考资料：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py">https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py</a></p>
<span id="more"></span>
<h2 id="函数接口"><a href="#函数接口" class="headerlink" title="函数接口"></a>函数接口</h2><p>torch中MultiheadAttention是调用torch.nn.functional中的multi_head_attention_forward函数，首先看下该函数接口：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">multi_head_attention_forward</span><span class="token punctuation">(</span>
    query<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span>
    key<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span>
    value<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span>
    embed_dim_to_check<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
    num_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
    in_proj_weight<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span>
    in_proj_bias<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span>
    bias_k<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span>
    bias_v<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span>
    add_zero_attn<span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">,</span>
    dropout_p<span class="token punctuation">:</span> <span class="token builtin">float</span><span class="token punctuation">,</span>
    out_proj_weight<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span>
    out_proj_bias<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span>
    training<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
    key_padding_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    need_weights<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
    attn_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    use_separate_proj_weight<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    q_proj_weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    k_proj_weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    v_proj_weight<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    static_k<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    static_v<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tuple<span class="token punctuation">[</span>Tensor<span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r"""
    Args:
        query, key, value: map a query and a set of key-value pairs to an output.
            See "Attention Is All You Need" for more details.
        embed_dim_to_check: total dimension of the model.
        num_heads: parallel attention heads.
        in_proj_weight, in_proj_bias: input projection weight and bias.
        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
        add_zero_attn: add a new batch of zeros to the key and
                       value sequences at dim=1.
        dropout_p: probability of an element to be zeroed.
        out_proj_weight, out_proj_bias: the output projection weight and bias.
        training: apply dropout if is ``True``.
        key_padding_mask: if provided, specified padding elements in the key will
            be ignored by the attention. This is an binary mask. When the value is True,
            the corresponding value on the attention layer will be filled with -inf.
        need_weights: output attn_output_weights.
        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
            the batches while a 3D mask allows to specify a different mask for the entries of each batch.
        use_separate_proj_weight: the function accept the proj. weights for query, key,
            and value in different forms. If false, in_proj_weight will be used, which is
            a combination of q_proj_weight, k_proj_weight, v_proj_weight.
        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
        static_k, static_v: static key and value used for attention operators.
    Shape:
        Inputs:
        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
          the embedding dimension.
        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
          the embedding dimension.
        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
          the embedding dimension.
        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.
          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions
          will be unchanged. If a BoolTensor is provided, the positions with the
          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``
          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
          is provided, it will be added to the attention weight.
        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
        Outputs:
        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
          E is the embedding dimension.
        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,
          L is the target sequence length, S is the source sequence length.
    """</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>主要输入：</p>
<ul>
<li>query: (L, N, E)</li>
<li>key: (S, N, E)</li>
<li>value: (S, N, E)</li>
<li>num_heads: attention的头数</li>
</ul>
<p>输出：</p>
<ul>
<li>attn_output: (L, N, E)</li>
<li>attn_output_weights: (N, L, S)</li>
</ul>
<p>说明：</p>
<ul>
<li>query, key, value是输入，注意论文中的q, k, v是query, key, value进行线性变换后的结果；对于selfattention，三者相同；</li>
<li>attn_output，即$\text{softmax}(QK^T)VW_{o}$；attn_output_weights是权重，即$\text{softmax}(QK^T)$。<ul>
<li>这里忽略了原始论文中的伸缩因子。</li>
</ul>
</li>
<li>L是目标序列长度，N是batch size， E是embedding的维度；</li>
<li>一般情况下，会给定一个输出维度d，然后将d / num_heads作为每个多头注意力的输出维度，最后将num_heads个结果拼接得到一个维度为d的结果；</li>
</ul>
<h2 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h2><p>为了叙述方便，后续讨论做出以下限制：</p>
<ul>
<li>简化问题，不关注batch size $N$；</li>
<li>暂时只考虑selfattention，即query = key = value；</li>
</ul>
<p>这里给出全部符号：</p>
<ul>
<li>$X\in \mathbb R^{L\times E_1}$：对应query = key = value</li>
<li>$n$: num_heads</li>
<li>$W_Q^i, W_K^i\in \mathbb R^{E_1\times d_2}, W_V^i\in \mathbb R^{E_1\times d_3},i=1,\ldots, n$：对应$Q,K,V$的线性变换矩阵（$n$个头）；</li>
<li>$d_2\times n = E_2,d_3\times n =E_3$：一般$E_2,E_3$是给定的值；</li>
<li>$W_{o}\in \mathbb R^{E_3\times E_4}$：out_projection矩阵；</li>
</ul>
<p>在Selfattention中，一般来说都有</p>
<script type="math/tex; mode=display">
E_1= E_2= E_3= E_4=E</script><h2 id="时间以及空间复杂度讨论"><a href="#时间以及空间复杂度讨论" class="headerlink" title="时间以及空间复杂度讨论"></a>时间以及空间复杂度讨论</h2><p>这部分讨论原始的selfattention中的时间以及空间复杂度，为了简化起见，这里的时间复杂度不考虑多机以及并行等等。</p>
<h3 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h3><h4 id="矩阵乘法的时间复杂度"><a href="#矩阵乘法的时间复杂度" class="headerlink" title="矩阵乘法的时间复杂度"></a>矩阵乘法的时间复杂度</h4><p>在讨论之前，首先给出矩阵乘法的时间复杂度：</p>
<ul>
<li>对于$X\in \mathbb R^{d_1\times d_2}, Y\in \mathbb R^{d_2\times d_3}$，矩阵乘法$XY\in \mathbb R^{d_1\times d_3}$的时间复杂度为$\Theta(d_1d_2d_3)$</li>
</ul>
<p>说明：</p>
<ul>
<li>$XY$一共有$d_1d_3$个元素，每个元素需要做$d_2$维内积得到，即每个元素需要做$d_2$次乘法以及$d_2-1$次加法，所以每个元素的时间复杂度为$\Theta(d_2)$，因此总时间复杂度为$\Theta(d_1d_2d_3)$；</li>
</ul>
<h4 id="矩阵乘法以及SoftMax的时间复杂度比较"><a href="#矩阵乘法以及SoftMax的时间复杂度比较" class="headerlink" title="矩阵乘法以及SoftMax的时间复杂度比较"></a>矩阵乘法以及SoftMax的时间复杂度比较</h4><p>20210815更新：</p>
<ul>
<li>Softmax的时间测试有点问题，后续讨论中会忽略Softmax的计算时间；</li>
</ul>
<p>原文：</p>
<p>假设$Q, K\in \mathbb R^{L\times d}$，考虑计算Attention时的如下操作：</p>
<ul>
<li>$W=QK^T\in \mathbb R^{L\times L}$</li>
<li>$\mathrm{Softmax}(W)\in \mathbb R^{L\times L}$</li>
</ul>
<p>第一步的时间复杂度为$\Theta(dL^2)$，第二步的时间复杂度为$\Theta(cL^2)$，其中$c$表示计算每个元素的时间复杂度，现在的问题是$c$和$d$相比是否能够忽略？</p>
<p>此处由于涉及太底层的知识，所以直接通过模拟判断，最终的结论是，在单机A100上，$c$比$d$小几十倍，所以可以得出如下结论：</p>
<ul>
<li>相对于矩阵乘法，$\mathrm {Softmax}$的时间复杂度可以忽略；</li>
<li>$\mathrm{Softmax}(W)$的时间复杂度可以近似为$\Theta(dL^2)$；</li>
</ul>
<h5 id="实验代码以及实验结果"><a href="#实验代码以及实验结果" class="headerlink" title="实验代码以及实验结果"></a>实验代码以及实验结果</h5><h3 id="完整分析"><a href="#完整分析" class="headerlink" title="完整分析"></a>完整分析</h3><p>有了前置知识，下面进行完整的分析。</p>
<p>首先回顾符号：</p>
<ul>
<li>$X\in \mathbb R^{L\times E_1}$</li>
<li>$W_Q^i, W_K^i\in \mathbb R^{E_1\times d_2}, W_V^i\in \mathbb R^{E_1\times d_3},i=1,\ldots, n$；</li>
<li>$d_2\times n = E_2,d_3\times n =E_3$</li>
<li>$W_{o}\in \mathbb R^{E_3\times E_4}$：out_projection矩阵；</li>
</ul>
<p>空间复杂度（忽略中间计算产生的结果）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>矩阵</th>
<th>空间复杂度</th>
</tr>
</thead>
<tbody>
<tr>
<td>$W_Q^i, W_K^i\in \mathbb R^{E_1\times d_2},i=1,\ldots,n$</td>
<td>$2\times E_1\times d_2 \times n =E_1E_2$</td>
</tr>
<tr>
<td>$W_V^i\in \mathbb R^{E_1\times d_3},i=1,\ldots, n$</td>
<td>$E_1\times d_3\times n =E_1 E_3$</td>
</tr>
<tr>
<td>$W_{o}\in \mathbb R^{E_3\times E_4}$</td>
<td>$E_3 E_4$</td>
</tr>
</tbody>
</table>
</div>
<p>总空间复杂度：</p>
<script type="math/tex; mode=display">
E_1 E_2 +E_1E_3+E_3E_4</script><p>空间复杂度为：</p>
<p>后续以表格的方式给出算法流程以及对应的时间复杂度：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>公式</th>
<th>时间复杂度（忽略$\Theta$）</th>
</tr>
</thead>
<tbody>
<tr>
<td>$Q_i=XW_Q^i\in \mathbb R^{L\times d_2},i=1,\ldots,n$</td>
<td>$LE_1 d_2n=LE_1E_2$</td>
</tr>
<tr>
<td>$K_i=XW_K^i\in \mathbb R^{L\times d_2},i=1,\ldots,n$</td>
<td>$LE_1 d_2n=LE_1 E_2$</td>
</tr>
<tr>
<td>$V_i=XW_V^i\in \mathbb R^{L\times d_3},i=1,\ldots,n$</td>
<td>$LE_1d_3n=LE_1E_3$</td>
</tr>
<tr>
<td>$W_i=\text{Softmax}(Q_iK_i^T)\in \mathbb R^{L\times L}$</td>
<td>$L^2d_2n=E_2L^2$</td>
</tr>
<tr>
<td>$O_i=W_i V_i\in \mathbb R^{L\times d_3},i=1,\ldots, n$</td>
<td>$L^2d_3n =E_3L^2$</td>
</tr>
<tr>
<td>$O=[O_1,\ldots,O_n] W_o\in \mathbb R^{L\times E_4}$</td>
<td>$LE_3 E_4$</td>
</tr>
</tbody>
</table>
</div>
<p>总时间复杂度：</p>
<script type="math/tex; mode=display">
(2E_1E_2+E_1E_3 +E_3 E_4)L + (E_2+E_3)L^2</script><p>在Selfattention中，一般来说都有</p>
<script type="math/tex; mode=display">
E_1= E_2= E_3= E_4=E</script><p>所以总的空间时间复杂度分别为：</p>
<ul>
<li>空间：$3E^2$</li>
<li>时间：$4E^2L + 2E L^2$</li>
</ul>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>待补充。</p>
<p>这部分对于之前的计算方式进行一些思考。</p>
<p>注意到</p>
<script type="math/tex; mode=display">
\begin{aligned}
O&=[O_1,\ldots,O_n] W_o\\
&=[W_1 V_1,\ldots W_n V_n] W_o\\
&= [W_1XW_V^1,W_2XW_V^2,\ldots W_nXW_V^n]W_o\\
&=[W_1,\ldots, W_n] X [W_V^1,\ldots ,W_V^n]W_o
\end{aligned}</script></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Doraemonzzz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://www.doraemonzzz.com/2021/07/30/2021-7-30-%E5%85%B3%E4%BA%8ESelfAttention%E6%97%B6%E9%97%B4%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E6%80%9D%E8%80%83/">http://www.doraemonzzz.com/2021/07/30/2021-7-30-%E5%85%B3%E4%BA%8ESelfAttention%E6%97%B6%E9%97%B4%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E6%80%9D%E8%80%83/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.doraemonzzz.com" target="_blank">Doraemonzzz</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/transformer%E4%BC%98%E5%8C%96/">transformer优化</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/08/01/2021-8-1-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9E%84%E9%80%A0%E5%92%8C%E8%A7%A3%E9%87%8A(SICP)-%E7%AC%AC5%E7%AB%A0-%E4%B9%A0%E9%A2%98%E8%A7%A3%E6%9E%90-Part2/"><img class="prev-cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">计算机程序的构造和解释(SICP) 第5章 习题解析 Part2</div></div></a></div><div class="next-post pull-right"><a href="/2021/07/25/2021-7-25-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9E%84%E9%80%A0%E5%92%8C%E8%A7%A3%E9%87%8A(SICP)-%E7%AC%AC5%E7%AB%A0-%E4%B9%A0%E9%A2%98%E8%A7%A3%E6%9E%90-Part1/"><img class="next-cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">计算机程序的构造和解释(SICP) 第5章 习题解析 Part1</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/10/24/2021-10-24-Hybrid-Random-Features：利用Bagging的思路降低Kernel法的误差/" title="Hybrid Random Features：利用Bagging的思路降低Kernel法的误差"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-24</div><div class="title">Hybrid Random Features：利用Bagging的思路降低Kernel法的误差</div></div></a></div><div><a href="/2022/07/25/2022-7-25-Performer和RFA的理论以及实现/" title="Performer和RFA的理论以及实现"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-25</div><div class="title">Performer和RFA的理论以及实现</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Livere</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNDcxOS8xMTI1Ng=="></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src= "/img/loading.gif" data-lazy-src="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Doraemonzzz</div><div class="author-info__description">个人博客，主要记录有关机器学习，数学以及计算机科学的笔记</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">789</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">79</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">32</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Doraemonzzz"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Doraemonzzz" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/doraemon_zzz@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://space.bilibili.com/291079982" target="_blank" title="Bilibili"><i class="iconfont icon-bilibili"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title=""><i class="fa fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">暂无公告</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E6%8E%A5%E5%8F%A3"><span class="toc-number">1.</span> <span class="toc-text">函数接口</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7"><span class="toc-number">2.</span> <span class="toc-text">符号</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%B6%E9%97%B4%E4%BB%A5%E5%8F%8A%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E8%AE%A8%E8%AE%BA"><span class="toc-number">3.</span> <span class="toc-text">时间以及空间复杂度讨论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="toc-number">3.1.</span> <span class="toc-text">前置知识</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">3.1.1.</span> <span class="toc-text">矩阵乘法的时间复杂度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E4%BB%A5%E5%8F%8ASoftMax%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%AF%94%E8%BE%83"><span class="toc-number">3.1.2.</span> <span class="toc-text">矩阵乘法以及SoftMax的时间复杂度比较</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E4%BB%A3%E7%A0%81%E4%BB%A5%E5%8F%8A%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">实验代码以及实验结果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E5%88%86%E6%9E%90"><span class="toc-number">3.2.</span> <span class="toc-text">完整分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E8%80%83"><span class="toc-number">4.</span> <span class="toc-text">思考</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/01/15/2023-1-15-Deep-Learning-Systems-Lecture-1-Introduction-and-Logistics/" title="Deep Learning Systems Lecture 1 Introduction and Logistics"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Deep Learning Systems Lecture 1 Introduction and Logistics"/></a><div class="content"><a class="title" href="/2023/01/15/2023-1-15-Deep-Learning-Systems-Lecture-1-Introduction-and-Logistics/" title="Deep Learning Systems Lecture 1 Introduction and Logistics">Deep Learning Systems Lecture 1 Introduction and Logistics</a><time datetime="2023-01-15T13:14:00.000Z" title="发表于 2023-01-15 21:14:00">2023-01-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/14/2023-1-14-ECE408-Lecture-8-Tiled-Convolution/" title="ECE408 Lecture 8 Tiled Convolution"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 8 Tiled Convolution"/></a><div class="content"><a class="title" href="/2023/01/14/2023-1-14-ECE408-Lecture-8-Tiled-Convolution/" title="ECE408 Lecture 8 Tiled Convolution">ECE408 Lecture 8 Tiled Convolution</a><time datetime="2023-01-14T08:21:00.000Z" title="发表于 2023-01-14 16:21:00">2023-01-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/14/2023-1-14-ECE408-Lecture-7-Convolution-and-Constant-Memory/" title="ECE408 Lecture 7 Convolution and Constant Memory"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 7 Convolution and Constant Memory"/></a><div class="content"><a class="title" href="/2023/01/14/2023-1-14-ECE408-Lecture-7-Convolution-and-Constant-Memory/" title="ECE408 Lecture 7 Convolution and Constant Memory">ECE408 Lecture 7 Convolution and Constant Memory</a><time datetime="2023-01-14T05:45:00.000Z" title="发表于 2023-01-14 13:45:00">2023-01-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/06/2023-1-6-ECE408-Lecture-6-Generalized-Tiling-and-DRAM-Bandwidth/" title="ECE408 Lecture 6 Generalized Tiling and DRAM Bandwidth"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 6 Generalized Tiling and DRAM Bandwidth"/></a><div class="content"><a class="title" href="/2023/01/06/2023-1-6-ECE408-Lecture-6-Generalized-Tiling-and-DRAM-Bandwidth/" title="ECE408 Lecture 6 Generalized Tiling and DRAM Bandwidth">ECE408 Lecture 6 Generalized Tiling and DRAM Bandwidth</a><time datetime="2023-01-06T06:30:00.000Z" title="发表于 2023-01-06 14:30:00">2023-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/06/2023-1-6-ECE408-Lecture-5-Locality-and-Tiled-Matrix-Multiply/" title="ECE408 Lecture 5 Locality and Tiled Matrix Multiply"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ECE408 Lecture 5 Locality and Tiled Matrix Multiply"/></a><div class="content"><a class="title" href="/2023/01/06/2023-1-6-ECE408-Lecture-5-Locality-and-Tiled-Matrix-Multiply/" title="ECE408 Lecture 5 Locality and Tiled Matrix Multiply">ECE408 Lecture 5 Locality and Tiled Matrix Multiply</a><time datetime="2023-01-06T04:07:00.000Z" title="发表于 2023-01-06 12:07:00">2023-01-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2023 By Doraemonzzz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.25
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'IpnmxCW9CvYWIXbol5QXsegX-MdYXbMMI',
      appKey: 'w57DVCdbxcyB1TYYagMIMJIU',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Valine' === 'Livere' || !false) {
  if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>