<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>CS229 Lesson 5 生成学习算法 | Doraemonzzz</title><meta name="keywords" content="CS229"><meta name="author" content="Doraemonzzz"><meta name="copyright" content="Doraemonzzz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="课程视频地址：http:&#x2F;&#x2F;open.163.com&#x2F;special&#x2F;opencourse&#x2F;machinelearning.html 课程主页：http:&#x2F;&#x2F;cs229.stanford.edu&#x2F; 更具体的资料链接：https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;0a6ef31ff77a 笔记参考自中文翻译版：https:&#x2F;&#x2F;github.com&#x2F;Kivy-CN&#x2F;Stanford-CS-229">
<meta property="og:type" content="article">
<meta property="og:title" content="CS229 Lesson 5 生成学习算法">
<meta property="og:url" content="http://www.doraemonzzz.com/2019/02/16/CS229%20Lesson%205%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="Doraemonzzz">
<meta property="og:description" content="课程视频地址：http:&#x2F;&#x2F;open.163.com&#x2F;special&#x2F;opencourse&#x2F;machinelearning.html 课程主页：http:&#x2F;&#x2F;cs229.stanford.edu&#x2F; 更具体的资料链接：https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;0a6ef31ff77a 笔记参考自中文翻译版：https:&#x2F;&#x2F;github.com&#x2F;Kivy-CN&#x2F;Stanford-CS-229">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2019-02-16T14:53:00.000Z">
<meta property="article:modified_time" content="2019-02-28T07:31:03.665Z">
<meta property="article:author" content="Doraemonzzz">
<meta property="article:tag" content="CS229">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true"><link rel="canonical" href="http://www.doraemonzzz.com/2019/02/16/CS229%20Lesson%205%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6f00f37f957f0608abb8c571105456f0";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-G-RE4B1LKRZD"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-G-RE4B1LKRZD');
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"距离上次更新已经","messageNext":"天了，文章内容可能已经过时。"},
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CS229 Lesson 5 生成学习算法',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2019-02-28 15:31:03'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/bilibili.css" media="defer" onload="this.media='all'"><meta name="google-site-verification" content="c4v-NmuUZRgl3cvtg9GKswryK1YLaPztd_5M-df5VNI" /><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Doraemonzzz" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src= "/img/loading.gif" data-lazy-src="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">622</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">62</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-chart-pie"></i><span> 博客统计</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/charts/"><i class="fa-fw far fa-chart-bar"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-area"></i><span> 访问统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/top/"><i class="fa-fw fab fa-hotjar"></i><span> 阅读排行榜</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Doraemonzzz</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-chart-pie"></i><span> 博客统计</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/charts/"><i class="fa-fw far fa-chart-bar"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-area"></i><span> 访问统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/top/"><i class="fa-fw fab fa-hotjar"></i><span> 阅读排行榜</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">CS229 Lesson 5 生成学习算法</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-02-16T14:53:00.000Z" title="发表于 2019-02-16 22:53:00">2019-02-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2019-02-28T07:31:03.665Z" title="更新于 2019-02-28 15:31:03">2019-02-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/CS229/">CS229</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>27分钟</span></span><span class="post-meta-separator">|</span><span class="leancloud_visitors" id="/2019/02/16/CS229%20Lesson%205%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" data-flag-title="CS229 Lesson 5 生成学习算法"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span class="leancloud-visitors-count"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2019/02/16/CS229%20Lesson%205%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2019/02/16/CS229%20Lesson%205%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>课程视频地址：<a target="_blank" rel="noopener" href="http://open.163.com/special/opencourse/machinelearning.html">http://open.163.com/special/opencourse/machinelearning.html</a></p>
<p>课程主页：<a target="_blank" rel="noopener" href="http://cs229.stanford.edu/">http://cs229.stanford.edu/</a></p>
<p>更具体的资料链接：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/0a6ef31ff77a">https://www.jianshu.com/p/0a6ef31ff77a</a></p>
<p>笔记参考自中文翻译版：<a target="_blank" rel="noopener" href="https://github.com/Kivy-CN/Stanford-CS-229-CN">https://github.com/Kivy-CN/Stanford-CS-229-CN</a></p>
<p>这一讲介绍了高斯判别分析以及朴素贝叶斯算法。</p>
<span id="more"></span>
<h2 id="Part-IV-生成学习算法"><a href="#Part-IV-生成学习算法" class="headerlink" title="Part IV 生成学习算法"></a>Part IV 生成学习算法</h2><p>到目前为止，我们主要讨论了模拟$p(y | x;θ)$的学习算法，即给定$x$的$y$的条件分布。 例如，logistic回归将$p(y | x;θ)$建模为$h_{\theta}(x)= g(θ^Tx)$，其中$g$是sigmoid函数。 在这部分讲义中，我们将讨论一种不同类型的学习算法。</p>
<p>​    考虑一个分类问题，我们希望根据动物的某些特征来学习区分大象$(y=1)$和狗$y=0​$。 给定训练集，像logistic回归或感知机算法的算法试图找到一条直线，即决定边界，然后将大象和狗分开。 然后，为了将新动物分类为大象或狗，它检查决定新动物落在边界的哪一侧，并相应地进行预测。</p>
<p>​    这里有个不同的方法。 首先，观察大象，我们可以建立一个大象的模型。 然后，观察狗，我们可以建立一个狗的模型。 最后，为了对新动物进行分类，我们可以将新动物与大象模型相匹配，并将其与狗模型相匹配，以查看新动物是否更像我们在训练集中看到的大象或狗。</p>
<p>​    尝试直接学习$p(y | x)$的算法（例如logistic回归），或者试图直接从输入$\mathcal X$的空间到标签$\{0,1\}$学习映射的算法（例如感知机算法）被称为<strong>判别</strong>学习算法。在这里，我们将讨论尝试对$p(x | y)$（和$p(y)$）建模的算法。这些算法被称为<strong>生成</strong>学习算法。例如，如果$y$表示的样本是狗$(0)$还是大象$(1)$，则$p(x | y = 0)$对狗的特征的分布建模，以及$p(x | y = 1)$对大象特征的分布建模。</p>
<p>​    在对$p(y)​$（称为<strong>分类先验</strong>）和$p(x|y)​$进行建模之后，我们的算法可以使用贝叶斯法则在给定$x​$的$y​$上导出后验分布：</p>
<script type="math/tex; mode=display">
p(y|x) = \frac {p(x|y)p(y)}
{p(x)}</script><p>这里，分母由$p(x) = p(x|y = 1)p(y = 1) + p(x|y =0)p(y = 0)​$给出，因此也可以用我们学到的量$p(x | y)​$和$p(y)​$表示。实际上，如果为了进行预测而计算$p(y|x)​$，那么我们实际上并不需要计算分母，因为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\arg \max_y p(y|x) &=  \arg \max_y\frac {p(x|y)p(y)} {p(x)}\\
&=\arg \max_y p(x|y)p(y)
\end{aligned}</script><p>​    </p>
<h3 id="1-高斯判别分析"><a href="#1-高斯判别分析" class="headerlink" title="1.高斯判别分析"></a>1.高斯判别分析</h3><p>我们将要看的第一个生成学习算法是高斯判别分析（GDA）。 在这个模型中，我们假设$p(x|y)​$是服从多元正态分布分布。 在进入GDA模型本身之前，让我们简单地谈谈多元正态分布的性质。</p>
<h4 id="1-1-多元正态分布"><a href="#1-1-多元正态分布" class="headerlink" title="1.1 多元正态分布"></a>1.1 多元正态分布</h4><p>$n$维的多元正态分布，也称为多元高斯分布，由<strong>均值向量</strong>$\mu\in \mathbb R^n$以及<strong>协方差矩阵</strong>$\Sigma \in \mathbb R^{n\times n}$参数化，其中$\Sigma\ge 0$是半正定对称矩阵的。多元正态分布也写为“$\mathcal N(\mu, \Sigma)$”，其密度由下式给出：</p>
<script type="math/tex; mode=display">
p(x; μ,\Sigma) =\frac{1}{(2\pi )^{n/2} |\Sigma|^{1/2}}
\exp \Big(
-\frac 1 2 (x-\mu)^T \Sigma^{-1}(x-\mu)
\Big)</script><p>在上面的等式中，“$|\Sigma|​$”表示矩阵$\Sigma​$的行列式。</p>
<p>​    对于服从$\mathcal N(\mu, \Sigma)$的随机变量$X$，期望由$\mu$给出：</p>
<script type="math/tex; mode=display">
\mathbb E [X]=\int_x xp(x;\mu,\Sigma) dx = \mu</script><p>​    向量值随机变量$Z$的<strong>协方差</strong>定义为$\text{Cov}(Z) =\mathbb E[(Z −\mathbb E[Z])(Z −\mathbb E[Z])^T ]$。 这概括了实值随机变量方差的概念。 协方差也可以定义为$\text{Cov}(Z) =\mathbb E[ZZ^T] −(\mathbb E[Z])(\mathbb E[Z])^T $。如果$X\sim \mathcal N(\mu,\Sigma)$，那么</p>
<script type="math/tex; mode=display">
\text{Cov}(X)=\Sigma</script><h4 id="1-2-高斯判别分析模型"><a href="#1-2-高斯判别分析模型" class="headerlink" title="1.2 高斯判别分析模型"></a>1.2 高斯判别分析模型</h4><p>当我们有一个输入特征$x​$是连续随机变量的分类问题时，我们可以使用高斯判别分析（GDA）模型，该模型使用多元正态分布对$p(x|y)​$进行建模。 该模型是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
y &\sim \text{Bernoulli}(\phi)\\
x|y=0& \sim  \mathcal N(\mu_0,\Sigma)\\
x|y=1& \sim  \mathcal N(\mu_1,\Sigma)
\end{aligned}</script><p>写出分布，即为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(y)&= \phi^{y} (1-\phi)^{1-y}\\
p(x|y=0) &=  \frac{1}{(2\pi )^{n/2} |\Sigma|^{1/2}}
\exp \Big(
-\frac 1 2 (x-\mu_0)^T \Sigma^{-1}(x-\mu_0)
\Big)\\
p(x|y=1) &=  \frac{1}{(2\pi )^{n/2} |\Sigma|^{1/2}}
\exp \Big(
-\frac 1 2 (x-\mu_1)^T \Sigma^{-1}(x-\mu_1)
\Big)
\end{aligned}</script><p>这里，我们的模型的参数是$\phi,\Sigma,\mu_0,\mu_1$。（注意，虽然存在两个不同的均值向量$\mu_0$和$\mu_1$，但是该模型通常仅使用同一个协方差矩阵$\Sigma $。）数据的对数似然性由下式给出：</p>
<script type="math/tex; mode=display">
\begin{aligned}
ℓ(\phi, \mu_0, \mu_1,\Sigma)
&=\log \prod_{i=1}^m p(x^{(i)},y^{(i)};\phi, \mu_0, \mu_1,\Sigma) \\
&=\log \prod_{i=1}^m p(x^{(i)}|y^{(i)}; \mu_0, \mu_1,\Sigma)p(y^{(i)};\phi)
\end{aligned}</script><p>通过关于参数最大化$ℓ$，我们发现参数的最大似然估计为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\phi &= \frac{1}{m} \sum_{i=1}^{m} 1\{y^{(i)}= 1\} \\
\mu_{0}&= \frac{\sum_{i=1}^{m}1\{y^{(i)} =0\}x^{(i)} }{\sum_{i=1}^{m}1\{y^{(i)} = 0\}} \\
\mu_1&= \frac{\sum_{i=1}^{m}1\{y^{(i)} = 1\}x^{(i)} }{\sum_{i=1}^{m}1\{y^{(i)} = 1\}} \\
 \Sigma &= \frac 1  m  \sum_{i=1}^{m}  (x^{(i)} − \mu_{y^{(i)}}) (x^{(i)} − \mu_{y^{(i)}})^T
\end{aligned}</script><p>推导过程如下：</p>
<p>先观察$P(x|y)​$的形式，可以得到如下公式</p>
<script type="math/tex; mode=display">
P(x|y) = \frac1{(2π)^{n/2}|Σ|^{1/2} }\text{exp} \Big (−\frac 12(x − \mu_y)^T Σ^{−1}(x − \mu_y)\Big)</script><p>接着计算$\text{log}P(x,y)$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{log} P(x,y) 
&= \text{log}P(x|y)P(y)\\
&=\text{log}\Big(  \frac1{(2π)^{n/2}|Σ|^{1/2} }\text{exp} \Big (−\frac 12(x − \mu_y)^T Σ^{−1}(x − \mu_y)\Big) \phi ^{ 1\{y = 1\}} (1-\phi)^{^{ 1\{y =0\}}} \Big) \\
&= \text{log}  \frac1{(2π)^{n/2}|Σ|^{1/2}} −\frac 12(x − \mu_y)^T Σ^{−1}(x − \mu_y)  + 1\{y = 1\} \text{log} \phi + 1\{y = 0\} \log(1-\phi)
\end{aligned}</script><p>对数似然函数为</p>
<script type="math/tex; mode=display">
\begin{aligned}
ℓ(φ, \mu_{−1}, \mu_1, Σ) 
&=\text{log}\prod_{i=1}^m p(x^{(i)}, y^{(i)}; φ, \mu_{0}, \mu_1, Σ)\\
&= \sum_{i=1}^{m}  \text{log}  p(x^{(i)}, y^{(i)}; φ, \mu_{0}, \mu_1, Σ)\\
&= \sum_{i=1}^{m} \Big( \text{log}  \frac1{(2π)^{n/2}|Σ|^{1/2}} −\frac 12(x^{(i)} − \mu_{y^{(i)}})^T Σ^{−1}(x^{(i)} − \mu_{y^{(i)}})  + 1\{y^{(i)} = 1\} \text{log} \phi + 1\{y^{(i)} = 0\} \log(1-\phi) \Big)\\
\end{aligned}</script><p>关于$\phi$求梯度</p>
<script type="math/tex; mode=display">
\frac{\partial ℓ}{\partial \phi} = \sum_{i=1}^{m} \Big( \frac{1\{y^{(i)} = 1\} }{\phi}- \frac{1\{y^{(i)} = 0\}}{1- \phi}\Big) = 0\\
\sum_{i=1}^{m} 1\{y^{(i)} = 1\} (1-\phi)   - 1\{y^{(i)} =0\}  \phi  =0 \\
\sum_{i=1}^{m} 1\{y^{(i)}= 1\}  = m\phi \\
\phi = \frac{1}{m} \sum_{i=1}^{m} 1\{y^{(i)}= 1\}</script><p>关于$\mu_1,\mu_{0}$求梯度</p>
<script type="math/tex; mode=display">
\nabla_{\mu_1} ℓ = \sum_{i=1}^{m}Σ^{−1}(x^{(i)} − \mu_{y^{(i)}})1 \{y^{(i)} = 1\} =0\\
\sum_{i=1}^{m} (x^{(i)} − \mu_1)1 \{y^{(i)} = 1\} =0\\
\mu_1= \frac{\sum_{i=1}^{m}1\{y^{(i)} = 1\}x^{(i)} }{\sum_{i=1}^{m}1\{y^{(i)} = 1\}} \\
\nabla_{\mu_{0}} ℓ = \sum_{i=1}^{m}Σ^{−1}(x^{(i)} − \mu_{y^{(i)}})1 \{y^{(i)} =0\} =0\\
\sum_{i=1}^{m} (x^{(i)} − \mu_{0})1 \{y^{(i)} = 0\} =0\\
\mu_{0}= \frac{\sum_{i=1}^{m}1\{y^{(i)} =0\}x^{(i)} }{\sum_{i=1}^{m}1\{y^{(i)} = 0\}}</script><p>求$\Sigma​$的时候利用一些技巧性，我们不求的$\Sigma​$极大似然估计，而是求$\Sigma^{-1}​$的极大似然估计，然后再求出$\Sigma ​$的极大似然估计，利用如下两个式子</p>
<script type="math/tex; mode=display">
\nabla_A \det|A| = \det|A|(A ^{-1})^T\\
\nabla_A (x^TAy) = \nabla_A  \text{trace}(x^TAy) =  xy^T</script><p>那么</p>
<script type="math/tex; mode=display">
\nabla_{\Sigma^{-1}} ℓ = \nabla_{\Sigma^{-1}} \Big( \frac m 2 \log \Big|{\Sigma}^{-1} \Big|   \Big) -  \frac 12 \nabla_{\Sigma^{-1}} \sum_{i=1}^{m} (x^{(i)} − \mu_{y^{(i)}})^T Σ^{−1}(x^{(i)} − \mu_{y^{(i)}}) = 0\\
  \frac m 2 \frac{1}{ \Big|{\Sigma}^{-1} \Big|}  \Big|{\Sigma}^{-1} \Big|{\Sigma} -\frac 1 2  \sum_{i=1}^{m}  (x^{(i)} − \mu_{y^{(i)}}) (x^{(i)} − \mu_{y^{(i)}})^T = 0\\
  \Sigma = \frac 1  m  \sum_{i=1}^{m}  (x^{(i)} − \mu_{y^{(i)}}) (x^{(i)} − \mu_{y^{(i)}})^T</script><p>所以结论成立。</p>
<p>​    从图中可以看出，算法的作用如下：</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://github.com/Doraemonzzz/md-photo/blob/master/CS229/lecture%205/2019021701.jpg?raw=true" alt=""></p>
<p>​    图中显示的是训练集，以及两个高斯分布的等高线，这两个高斯分布已经拟合了两个类中的每个类中的数据。 注意，两个高斯具有相同形状和方向的等高线，因为它们共享协方差矩阵，但是它们具有不同的均值$\mu_0 $和$\mu_1$。 图中还示出了给出决策边界的直线，即$p(y = 1|x) = 0.5.$。 在边界的一侧，我们预测$y = 1$是最可能的结果，而另一侧，我们预测$y = 0​$。</p>
<p>​    </p>
<h4 id="1-3-讨论：GDA和logistic回归"><a href="#1-3-讨论：GDA和logistic回归" class="headerlink" title="1.3 讨论：GDA和logistic回归"></a>1.3 讨论：GDA和logistic回归</h4><p>GDA模型与logistic回归有着有趣的关系。 如果我们将$p(y = 1|x; φ, μ_0, μ_1,\Sigma)$视为$x$的函数，我们发现它可以表达为如下形式</p>
<script type="math/tex; mode=display">
p(y = 1|x; \phi,\Sigma, \mu_0, \mu_1) =
\frac 1
{1 + \exp(−θ^T x)}</script><p>其中θ是$\phi,\Sigma, \mu_0, \mu_1$的函数。这正是logistic回归——一个判别算法——对模型$p(y=1|x)$建模的形式。</p>
<p>证明如下：</p>
<p>先计算$P(x)$</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(x) &= P(y=1)P(x|y=1) + P(y=0)P(x|y=0)\\
&=\phi  \frac1{(2π)^{n/2}|Σ|^{1/2} }\text{exp} \Big (−\frac 12(x − \mu_1)^T Σ^{−1}(x − \mu_1)\Big)+
(1-\phi)  \frac1{(2π)^{n/2}|Σ|^{1/2} }\text{exp} \Big (−\frac 12(x − \mu_{0})^T Σ^{−1}(x − \mu_{0})\Big)
\end{aligned}</script><p>利用贝叶斯公式计算$P(y|x)$，分$y=1,y=0$计算</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(y|x)
&=\frac{P(x|y)P(y)}{P(x)}\\
&=\frac{P(x|y)P(y)}{\phi  \frac1{(2π)^{n/2}|Σ|^{1/2} }\text{exp} \Big (−\frac 12(x − \mu_1)^T Σ^{−1}(x − \mu_1)\Big)+
(1-\phi)  \frac1{(2π)^{n/2}|Σ|^{1/2} }\text{exp} \Big (−\frac 12(x − \mu_{0})^T Σ^{−1}(x − \mu_{0})\Big)}
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(y=1|x)
&=\frac{P(x|y=1)P(y=1)}{\phi  \frac1{(2π)^{n/2}|Σ|^{1/2} }\text{exp} \Big (−\frac 12(x − \mu_1)^T Σ^{−1}(x − \mu_1)\Big)+
(1-\phi)  \frac1{(2π)^{n/2}|Σ|^{1/2} }\text{exp} \Big (−\frac 12(x − \mu_{0})^T Σ^{−1}(x − \mu_{0})\Big)}\\
&=\frac{ \frac1{(2π)^{n/2}|Σ|^{1/2} }\text{exp} \Big (−\frac 12(x − \mu_1)^T Σ^{−1}(x − \mu_1)\Big)\phi}{\phi  \frac1{(2π)^{n/2}|Σ|^{1/2} }\text{exp} \Big (−\frac 12(x − \mu_1)^T Σ^{−1}(x − \mu_1)\Big)+
(1-\phi)  \frac1{(2π)^{n/2}|Σ|^{1/2} }\text{exp} \Big (−\frac 12(x − \mu_{0})^T Σ^{−1}(x − \mu_{0})\Big)}\\
&=\frac{1}{1+\frac{1-\phi}{\phi} \text{exp} \Big (−\frac 12(x − \mu_{0})^T Σ^{−1}(x − \mu_{0})+\frac 12(x − \mu_1)^T Σ^{−1}(x − \mu_1)\Big)}
\end{aligned}</script><p>现在计算指数部分的式子</p>
<script type="math/tex; mode=display">
\begin{aligned}
−\frac 12(x − \mu_{0})^T Σ^{−1}(x − \mu_{0})+\frac 12(x − \mu_1)^T Σ^{−1}(x − \mu_1)
&=\frac 1 2
\Big(x^T Σ^{−1}x -2\mu_1 ^T Σ^{−1}x + \mu_1 ^T Σ^{−1}  \mu_1
-x^T Σ^{−1}x+2\mu_{0} ^T Σ^{−1}x -\mu_{0} ^T Σ^{−1}  \mu_{0}\Big) \\ 
&=\frac 1 2 \Big( 2(\mu_{0} ^T Σ^{−1}-\mu_1 ^T Σ^{−1})x + \mu_1 ^T Σ^{−1}\mu_1 -\mu_{0} ^T Σ^{−1}  \mu_{0} \Big)\\
&=(\mu_{0} ^T Σ^{−1}-\mu_1 ^T Σ^{−1})x +\frac 1 2 (\mu_1 ^T Σ^{−1}\mu_1 -\mu_{0} ^T Σ^{−1}  \mu_{0} )
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(y=1|x) 
&=\frac{1}{1+\frac{1-\phi}{\phi} \text{exp} \Big ((\mu_{0} ^T Σ^{−1}-\mu_1 ^T Σ^{−1})x +\frac 1 2 (\mu_1 ^T Σ^{−1}\mu_1 -\mu_{0} ^T Σ^{−1}  \mu_{0} )\Big)}\\
&=\frac{1}{1+\text{exp} \Big ((\mu_{0} ^T Σ^{−1}-\mu_1 ^T Σ^{−1})x +\frac 1 2 (\mu_1 ^T Σ^{−1}\mu_1 -\mu_{0} ^T Σ^{−1}  \mu_{0} )
+\text{ln}(\frac{1-\phi}{\phi}) \Big)}
\end{aligned}</script><p>令</p>
<script type="math/tex; mode=display">
\theta = (\mu_1 ^T Σ^{−1} -\mu_{0} ^T Σ^{−1})^T = Σ^{−1} (\mu_1 - \mu_{0})\\
\theta_0 =  -\frac 1 2 (\mu_1 ^T Σ^{−1}\mu_1 -\mu_{0} ^T Σ^{−1}  \mu_{0} )
-\text{ln}(\frac{1-\phi}{\phi})</script><p>从而</p>
<script type="math/tex; mode=display">
p(y =1| x; φ, Σ, \mu_{0}, \mu_1) = \frac{1}
{1 + \text{exp}(−(θ^T x + θ_0))}</script><p>​    在什么时候我们会更喜欢一种模型呢？ 一般而言，GDA和logistic回归在对同一数据集进行训练时会给出不同的决策边界。哪个更好？</p>
<p>​    我们之前讨论了如果$p(x|y)$是多元高斯分布（有共同的$\Sigma$），那么$p(y|x )$必然遵循logistic函数。 然而，相反的情况并非如此；即，$p(y|x )$是logistic函数并不意味着$p(x|y)$是多元高斯分布。这表明GDA比logistic回归对数据做出更强的建模假设。事实证明，当这些建模假设正确时，GDA对数据的拟合更好，是更好的模型。具体来说，当$p(x|y)$确实是高斯分布（有共同的$\Sigma$）时，则GDA<strong>渐近有效</strong>。 非正式地，这意味着在非常大的训练集（大$m$）的限制下，没有严格优于GDA的算法（就他们估计$p(y|x )​$的准确程度而言）。特别是，可以证明，在这种情况下，GDA将是一种比logistic回归更好的算法；更一般地说，即使是小型训练集，我们通常也会期望GDA更好。</p>
<p>​    相反，通过做出明显较弱的假设，logistic回归则更加稳健，对不正确的建模假设也不那么敏感。有许多不同的假设会导致$p(y|x )​$采用logistic函数的形式。 例如，如果$x|y = 0 \sim \text{Poisson}(λ_0)​$，并且$x|y = 1 \sim \text{Poisson}(λ_1)​$，那么$p(y|x )​$将是logistic。 Logistic回归也可以很好地处理像这样的泊松数据。 但是如果我们在这样的数据上使用GDA并且将高斯分布拟合到这样的非高斯数据——那么结果将是不太可预测的，并且GDA可能（或可能不）表现良好。</p>
<p>​    总而言之：当建模假设正确或至少近似正确时，GDA做出更强的建模假设，并且数据效率更高（即，需要更少的训练数据来完成“好”的学习）。 Logistic回归假设较弱，并且对建模假设的偏差明显更加稳健。 具体来说，当数据确实是非高斯数据时，那么在大数据集的限制下，logistic回归几乎总是比GDA更好。 因此，在实际中，logistic回归比GDA更常用。（关于判别模型与生成模型的一些相关考虑也适用于我们接下来讨论的朴素贝叶斯算法，但朴素贝叶斯算法仍然被认为是非常好的，并且当然也是一种非常流行的分类算法。）</p>
<h3 id="2-朴素贝叶斯"><a href="#2-朴素贝叶斯" class="headerlink" title="2.朴素贝叶斯"></a>2.朴素贝叶斯</h3><p>在GDA中，特征向量$x$是连续的实值向量。 现在让我们讨论一种不同的学习算法，其中$x_i$是离散值的。</p>
<p>​    作为我们的示例，请考虑使用机器学习构建垃圾邮件过滤器。 在这里，我们希望根据消息是垃圾邮件还是非垃圾邮件来对邮件进行分类。 在学会这样做之后，我们可以让我们的邮件阅读器自动过滤出垃圾邮件，并可能将它们放在一个单独的邮件文件夹中。 对电子邮件进行分类是称为<strong>文本分类</strong>的更广泛问题的一个示例。</p>
<p>​    假设我们有一个训练集（一组标记为垃圾邮件或非垃圾邮件的电子邮件）。我们将通过指定用于表示电子邮件的特征$x_i$来开始构建我们的垃圾邮件过滤器。</p>
<p>​    我们将通过特征向量表示电子邮件，其长度等于字典中的单词数。 具体来说，如果电子邮件包含字典的第$i$个单词，那么我们将令$x_i = 1$; 否则，我们令$x_i = 0$。例如，向量</p>
<script type="math/tex; mode=display">
x= \left[
 \begin{matrix}
 1 \\
  0 \\
0\\
\vdots \\
1\\
\vdots \\
0
  \end{matrix}
  \right] \    \begin{matrix}
 \text{a}\\
 \text{aardvark} \\
\text{aardwolf}\\
\vdots \\
 \text{buy}\\
\vdots \\
 \text{zygmurgy}
  \end{matrix}</script><p>用于表示包含单词“a”和“buy”，但不包含“aardvark”，“aardwolf”或“zygmurgy”的电子邮件。编码到特征向量中的单词集称为<strong>词汇表</strong>，因此$x$的维度等于词汇量的大小。</p>
<p>​    为了对$p(x|y)$建模，我们将做出一个非常强的假设。 我们假设$x_i$关于$y$是条件独立的。 该假设被称为<strong>朴素贝叶斯（NB）假设</strong>，并且所得算法被称为<strong>朴素贝叶斯分类器</strong>。 例如，如果$y = 1$表示垃圾邮件；“buy”是第2087个单词，“price”是第39831个单词；然后假设已知$y=1$（某个特定的电子邮件是垃圾邮件），那么$x_{2087}$的信息（消息中是否出现“buy”的信息）将不会影响到$x_{39831}$的值（是否出现“price”）。更正式地说，这可以写成$p(x_{2087}|y) = p(x_{2087}|y, x_{39831})$。（注意，这与$x_{2087}$和$x_{39831}$是独立的不一样，这个条件为“$p(x_{2087}) = p(x_{2087}| x_{39831})$”；相反，我们只假设$x_{2087}$和$x_{39831}$关于$y$条件独立。）</p>
<p>​    我们现在有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(x_1, . . . , x_{50000}|y) 
&=p(x_1|y)p(x_2|y, x_1)p(x_3|y, x_1, x_2) ... p(x_{50000}|y, x_1, . . . , x_{49999})\\
&=p(x_1|y)p(x_2|y)p(x_3|y) · · · p(x_{50000}|y) \\
&=\prod_{i=1}^n p(x_i|y)
\end{aligned}</script><p>第一个等式简单地遵循概率的通常属性，第二个等式使用NB假设。 我们注意到，即使朴素贝叶斯假设是一个非常强的假设，所得到的算法在许多问题上也能很好地工作。</p>
<p>​    我们的模型通过$\phi_{i|y=1} = p(x_i = 1|y = 1),\phi_{i|y=0} = p(x_i = 1|y = 0)$，以及$\phi_y= p(y = 1)$来参数化。 像往常一样，给定训练集$\{(x^{(i)}, y^{(i)}); i =1, . . . ,m\}$，数据的似然性为：</p>
<script type="math/tex; mode=display">
\mathcal L(\phi_y, \phi_{j|y=0}, \phi_{j|y=1}) =
\prod^m_{i=1}
p(x^{(i)}, y^{(i)})</script><p>关于$\phi_y, \phi_{j|y=0}, \phi_{j|y=1}$最大化上式给出最大似然估计：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\phi_{j|y=1}& =\frac{\sum_{i=1}^m1\{y^{(i)}=1\land x^{(i)}_j=1 \}}{\sum_{i=1}^m 1\{y^{(i)}=1\}}\\
\phi_{j|y=0} &=\frac{\sum_{i=1}^m1\{y^{(i)}=0\land x^{(i)}_j=1 \}}{\sum_{i=1}^m 1\{y^{(i)}=0\}}\\
\phi_y& = \frac {\sum_{i=1}^m1\{y^{(i)}=1 \}}{m}
\end{aligned}</script><p>证明如下：</p>
<p>不难看出</p>
<script type="math/tex; mode=display">
\begin{aligned}

p(y) &= (\phi_y)^y(1 − \phi_y)^{1−y}\\
p(x|y=k)&= \prod_{j=1}^{n} (\phi_{j|y=k})^{x_j}(1-\phi_{j|y=k})^{1-x_j}
\end{aligned}</script><p>令$\varphi $表示参数集$\{\phi_y, \phi_{j|y=0}, \phi_{j|y=1}, j = 1, . . . , n\}$，所以对数似然函数$ℓ(\varphi)​$为</p>
<script type="math/tex; mode=display">
\begin{aligned}
ℓ(\varphi)&=\log \prod_{i=1}^m p(x^{(i)},y^{(i)};\varphi) \\
&=\sum_{i=1}^m \log p(x^{(i)},y^{(i)};\varphi) \\
&=\sum_{i=1}^m \log p(x^{(i)}|y^{(i)}) p(y^{(i)}) \\
&=\sum_{i=1}^m \log  \prod_{j=1}^{n} (\phi_{j|y=y^{(i)}})^{x^{(i)}_j}
(1-\phi_{j|y=y^{(i)}})^{1-x^{(i)}_j} 
(\phi_{y})^{y^{(i)}}
(1-\phi_{y})^{1-y^{(i)}}  \\
&=\sum_{i=1}^m\sum_{j=1}^n \Big( 
x^{(i)}_j \log(\phi_{j|y=y^{(i)}}) + (1-x^{(i)}_j) \log(1-\phi_{j|y=y^{(i)}})\Big)+
\sum_{i=1}^m\Big( y^{(i)} \log \phi_{y}
+ (1-y^{(i)}) \log (1- \phi_{y})
\Big)
\end{aligned}</script><p>先关于$\phi_{j|y=k}​$求梯度</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\phi_{j|y=k}} ℓ(\varphi)&= \sum_{i=1}^m\Big( 
x^{(i)}_j \frac 1 {\phi_{j|y=y^{(i)}}}1\{y^{(i)}=k\} + (1-x^{(i)}_j) 
\frac 1 {1-\phi_{j|y=y^{(i)}}}(-1) 1\{y^{(i)}=k\}\Big)\\
&=
 \sum_{i=1}^m \frac{1\{y^{(i)}=k\}}{\phi_{j|y=y^{(i)}}(1-\phi_{j|y=y^{(i)}})}\Big( 
 x^{(i)}_j  (1-\phi_{j|y=y^{(i)}}) -
(1-x^{(i)}_j) \phi_{j|y=y^{(i)}}
 \Big)\\
 &=\frac{1}{\phi_{j|y=k}(1-\phi_{j|y=k})}
 \sum_{i=1}^m1\{y^{(i)}=k\}\Big( 
 x^{(i)}_j  -\phi_{j|y=k}
 \Big)
\end{aligned}</script><p>令上式为$0​$可得</p>
<script type="math/tex; mode=display">
\sum_{i=1}^m1\{y^{(i)}=k\}\Big( 
 x^{(i)}_j  -\phi_{j|y=k}
 \Big) = 0 \\
(\sum_{i=1}^m 1\{y^{(i)}=k\} )\phi_{j|y=k}=
\sum_{i=1}^m1\{y^{(i)}=k\}x^{(i)}_j =
\sum_{i=1}^m1\{y^{(i)}=k\land x^{(i)}_j=1 \} \\
\phi_{j|y=k} =\frac{\sum_{i=1}^m1\{y^{(i)}=k\land x^{(i)}_j=1 \}}{\sum_{i=1}^m 1\{y^{(i)}=k\}}</script><p>从而</p>
<script type="math/tex; mode=display">
\phi_{j|y=0} =\frac{\sum_{i=1}^m1\{y^{(i)}=0\land x^{(i)}_j=1 \}}{\sum_{i=1}^m 1\{y^{(i)}=0\}}\\
\phi_{j|y=1} =\frac{\sum_{i=1}^m1\{y^{(i)}=1\land x^{(i)}_j=1 \}}{\sum_{i=1}^m 1\{y^{(i)}=1\}}</script><p>关于$\phi_{y}$求梯度可得</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\phi_{y}} ℓ(\varphi)
&= \sum_{i=1}^m\nabla_{\phi_{y}} \Big( y^{(i)} \log \phi_{y}
+ (1-y^{(i)}) \log (1- \phi_{y})
\Big)  \\
&= \sum_{i=1}^m\Big( 
y^{(i)} \frac 1 {\phi_{y}} - (1-y^{(i)}) \frac 1 {1-\phi_{y}}\Big)\\
&=
 \frac{1}{\phi_{y}(1-\phi_{y})}\sum_{i=1}^m \Big( 
y^{(i)}(1-\phi_{y}) -
(1-y^{(i)})\phi_{y} \Big)\\
 &=\frac{1}{\phi_{y}(1-\phi_{y})}
 \sum_{i=1}^m \Big( 
 y^{(i)}  -\phi_{y}
 \Big)
\end{aligned}</script><p>令上式为$0$可得</p>
<script type="math/tex; mode=display">
\phi_y = \frac {\sum_{i=1}^m1\{y^{(i)}=1 \}}{m}</script><p>在上面的等式中，“$∧$”符号表示“and”。这些参数具有非常自然的解释。 例如，$\phi_{j|y=1} $是单词$j$出现的垃圾邮件$(y=1 )$的比例。</p>
<p>​    在拟合了所有这些参数之后，为了对具有特征$x$的新样本进行预测，我们就可以简单地进行计算</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(y=1|x)
&= \frac{p(x|y=1)p(y=1)}{p(x)} \\
&=\frac{\big(\prod_{i=1}^n p(x_i|y=1)\big)p(y=1)}
{\big(\prod_{i=1}^n p(x_i|y=1)\big)p(y=1)+\big(\prod_{i=1}^n p(x_i|y=0)\big)p(y=0)}
\end{aligned}</script><p>并选择具有较高后验概率的类别。</p>
<p>​    最后，我们注意到虽然我们已经开发了朴素贝叶斯算法，主要是针对特征$x_i ​$是二值的问题，将其推广到$x_i ​$可以取$\{1, 2, . . . , k_i\}​$很简单。 在这里，我们只是将$p(x_i|y)​$建模为多项分布而不是伯努利分布。 实际上，即使某些原始输入属性（例如，房屋的生活区域，如我们之前的示例中）是连续值，将其<strong>离散化</strong>也是很常见的，即将其转换为一小组离散值，然后应用朴素贝叶斯。 例如，如果我们使用某些特征$x_i​$来表示生活区域，我们可以将连续值离散化如下：</p>
<p>​    <img src= "/img/loading.gif" data-lazy-src="https://github.com/Doraemonzzz/md-photo/blob/master/CS229/lecture%205/2019021702.jpg?raw=true" alt=""></p>
<p>因此，对于居住面积为$890$平方英尺的房屋，我们将相应特征$x_i$的值设置为$3$，然后我们可以应用朴素贝叶斯算法，并使用多项分布对$p(x_i|y)​$建模，如前所述。当原始的连续值属性没有通过多元正态分布很好地建模时，对特征进行离散化并使用朴素贝叶斯（而不是GDA）通常会产生更好的分类器。</p>
<h4 id="2-1-拉普拉斯平滑"><a href="#2-1-拉普拉斯平滑" class="headerlink" title="2.1 拉普拉斯平滑"></a>2.1 拉普拉斯平滑</h4><p>我们已经描述过的朴素贝叶斯算法可以很好地解决许多问题，但是有一个简单的改变可以使它更好地工作，特别是对于文本分类。 让我们简要讨论当前形式的算法问题，然后讨论我们如何解决它。</p>
<p>​    考虑垃圾邮件/电子邮件分类，让我们假设，在完成CS229并完成项目的优秀工作后，您决定在2003年6月左右将您所做的工作提交给NIPS会议进行发布。（NIPS是顶级机器学习会议之一，提交论文的截止日期通常是在6月底或7月初。）因为您最终在电子邮件中讨论会议，所以您也开始收到带有“nips”字样的消息。 但这是你的第一篇NIPS论文，直到这个时候，你还没有看到任何包含“nips”这个词的电子邮件；特别是“nips”并没有出现在你的垃圾邮件/非垃圾邮件的训练集中。假设“nips”是字典中的第$35000$个单词，那么你的朴素贝叶斯垃圾邮件过滤器已经选择了参数$\phi_{35000|y}$的最大似然估计值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\phi_{35000|y=1} &=\frac{\sum_{i=1}^m 1\{x_{35000}^{(i)}\land y^{(i)}=1\}}{\sum_{i=1}^m 1\{y^{(i)}=1\}} =0 \\
\phi_{35000|y=0} &=\frac{\sum_{i=1}^m 1\{x_{35000}^{(i)}\land y^{(i)}=0\}}{\sum_{i=1}^m 1\{y^{(i)}=0\}} =0
\end{aligned}</script><p>即，因为它之前从未在垃圾邮件或非垃圾邮件训练样本中看到过“nips”，它认为在任一类型的电子邮件中看到它的概率为零。因此，当试图确定这些包含“nips”的消息之一是否是垃圾邮件时，它会计算后验概率，并得到</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(y=1|x)& = \frac{\big(\prod_{i=1}^n p(x_i|y=1)\big)p(y=1)}
{\big(\prod_{i=1}^n p(x_i|y=1)\big)p(y=1)+\big(\prod_{i=1}^n p(x_i|y=0)\big)p(y=0)} \\
&= \frac 0 0
\end{aligned}</script><p>这是因为“$\prod_{i=1}^n p(x_i|y)$”中的每一项都包含$p(x_{35000}|y) =0$。 因此，我们的算法获得$0/0$，并且不知道如何进行预测。</p>
<p>​    更广泛的，仅仅因为你之前没有在有限训练集中看到它，估计一些事件的概率为零在统计上是个坏主意。考虑估计取值$\{1, . . . , k\}$的多项分布随机变量$z$的均值的问题。 我们可以用$\phi_i =p(z=i)$参数化我们的多项式。 给定一组$m$个独立观测变量$\{z^{(1)}, . . . , z^{(m)}\}​$，最大似然估计由下式给出</p>
<script type="math/tex; mode=display">
\phi_j = \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}}{m}</script><p>正如我们之前看到的，如果我们使用这些最大似然估计，那么一些$\phi_j$可能最终为零，这就会产生问题。为了避免这种情况，我们可以使用<strong>拉普拉斯平滑</strong>，它取代上面的估计</p>
<script type="math/tex; mode=display">
\phi_j = \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}+1}{m+k}</script><p>这里，我们对分子加$1$，对分母加$k$。 注意$\sum_{j=1}^k \phi_j=1$仍然成立。对于所有$j$，$\phi_j \neq 0$。 在某些（可以说是相当强的）条件下，可以证明拉普拉斯平滑实际上给出了$\phi_j$的最优估计。</p>
<p>​    返回我们的朴素贝叶斯分类器，使用拉普拉斯平滑，我们因此得到以下参数估计：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\phi_{35000|y=1} &=\frac{\sum_{i=1}^m 1\{x_{35000}^{(i)}\land y^{(i)}=1\}+1}{\sum_{i=1}^m 1\{y^{(i)}=1\}+2} \\
\phi_{35000|y=0} &=\frac{\sum_{i=1}^m 1\{x_{35000}^{(i)}\land y^{(i)}=0\}+1}{\sum_{i=1}^m 1\{y^{(i)}=0\}+2}
\end{aligned}</script></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Doraemonzzz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://www.doraemonzzz.com/2019/02/16/CS229%20Lesson%205%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">http://www.doraemonzzz.com/2019/02/16/CS229%20Lesson%205%20%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.doraemonzzz.com" target="_blank">Doraemonzzz</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CS229/">CS229</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/02/17/CS229%20Lesson%206%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"><img class="prev-cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">CS229 Lesson 6 朴素贝叶斯算法</div></div></a></div><div class="next-post pull-right"><a href="/2019/02/15/CS229%20Lesson%204%20%E7%89%9B%E9%A1%BF%E6%96%B9%E6%B3%95/"><img class="next-cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CS229 Lesson 4 牛顿方法</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2019/03/01/CS229 2017版作业0/" title="CS229 2017版作业0"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-03-01</div><div class="title">CS229 2017版作业0</div></div></a></div><div><a href="/2018/12/12/CS229 Hoeffding不等式补充资料/" title="CS229 Hoeffding不等式补充资料"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-12-12</div><div class="title">CS229 Hoeffding不等式补充资料</div></div></a></div><div><a href="/2019/04/02/CS229 2017版作业4/" title="CS229 2017版作业4"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-04-02</div><div class="title">CS229 2017版作业4</div></div></a></div><div><a href="/2019/02/01/CS229 Lesson 1 机器学习的动机与应用/" title="CS229 Lesson 1 机器学习的动机与应用"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-02-01</div><div class="title">CS229 Lesson 1 机器学习的动机与应用</div></div></a></div><div><a href="/2018/12/16/CS229 Lesson 10 特征选择/" title="CS229 Lesson 10 特征选择"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-12-16</div><div class="title">CS229 Lesson 10 特征选择</div></div></a></div><div><a href="/2018/12/25/CS229 Lesson 11 贝叶斯统计正则化/" title="CS229 Lesson 11 贝叶斯统计正则化"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-12-25</div><div class="title">CS229 Lesson 11 贝叶斯统计正则化</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Livere</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNDcxOS8xMTI1Ng=="></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src= "/img/loading.gif" data-lazy-src="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Doraemonzzz</div><div class="author-info__description">个人博客，主要记录有关机器学习，数学以及计算机科学的笔记</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">622</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">62</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Doraemonzzz"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Doraemonzzz" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/doraemon_zzz@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://space.bilibili.com/291079982" target="_blank" title="Bilibili"><i class="iconfont icon-bilibili"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title=""><i class="fa fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">暂无公告</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-IV-%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">Part IV 生成学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90"><span class="toc-number">1.1.</span> <span class="toc-text">1.高斯判别分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 多元正态分布</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 高斯判别分析模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E8%AE%A8%E8%AE%BA%EF%BC%9AGDA%E5%92%8Clogistic%E5%9B%9E%E5%BD%92"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 讨论：GDA和logistic回归</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-number">1.2.</span> <span class="toc-text">2.朴素贝叶斯</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 拉普拉斯平滑</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/06/26/2021-6-26-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F-%E7%AC%AC6%E7%AB%A0-%E4%B9%A0%E9%A2%98%E8%A7%A3%E6%9E%90/" title="深入理解计算机系统 第6章 习题解析"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深入理解计算机系统 第6章 习题解析"/></a><div class="content"><a class="title" href="/2021/06/26/2021-6-26-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F-%E7%AC%AC6%E7%AB%A0-%E4%B9%A0%E9%A2%98%E8%A7%A3%E6%9E%90/" title="深入理解计算机系统 第6章 习题解析">深入理解计算机系统 第6章 习题解析</a><time datetime="2021-06-26T08:02:00.000Z" title="发表于 2021-06-26 16:02:00">2021-06-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/24/2021-6-24-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95%EF%BC%88%E7%AC%AC%E4%B8%83%E7%89%88%EF%BC%89Socket-Programming-Assignment-4ProxyServer/" title="计算机网络——自顶向下方法（第七版）Socket Programming Assignment 4:ProxyServer"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机网络——自顶向下方法（第七版）Socket Programming Assignment 4:ProxyServer"/></a><div class="content"><a class="title" href="/2021/06/24/2021-6-24-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95%EF%BC%88%E7%AC%AC%E4%B8%83%E7%89%88%EF%BC%89Socket-Programming-Assignment-4ProxyServer/" title="计算机网络——自顶向下方法（第七版）Socket Programming Assignment 4:ProxyServer">计算机网络——自顶向下方法（第七版）Socket Programming Assignment 4:ProxyServer</a><time datetime="2021-06-24T15:59:00.000Z" title="发表于 2021-06-24 23:59:00">2021-06-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/22/2021-6-22-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95%EF%BC%88%E7%AC%AC%E4%B8%83%E7%89%88%EF%BC%89Socket-Programming-Assignment-3SMTP/" title="计算机网络——自顶向下方法（第七版）Socket Programming Assignment 3:SMTP"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机网络——自顶向下方法（第七版）Socket Programming Assignment 3:SMTP"/></a><div class="content"><a class="title" href="/2021/06/22/2021-6-22-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95%EF%BC%88%E7%AC%AC%E4%B8%83%E7%89%88%EF%BC%89Socket-Programming-Assignment-3SMTP/" title="计算机网络——自顶向下方法（第七版）Socket Programming Assignment 3:SMTP">计算机网络——自顶向下方法（第七版）Socket Programming Assignment 3:SMTP</a><time datetime="2021-06-22T15:51:00.000Z" title="发表于 2021-06-22 23:51:00">2021-06-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/22/2021-6-22-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95%EF%BC%88%E7%AC%AC%E4%B8%83%E7%89%88%EF%BC%89Socket-Programming-Assignment-2UDPpinger/" title="计算机网络——自顶向下方法（第七版）Socket Programming Assignment 2:UDPpinger"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机网络——自顶向下方法（第七版）Socket Programming Assignment 2:UDPpinger"/></a><div class="content"><a class="title" href="/2021/06/22/2021-6-22-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95%EF%BC%88%E7%AC%AC%E4%B8%83%E7%89%88%EF%BC%89Socket-Programming-Assignment-2UDPpinger/" title="计算机网络——自顶向下方法（第七版）Socket Programming Assignment 2:UDPpinger">计算机网络——自顶向下方法（第七版）Socket Programming Assignment 2:UDPpinger</a><time datetime="2021-06-22T15:21:00.000Z" title="发表于 2021-06-22 23:21:00">2021-06-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/22/2021-6-22-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95%EF%BC%88%E7%AC%AC%E4%B8%83%E7%89%88%EF%BC%89Socket-Programming-Assignment-1WebServer/" title="计算机网络——自顶向下方法（第七版）Socket Programming Assignment 1:WebServer"><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机网络——自顶向下方法（第七版）Socket Programming Assignment 1:WebServer"/></a><div class="content"><a class="title" href="/2021/06/22/2021-6-22-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95%EF%BC%88%E7%AC%AC%E4%B8%83%E7%89%88%EF%BC%89Socket-Programming-Assignment-1WebServer/" title="计算机网络——自顶向下方法（第七版）Socket Programming Assignment 1:WebServer">计算机网络——自顶向下方法（第七版）Socket Programming Assignment 1:WebServer</a><time datetime="2021-06-22T14:44:00.000Z" title="发表于 2021-06-22 22:44:00">2021-06-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2021 By Doraemonzzz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.25
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'IpnmxCW9CvYWIXbol5QXsegX-MdYXbMMI',
      appKey: 'w57DVCdbxcyB1TYYagMIMJIU',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Valine' === 'Livere' || !false) {
  if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>