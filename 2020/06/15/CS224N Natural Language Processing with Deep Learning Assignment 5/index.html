<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>CS224N Natural Language Processing with Deep Learning Assignment 5 | Doraemonzzz</title><meta name="keywords" content="CS224N"><meta name="author" content="Doraemonzzz"><meta name="copyright" content="Doraemonzzz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="课程主页：https:&#x2F;&#x2F;web.stanford.edu&#x2F;class&#x2F;archive&#x2F;cs&#x2F;cs224n&#x2F;cs224n.1194&#x2F; 视频地址：https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;av46216519?from&#x3D;search&amp;seid&#x3D;13229282510647565239">
<meta property="og:type" content="article">
<meta property="og:title" content="CS224N Natural Language Processing with Deep Learning Assignment 5">
<meta property="og:url" content="http://doraemonzzz.com/2020/06/15/CS224N%20Natural%20Language%20Processing%20with%20Deep%20Learning%20Assignment%205/index.html">
<meta property="og:site_name" content="Doraemonzzz">
<meta property="og:description" content="课程主页：https:&#x2F;&#x2F;web.stanford.edu&#x2F;class&#x2F;archive&#x2F;cs&#x2F;cs224n&#x2F;cs224n.1194&#x2F; 视频地址：https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;av46216519?from&#x3D;search&amp;seid&#x3D;13229282510647565239">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2020-06-15T15:27:00.000Z">
<meta property="article:modified_time" content="2020-06-20T04:27:20.839Z">
<meta property="article:author" content="Doraemonzzz">
<meta property="article:tag" content="CS224N">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true"><link rel="canonical" href="http://doraemonzzz.com/2020/06/15/CS224N%20Natural%20Language%20Processing%20with%20Deep%20Learning%20Assignment%205/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6f00f37f957f0608abb8c571105456f0";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CS224N Natural Language Processing with Deep Learning Assignment 5',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2020-06-20 12:27:20'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/bilibili.css" media="defer" onload="this.media='all'"><meta name="google-site-verification" content="c4v-NmuUZRgl3cvtg9GKswryK1YLaPztd_5M-df5VNI" /><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Doraemonzzz" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">611</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">60</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-chart-pie"></i><span> 博客统计</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/charts/"><i class="fa-fw far fa-chart-bar"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-area"></i><span> 访问统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/top/"><i class="fa-fw fab fa-hotjar"></i><span> 阅读排行榜</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Doraemonzzz</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-chart-pie"></i><span> 博客统计</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/charts/"><i class="fa-fw far fa-chart-bar"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-area"></i><span> 访问统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/top/"><i class="fa-fw fab fa-hotjar"></i><span> 阅读排行榜</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">CS224N Natural Language Processing with Deep Learning Assignment 5</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-06-15T15:27:00.000Z" title="发表于 2020-06-15 23:27:00">2020-06-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-06-20T04:27:20.839Z" title="更新于 2020-06-20 12:27:20">2020-06-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="CS224N Natural Language Processing with Deep Learning Assignment 5"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><p>课程主页：<a target="_blank" rel="noopener" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/">https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/</a></p>
<p>视频地址：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av46216519?from=search&amp;seid=13229282510647565239">https://www.bilibili.com/video/av46216519?from=search&amp;seid=13229282510647565239</a></p>
<span id="more"></span>
<h3 id="1-Character-based-convolutional-encoder-for-NMT"><a href="#1-Character-based-convolutional-encoder-for-NMT" class="headerlink" title="1. Character-based convolutional encoder for NMT"></a>1. Character-based convolutional encoder for NMT</h3><h4 id="a"><a href="#a" class="headerlink" title="(a)"></a>(a)</h4><p>因为字母的复杂度比单词要小很多。</p>
<h4 id="b"><a href="#b" class="headerlink" title="(b)"></a>(b)</h4><p>character-based</p>
<script type="math/tex; mode=display">
V_{\text {char}} \times e_{\mathrm{char}}  + e_{\text{word}} \times e_{\mathrm{char}} \times k+e_{\text{word}}
\approx 96\times 50+256\times 50\times 5+256=69056</script><p>word-based</p>
<script type="math/tex; mode=display">
V_{\text {word}} \times e_{\text {word}} \approx 50000 \times 256 =12800000</script><h4 id="c"><a href="#c" class="headerlink" title="(c)"></a>(c)</h4><p>1D卷积的计算速度快，RNN计算速度慢，另外1D卷积可以捕捉局部相关性。</p>
<h4 id="d"><a href="#d" class="headerlink" title="(d)"></a>(d)</h4><p>最大池化可以过滤无用信息，平均池化可以保留大部分信息。</p>
<h4 id="e"><a href="#e" class="headerlink" title="(e)"></a>(e)</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">words2charindices</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sents<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Convert list of sentences of words into list of list of list of character indices.
    @param sents (list[list[str] ]): sentence(s) in words
    @return word_ids (list[list[list[int] ]]): sentence(s) in indices
    """</span>
    <span class="token comment">### YOUR CODE HERE for part 1e</span>
    <span class="token comment">### TODO: </span>
    <span class="token comment">###     This method should convert characters in the input sentences into their </span>
    <span class="token comment">###     corresponding character indices using the character vocabulary char2id </span>
    <span class="token comment">###     defined above.</span>
    <span class="token comment">###</span>
    <span class="token comment">###     You must prepend each word with the `start_of_word` character and append </span>
    <span class="token comment">###     with the `end_of_word` character. </span>
    res <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> sent <span class="token keyword">in</span> sents<span class="token punctuation">:</span>
        l1 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> word <span class="token keyword">in</span> sent<span class="token punctuation">:</span>
            l2 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            l2<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>char2id<span class="token punctuation">[</span><span class="token string">'&#123;'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> char <span class="token keyword">in</span> word<span class="token punctuation">:</span>
                l2<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>char2id<span class="token punctuation">[</span>char<span class="token punctuation">]</span><span class="token punctuation">)</span>
            l2<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>char2id<span class="token punctuation">[</span><span class="token string">'&#125;'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            l1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>l2<span class="token punctuation">)</span>
        res<span class="token punctuation">.</span>append<span class="token punctuation">(</span>l1<span class="token punctuation">)</span>
    <span class="token keyword">return</span> res
    
    <span class="token comment">### END YOUR CODE</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用如下代码测试：</p>
<pre class="line-numbers language-none"><code class="language-none">python sanity_check.py 1e<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>得到如下结果：</p>
<pre class="line-numbers language-none"><code class="language-none">--------------------------------------------------------------------------------
Running Sanity Check for Question 1e: words2charindices()
--------------------------------------------------------------------------------
Running test on small list of sentences
Running test on large list of sentences
All Sanity Checks Passed for Question 1e: words2charindices()!
--------------------------------------------------------------------------------<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="f"><a href="#f" class="headerlink" title="(f)"></a>(f)</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">pad_sents_char</span><span class="token punctuation">(</span>sents<span class="token punctuation">,</span> char_pad_token<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Pad list of sentences according to the longest sentence in the batch and max_word_length.
    @param sents (list[list[list[int] ]]): list of sentences, result of `words2charindices()`
        from `vocab.py`
    @param char_pad_token (int): index of the character-padding token
    @returns sents_padded (list[list[list[int] ]]): list of sentences where sentences/words shorter
        than the max length sentence/word are padded out with the appropriate pad token, such that
        each sentence in the batch now has same number of words and each word has an equal
        number of characters
        Output shape: (batch_size, max_sentence_length, max_word_length)
    """</span>
    <span class="token comment"># Words longer than 21 characters should be truncated</span>
    max_word_length <span class="token operator">=</span> <span class="token number">21</span>

    <span class="token comment">### YOUR CODE HERE for part 1f</span>
    <span class="token comment">### TODO:</span>
    <span class="token comment">###     Perform necessary padding to the sentences in the batch similar to the pad_sents()</span>
    <span class="token comment">###     method below using the padding character from the arguments. You should ensure all</span>
    <span class="token comment">###     sentences have the same number of words and each word has the same number of</span>
    <span class="token comment">###     characters.</span>
    <span class="token comment">###     Set padding words to a `max_word_length` sized vector of padding characters.</span>
    <span class="token comment">###</span>
    <span class="token comment">###     You should NOT use the method `pad_sents()` below because of the way it handles</span>
    <span class="token comment">###     padding and unknown words.</span>
    L <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> sent <span class="token keyword">in</span> sents<span class="token punctuation">:</span>
        L <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>L<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sent<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
    sents_padded <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> sent <span class="token keyword">in</span> sents<span class="token punctuation">:</span>
        l1 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> word <span class="token keyword">in</span> sent<span class="token punctuation">:</span>
            l2 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span>
            m <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> max_word_length<span class="token punctuation">)</span>
            <span class="token keyword">for</span> index <span class="token keyword">in</span> word<span class="token punctuation">[</span><span class="token punctuation">:</span>m<span class="token punctuation">]</span><span class="token punctuation">:</span>
                l2<span class="token punctuation">.</span>append<span class="token punctuation">(</span>index<span class="token punctuation">)</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_word_length <span class="token operator">-</span> m<span class="token punctuation">)</span><span class="token punctuation">:</span>
                l2<span class="token punctuation">.</span>append<span class="token punctuation">(</span>char_pad_token<span class="token punctuation">)</span>
            l1<span class="token punctuation">.</span>append<span class="token punctuation">(</span>l2<span class="token punctuation">)</span>
        <span class="token comment">#补充单词</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>L <span class="token operator">-</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sent<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            l1<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>char_pad_token <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_word_length<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        sents_padded<span class="token punctuation">.</span>append<span class="token punctuation">(</span>l1<span class="token punctuation">)</span>

    <span class="token comment">### END YOUR CODE</span>

    <span class="token keyword">return</span> sents_padded<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用如下代码测试：</p>
<pre class="line-numbers language-none"><code class="language-none">python sanity_check.py 1f<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>得到如下结果：</p>
<pre class="line-numbers language-none"><code class="language-none">--------------------------------------------------------------------------------
Running Sanity Check for Question 1f: Padding
--------------------------------------------------------------------------------
Running test on a list of sentences
Sanity Check Passed for Question 1f: Padding!
--------------------------------------------------------------------------------<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="g"><a href="#g" class="headerlink" title="(g)"></a>(g)</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">to_input_tensor_char</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sents<span class="token punctuation">:</span> List<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Convert list of sentences (words) into tensor with necessary padding for 
    shorter sentences.

    @param sents (List[List[str] ]): list of sentences (words)
    @param device: device on which to load the tensor, i.e. CPU or GPU

    @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)
    """</span>
    <span class="token comment">### YOUR CODE HERE for part 1g</span>
    <span class="token comment">### TODO: </span>
    <span class="token comment">###     Connect `words2charindices()` and `pad_sents_char()` which you've defined in </span>
    <span class="token comment">###     previous parts</span>
    charindices <span class="token operator">=</span> self<span class="token punctuation">.</span>words2charindices<span class="token punctuation">(</span>sents<span class="token punctuation">)</span>
    charindices_pad <span class="token operator">=</span> pad_sents_char<span class="token punctuation">(</span>charindices<span class="token punctuation">,</span> self<span class="token punctuation">.</span>char2id<span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    res <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>charindices_pad<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    res <span class="token operator">=</span> res<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> res<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="h"><a href="#h" class="headerlink" title="(h)"></a>(h)</h4><p>公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathrm{x}_{\mathrm{proj}}&=\operatorname{Re} \mathrm{L} \mathrm{U}\left(\mathrm{W}_{\mathrm{proj}} \mathrm{x}_{\text {conv_out }}+\mathrm{b}_{\text {proj }}\right) \\
\mathrm{x}_{\text {gate }}&=\sigma\left(\mathrm{W}_{\text {gate }} \mathrm{X}_{\text {conv_out }}+\mathrm{b}_{\text {gate }}\right)\\
\mathrm{X}_{\mathrm{highway}}&=\mathrm{x}_{\mathrm{gate}} \odot \mathrm{x}_{\mathrm{proj}}+\left(1-\mathrm{x}_{\mathrm{gate}}\right) \odot \mathrm{x}_{\mathrm{conv}{\text{_out}} } 
\end{aligned}</script><p>所以代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token comment">#sigmoid函数</span>
<span class="token comment">#https://blog.csdn.net/qq_39938666/article/details/88809726</span>

<span class="token keyword">class</span> <span class="token class-name">Highway</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Highway<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d<span class="token punctuation">,</span> d<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gate <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d<span class="token punctuation">,</span> d<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x_proj <span class="token operator">=</span> self<span class="token punctuation">.</span>a1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x_gate <span class="token operator">=</span> self<span class="token punctuation">.</span>a2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gate<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x_highway <span class="token operator">=</span> x_gate <span class="token operator">*</span> x_proj <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> x_gate<span class="token punctuation">)</span> <span class="token operator">*</span> x
        
        <span class="token keyword">return</span> x_highway<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="i"><a href="#i" class="headerlink" title="(i)"></a>(i)</h4><p>一维卷积：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#conv1d">https://pytorch.org/docs/stable/nn.html#conv1d</a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token keyword">class</span> <span class="token class-name">CNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> din<span class="token punctuation">,</span> dout<span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>CNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>din<span class="token punctuation">,</span> dout<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x_conv <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x_conv_temp <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv<span class="token punctuation">)</span>
        x_conv_out <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool1d<span class="token punctuation">(</span>x_conv_temp<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x_conv_temp<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> x_conv_out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="j"><a href="#j" class="headerlink" title="(j)"></a>(j)</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ModelEmbeddings</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span> 
    <span class="token triple-quoted-string string">"""
    Class that converts input words to their CNN-based embeddings.
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> vocab<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Init the Embedding layer for one language
        @param embed_size (int): Embedding size (dimensionality) for the output 
        @param vocab (VocabEntry): VocabEntry object. See vocab.py for documentation.
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ModelEmbeddings<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment">## A4 code</span>
        <span class="token comment"># pad_token_idx = vocab.src['&lt;pad>']</span>
        <span class="token comment"># self.embeddings = nn.Embedding(len(vocab.src), embed_size, padding_idx=pad_token_idx)</span>
        pad_token_idx <span class="token operator">=</span> vocab<span class="token punctuation">.</span>char2id<span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>embeddings <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">.</span>char2id<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> padding_idx<span class="token operator">=</span>pad_token_idx<span class="token punctuation">)</span>
        <span class="token comment">## End A4 code</span>

        <span class="token comment">### YOUR CODE HERE for part 1j</span>
        self<span class="token punctuation">.</span>embed_size <span class="token operator">=</span> embed_size
        self<span class="token punctuation">.</span>p <span class="token operator">=</span> <span class="token number">0.3</span>
        self<span class="token punctuation">.</span>m_word <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">.</span>word2id<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>e_char <span class="token operator">=</span> embed_size
        self<span class="token punctuation">.</span>e_char <span class="token operator">=</span> <span class="token number">50</span>
        self<span class="token punctuation">.</span>e_word <span class="token operator">=</span> embed_size
        self<span class="token punctuation">.</span>cnn <span class="token operator">=</span> CNN<span class="token punctuation">(</span>self<span class="token punctuation">.</span>e_char<span class="token punctuation">,</span> self<span class="token punctuation">.</span>e_word<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>highway <span class="token operator">=</span> Highway<span class="token punctuation">(</span>self<span class="token punctuation">.</span>e_word<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p<span class="token punctuation">)</span>


        <span class="token comment">### END YOUR CODE</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Looks up character-based CNN embeddings for the words in a batch of sentences.
        @param input: Tensor of integers of shape (sentence_length, batch_size, max_word_length) where
            each integer is an index into the character vocabulary

        @param output: Tensor of shape (sentence_length, batch_size, embed_size), containing the 
            CNN-based embeddings for each word of the sentences in the batch
        """</span>
        <span class="token comment">## A4 code</span>
        <span class="token comment"># output = self.embeddings(input)</span>
        <span class="token comment"># return output</span>
        
        x_emb <span class="token operator">=</span> self<span class="token punctuation">.</span>embeddings<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
        sentence_length<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> max_word_length<span class="token punctuation">,</span> emb_size <span class="token operator">=</span> x_emb<span class="token punctuation">.</span>shape
        x_emb <span class="token operator">=</span> x_emb<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        x_emb <span class="token operator">=</span> x_emb<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> emb_size<span class="token punctuation">,</span> max_word_length<span class="token punctuation">)</span>

        <span class="token comment">### YOUR CODE HERE for part 1j</span>
        x_conv <span class="token operator">=</span> self<span class="token punctuation">.</span>cnn<span class="token punctuation">(</span>x_emb<span class="token punctuation">)</span>
        x_high <span class="token operator">=</span> self<span class="token punctuation">.</span>highway<span class="token punctuation">(</span>x_conv<span class="token punctuation">)</span>
        x_word_emb <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x_high<span class="token punctuation">)</span>
        
        x_word_emb <span class="token operator">=</span> x_word_emb<span class="token punctuation">.</span>view<span class="token punctuation">(</span>sentence_length<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> x_word_emb
        <span class="token comment">### END YOUR CODE</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用如下代码测试：</p>
<pre class="line-numbers language-none"><code class="language-none">python sanity_check.py 1j<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>得到如下结果：</p>
<pre class="line-numbers language-none"><code class="language-none">--------------------------------------------------------------------------------
Running Sanity Check for Question 1j: Model Embedding
--------------------------------------------------------------------------------
Sanity Check Passed for Question 1j: Model Embedding!
--------------------------------------------------------------------------------<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="k"><a href="#k" class="headerlink" title="(k)"></a>(k)</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> source<span class="token punctuation">:</span> List<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token punctuation">]</span><span class="token punctuation">,</span> target<span class="token punctuation">:</span> List<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Take a mini-batch of source and target sentences, compute the log-likelihood of
    target sentences under the language models learned by the NMT system.

    @param source (List[List[str] ]): list of source sentence tokens
    @param target (List[List[str] ]): list of target sentence tokens, wrapped by `&lt;s>` and `&lt;/s>`

    @returns scores (Tensor): a variable/tensor of shape (b, ) representing the
                                log-likelihood of generating the gold-standard target sentence for
                                each example in the input batch. Here b = batch size.
    """</span>
    <span class="token comment"># Compute sentence lengths</span>
    source_lengths <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span> <span class="token keyword">for</span> s <span class="token keyword">in</span> source<span class="token punctuation">]</span>

    <span class="token comment"># Convert list of lists into tensors</span>

    <span class="token comment">## A4 code</span>
    <span class="token comment"># source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)</span>
    <span class="token comment"># target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)</span>

    <span class="token comment"># enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)</span>
    <span class="token comment"># enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)</span>
    <span class="token comment"># combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)</span>
    <span class="token comment">## End A4 code</span>

    <span class="token comment">### YOUR CODE HERE for part 1k</span>
    <span class="token comment">### TODO:</span>
    <span class="token comment">###     Modify the code lines above as needed to fetch the character-level tensor</span>
    <span class="token comment">###     to feed into encode() and decode(). You should:</span>
    <span class="token comment">###     - Keep `target_padded` from A4 code above for predictions</span>
    <span class="token comment">###     - Add `source_padded_chars` for character level padded encodings for source</span>
    <span class="token comment">###     - Add `target_padded_chars` for character level padded encodings for target</span>
    <span class="token comment">###     - Modify calls to encode() and decode() to use the character level encodings</span>
    target_padded <span class="token operator">=</span> self<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>tgt<span class="token punctuation">.</span>to_input_tensor<span class="token punctuation">(</span>target<span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    source_padded_chars <span class="token operator">=</span> self<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>src<span class="token punctuation">.</span>to_input_tensor_char<span class="token punctuation">(</span>source<span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    target_padded_chars <span class="token operator">=</span> self<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>tgt<span class="token punctuation">.</span>to_input_tensor_char<span class="token punctuation">(</span>target<span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    enc_hiddens<span class="token punctuation">,</span> dec_init_state <span class="token operator">=</span> self<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>source_padded_chars<span class="token punctuation">,</span> source_lengths<span class="token punctuation">)</span>
    enc_masks <span class="token operator">=</span> self<span class="token punctuation">.</span>generate_sent_masks<span class="token punctuation">(</span>enc_hiddens<span class="token punctuation">,</span> source_lengths<span class="token punctuation">)</span>
    combined_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>enc_hiddens<span class="token punctuation">,</span> enc_masks<span class="token punctuation">,</span> dec_init_state<span class="token punctuation">,</span> target_padded_chars<span class="token punctuation">)</span>
    <span class="token comment">### END YOUR CODE</span>

    P <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>target_vocab_projection<span class="token punctuation">(</span>combined_outputs<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token comment"># Zero out, probabilities for which we have nothing in the target text</span>
    target_masks <span class="token operator">=</span> <span class="token punctuation">(</span>target_padded <span class="token operator">!=</span> self<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>tgt<span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># Compute log probability of generating true target words</span>
    target_gold_words_log_prob <span class="token operator">=</span> torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>P<span class="token punctuation">,</span> index<span class="token operator">=</span>target_padded<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> target_masks<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
    scores <span class="token operator">=</span> target_gold_words_log_prob<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># mhahn2 Small modification from A4 code.</span>



    <span class="token keyword">if</span> self<span class="token punctuation">.</span>charDecoder <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        max_word_len <span class="token operator">=</span> target_padded_chars<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

        target_words <span class="token operator">=</span> target_padded<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment">#print(max_word_len, target_padded_chars[1:].shape)</span>
        <span class="token comment">#target_chars = target_padded_chars[1:].view(-1, max_word_len)</span>
        target_chars <span class="token operator">=</span> target_padded_chars<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> max_word_len<span class="token punctuation">)</span>
        target_outputs <span class="token operator">=</span> combined_outputs<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>

        target_chars_oov <span class="token operator">=</span> target_chars <span class="token comment">#torch.index_select(target_chars, dim=0, index=oovIndices)</span>
        rnn_states_oov <span class="token operator">=</span> target_outputs <span class="token comment">#torch.index_select(target_outputs, dim=0, index=oovIndices)</span>
        oovs_losses <span class="token operator">=</span> self<span class="token punctuation">.</span>charDecoder<span class="token punctuation">.</span>train_forward<span class="token punctuation">(</span>target_chars_oov<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>rnn_states_oov<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> rnn_states_oov<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        scores <span class="token operator">=</span> scores <span class="token operator">-</span> oovs_losses

    <span class="token keyword">return</span> scores<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="l"><a href="#l" class="headerlink" title="(l)"></a>(l)</h4><p>使用如下命令训练测试即可：</p>
<pre class="line-numbers language-none"><code class="language-none">python run.py train --train-src&#x3D;.&#x2F;en_es_data&#x2F;train_tiny.es --train-tgt&#x3D;.&#x2F;en_es_data&#x2F;train_tiny.en --dev-src&#x3D;.&#x2F;en_es_data&#x2F;dev_tiny.es --dev-tgt&#x3D;.&#x2F;en_es_data&#x2F;dev_tiny.en --vocab&#x3D;vocab_tiny_q1.json --batch-size&#x3D;2 --valid-niter&#x3D;100 --max-epoch&#x3D;101 --no-char-decoder

python run.py decode model.bin .&#x2F;en_es_data&#x2F;test_tiny.es .&#x2F;en_es_data&#x2F;test_tiny.en outputs&#x2F;test_outputs_local_q1.txt --no-char-decoder<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="2-Character-based-LSTM-decoder-for-NMT"><a href="#2-Character-based-LSTM-decoder-for-NMT" class="headerlink" title="2. Character-based LSTM decoder for NMT"></a>2. Character-based LSTM decoder for NMT</h3><h4 id="a-1"><a href="#a-1" class="headerlink" title="(a)"></a>(a)</h4><p>根据公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{h}_{t}, \mathbf{c}_{t}&=\text { CharDecoderLSTM }\left(\mathbf{x}_{t}, \mathbf{h}_{t-1}, \mathbf{c}_{t-1}\right)\\
\mathbf{s}_{t}&=\mathbf{W}_{\mathrm{dec}} \mathbf{h}_{t}+\mathbf{b}_{\mathrm{dec}}\\

\mathbf{p}_{t} &=\operatorname{softmax}\left(\mathbf{s}_{t}\right) \\
\text { loss char_dec } &=-\sum_{t=1}^{n} \log \mathbf{p}_{t}\left(x_{t+1}\right) 
\end{aligned}</script><p>得到如下代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> char_embedding_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> target_vocab<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Init Character Decoder.

    @param hidden_size (int): Hidden size of the decoder LSTM
    @param char_embedding_size (int): dimensionality of character embeddings
    @param target_vocab (VocabEntry): vocabulary for the target language. See vocab.py for documentation.
    """</span>
    <span class="token comment">### YOUR CODE HERE for part 2a</span>
    <span class="token comment">### TODO - Initialize as an nn.Module.</span>
    <span class="token comment">###      - Initialize the following variables:</span>
    <span class="token comment">###        self.charDecoder: LSTM. Please use nn.LSTM() to construct this.</span>
    <span class="token comment">###        self.char_output_projection: Linear layer, called W_&#123;dec&#125; and b_&#123;dec&#125; in the PDF</span>
    <span class="token comment">###        self.decoderCharEmb: Embedding matrix of character embeddings</span>
    <span class="token comment">###        self.target_vocab: vocabulary for the target language</span>
    <span class="token comment">###</span>
    <span class="token comment">### Hint: - Use target_vocab.char2id to access the character vocabulary for the target language.</span>
    <span class="token comment">###       - Set the padding_idx argument of the embedding matrix.</span>
    <span class="token comment">###       - Create a new Embedding layer. Do not reuse embeddings created in Part 1 of this assignment.</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>CharDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>charDecoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>char_embedding_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
    V_char <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>target_vocab<span class="token punctuation">.</span>char2id<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>pad <span class="token operator">=</span> target_vocab<span class="token punctuation">.</span>char2id<span class="token punctuation">[</span><span class="token string">'&lt;pad>'</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>char_output_projection <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> V_char<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>decoderCharEmb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>V_char<span class="token punctuation">,</span> char_embedding_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>pad<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>target_vocab <span class="token operator">=</span> target_vocab
    <span class="token comment">### END YOUR CODE</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用如下代码测试：</p>
<pre class="line-numbers language-none"><code class="language-none">python sanity_check.py 2a<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>得到如下结果：</p>
<pre class="line-numbers language-none"><code class="language-none">--------------------------------------------------------------------------------
Running Sanity Check for Question 2a: CharDecoder.__init__()
--------------------------------------------------------------------------------
Sanity Check Passed for Question 2a: CharDecoder.__init__()!
--------------------------------------------------------------------------------<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="b-1"><a href="#b-1" class="headerlink" title="(b)"></a>(b)</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> dec_hidden<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Forward pass of character decoder.

    @param input: tensor of integers, shape (length, batch)
    @param dec_hidden: internal state of the LSTM before reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)

    @returns scores: called s_t in the PDF, shape (length, batch, self.vocab_size)
    @returns dec_hidden: internal state of the LSTM after reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)
    """</span>
    <span class="token comment">### YOUR CODE HERE for part 2b</span>
    <span class="token comment">### TODO - Implement the forward pass of the character decoder.</span>
    X <span class="token operator">=</span> self<span class="token punctuation">.</span>decoderCharEmb<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
    enc_hiddens<span class="token punctuation">,</span> dec_hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>charDecoder<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dec_hidden<span class="token punctuation">)</span>
    scores <span class="token operator">=</span> self<span class="token punctuation">.</span>char_output_projection<span class="token punctuation">(</span>enc_hiddens<span class="token punctuation">)</span>
    <span class="token keyword">return</span> scores<span class="token punctuation">,</span> dec_hidden
    <span class="token comment">### END YOUR CODE </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用如下代码测试：</p>
<pre class="line-numbers language-none"><code class="language-none">python sanity_check.py 2b<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>得到如下结果：</p>
<pre class="line-numbers language-none"><code class="language-none">--------------------------------------------------------------------------------
Running Sanity Check for Question 2b: CharDecoder.forward()
--------------------------------------------------------------------------------
Sanity Check Passed for Question 2b: CharDecoder.forward()!
--------------------------------------------------------------------------------<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="c-1"><a href="#c-1" class="headerlink" title="(c)"></a>(c)</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> char_sequence<span class="token punctuation">,</span> dec_hidden<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Forward computation during training.

    @param char_sequence: tensor of integers, shape (length, batch). Note that "length" here and in forward() need not be the same.
    @param dec_hidden: initial internal state of the LSTM, obtained from the output of the word-level decoder. A tuple of two tensors of shape (1, batch, hidden_size)

    @returns The cross-entropy loss, computed as the *sum* of cross-entropy losses of all the words in the batch.
    """</span>
    <span class="token comment">### YOUR CODE HERE for part 2c</span>
    <span class="token comment">### TODO - Implement training forward pass.</span>
    <span class="token comment">###</span>
    <span class="token comment">### Hint: - Make sure padding characters do not contribute to the cross-entropy loss.</span>
    <span class="token comment">###       - char_sequence corresponds to the sequence x_1 ... x_&#123;n+1&#125; from the handout (e.g., &lt;START>,m,u,s,i,c,&lt;END>).</span>
    scores<span class="token punctuation">,</span> dec_hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>char_sequence<span class="token punctuation">,</span> dec_hidden<span class="token punctuation">)</span>
    scores <span class="token operator">=</span> scores<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
    target <span class="token operator">=</span> char_sequence<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
    loss <span class="token operator">=</span> <span class="token number">0</span>
    batch_size <span class="token operator">=</span> char_sequence<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    loss_func <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>ignore_index<span class="token operator">=</span>self<span class="token punctuation">.</span>pad<span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss <span class="token operator">+=</span> loss_func<span class="token punctuation">(</span>scores<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">,</span> target<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> loss
    <span class="token comment">### END YOUR CODE</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用如下代码测试：</p>
<pre class="line-numbers language-none"><code class="language-none">python sanity_check.py 2c<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>得到如下结果：</p>
<pre class="line-numbers language-none"><code class="language-none">--------------------------------------------------------------------------------
Running Sanity Check for Question 2c: CharDecoder.train_forward()
--------------------------------------------------------------------------------
Sanity Check Passed for Question 2c: CharDecoder.train_forward()!
--------------------------------------------------------------------------------<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="d-1"><a href="#d-1" class="headerlink" title="(d)"></a>(d)</h4><p>按照伪代码实现即可：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">decode_greedy</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> initialStates<span class="token punctuation">,</span> device<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">21</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Greedy decoding
    @param initialStates: initial internal state of the LSTM, a tuple of two tensors of size (1, batch, hidden_size)
    @param device: torch.device (indicates whether the model is on CPU or GPU)
    @param max_length: maximum length of words to decode

    @returns decodedWords: a list (of length batch) of strings, each of which has length &lt;= max_length.
                          The decoded strings should NOT contain the start-of-word and end-of-word characters.
    """</span>

    <span class="token comment">### YOUR CODE HERE for part 2d</span>
    <span class="token comment">### TODO - Implement greedy decoding.</span>
    <span class="token comment">### Hints:</span>
    <span class="token comment">###      - Use target_vocab.char2id and target_vocab.id2char to convert between integers and characters</span>
    <span class="token comment">###      - Use torch.tensor(..., device=device) to turn a list of character indices into a tensor.</span>
    <span class="token comment">###      - We use curly brackets as start-of-word and end-of-word characters. That is, use the character '&#123;' for &lt;START> and '&#125;' for &lt;END>.</span>
    <span class="token comment">###        Their indices are self.target_vocab.start_of_word and self.target_vocab.end_of_word, respectively.</span>
    batch_size <span class="token operator">=</span> initialStates<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    start <span class="token operator">=</span> self<span class="token punctuation">.</span>target_vocab<span class="token punctuation">.</span>start_of_word
    end <span class="token operator">=</span> self<span class="token punctuation">.</span>target_vocab<span class="token punctuation">.</span>end_of_word
    current_char <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token punctuation">[</span>start<span class="token punctuation">]</span> <span class="token operator">*</span> batch_size<span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
    dec_hidden <span class="token operator">=</span> initialStates
    output <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    char_index <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_length<span class="token punctuation">)</span><span class="token punctuation">:</span>
        scores<span class="token punctuation">,</span> dec_hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>current_char<span class="token punctuation">,</span> dec_hidden<span class="token punctuation">)</span>
        current_char <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        char_index<span class="token punctuation">.</span>append<span class="token punctuation">(</span>current_char<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    char_index <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>char_index<span class="token punctuation">)</span>
    <span class="token comment">#截断</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        index <span class="token operator">=</span> char_index<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span>
        tmp <span class="token operator">=</span> <span class="token string">""</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> index<span class="token punctuation">:</span>
            char <span class="token operator">=</span> self<span class="token punctuation">.</span>target_vocab<span class="token punctuation">.</span>id2char<span class="token punctuation">[</span>j<span class="token punctuation">]</span>
            <span class="token keyword">if</span> j <span class="token operator">!=</span> end<span class="token punctuation">:</span>
                tmp <span class="token operator">+=</span> char
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token keyword">break</span>
        output<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tmp<span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> output
    
    <span class="token comment">### END YOUR CODE</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>使用如下代码测试：</p>
<pre class="line-numbers language-none"><code class="language-none">python sanity_check.py 2d<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>得到如下结果：</p>
<pre class="line-numbers language-none"><code class="language-none">--------------------------------------------------------------------------------
Running Sanity Check for Question 2d: CharDecoder.decode_greedy()
--------------------------------------------------------------------------------
Sanity Check Passed for Question 2d: CharDecoder.decode_greedy()!
--------------------------------------------------------------------------------<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="e-1"><a href="#e-1" class="headerlink" title="(e)"></a>(e)</h4><p>使用如下命令训练和测试：</p>
<pre class="line-numbers language-none"><code class="language-none">python run.py train --train-src&#x3D;.&#x2F;en_es_data&#x2F;train_tiny.es --train-tgt&#x3D;.&#x2F;en_es_data&#x2F;train_tiny.en --dev-src&#x3D;.&#x2F;en_es_data&#x2F;dev_tiny.es --dev-tgt&#x3D;.&#x2F;en_es_data&#x2F;dev_tiny.en --vocab&#x3D;vocab_tiny_q2.json --batch-size&#x3D;2 --max-epoch&#x3D;201 --valid-niter&#x3D;100
python run.py decode model.bin .&#x2F;en_es_data&#x2F;test_tiny.es .&#x2F;en_es_data&#x2F;test_tiny.en outputs&#x2F;test_outputs_local_q2.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h4 id="f-1"><a href="#f-1" class="headerlink" title="(f)"></a>(f)</h4><p>使用如下命令训练和测试：</p>
<pre class="line-numbers language-none"><code class="language-none">python run.py train --train-src&#x3D;.&#x2F;en_es_data&#x2F;train.es --train-tgt&#x3D;.&#x2F;en_es_data&#x2F;train.en --dev-src&#x3D;.&#x2F;en_es_data&#x2F;dev.es --dev-tgt&#x3D;.&#x2F;en_es_data&#x2F;dev.en --vocab&#x3D;vocab.json --cuda
python run.py decode model.bin .&#x2F;en_es_data&#x2F;test.es .&#x2F;en_es_data&#x2F;test.en outputs&#x2F;test_outputs.txt --cuda<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>得到如下结果：</p>
<pre class="line-numbers language-none"><code class="language-none">Corpus BLEU: 22.732161209949027<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h3 id="3-Analyzing-NMT-Systems"><a href="#3-Analyzing-NMT-Systems" class="headerlink" title="3. Analyzing NMT Systems"></a>3. Analyzing NMT Systems</h3><p>从略。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Doraemonzzz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://doraemonzzz.com/2020/06/15/CS224N%20Natural%20Language%20Processing%20with%20Deep%20Learning%20Assignment%205/">http://doraemonzzz.com/2020/06/15/CS224N%20Natural%20Language%20Processing%20with%20Deep%20Learning%20Assignment%205/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://doraemonzzz.com" target="_blank">Doraemonzzz</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CS224N/">CS224N</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/06/16/CMU%2015-213%20Intro%20to%20Computer%20Systems%20Lecture%2013/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">CMU 15-213 Intro to Computer Systems Lecture 13</div></div></a></div><div class="next-post pull-right"><a href="/2020/06/15/CS224N%20Natural%20Language%20Processing%20with%20Deep%20Learning%20Assignment%204/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CS224N Natural Language Processing with Deep Learning Assignment 4</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/03/17/CS224N Natural Language Processing with Deep Learning Assignment 2/" title="CS224N Natural Language Processing with Deep Learning Assignment 2"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-03-17</div><div class="title">CS224N Natural Language Processing with Deep Learning Assignment 2</div></div></a></div><div><a href="/2020/03/29/CS224N Natural Language Processing with Deep Learning Assignment 3/" title="CS224N Natural Language Processing with Deep Learning Assignment 3"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-03-29</div><div class="title">CS224N Natural Language Processing with Deep Learning Assignment 3</div></div></a></div><div><a href="/2020/01/14/CS224N Natural Language Processing with Deep Learning Lecture 1/" title="CS224N Natural Language Processing with Deep Learning Lecture 1"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-14</div><div class="title">CS224N Natural Language Processing with Deep Learning Lecture 1</div></div></a></div><div><a href="/2020/04/21/CS224N Natural Language Processing with Deep Learning Lecture 10/" title="CS224N Natural Language Processing with Deep Learning Lecture 10"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-04-21</div><div class="title">CS224N Natural Language Processing with Deep Learning Lecture 10</div></div></a></div><div><a href="/2020/04/22/CS224N Natural Language Processing with Deep Learning Lecture 11/" title="CS224N Natural Language Processing with Deep Learning Lecture 11"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-04-22</div><div class="title">CS224N Natural Language Processing with Deep Learning Lecture 11</div></div></a></div><div><a href="/2020/04/27/CS224N Natural Language Processing with Deep Learning Lecture 12/" title="CS224N Natural Language Processing with Deep Learning Lecture 12"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-04-27</div><div class="title">CS224N Natural Language Processing with Deep Learning Lecture 12</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="https://github.com/Doraemonzzz/md-photo/blob/master/%E5%A4%B4%E5%83%8F.jpg?raw=true" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Doraemonzzz</div><div class="author-info__description">个人博客，主要记录有关机器学习，数学以及计算机科学的笔记</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">611</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">60</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Doraemonzzz"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Doraemonzzz" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/doraemon_zzz@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://space.bilibili.com/291079982" target="_blank" title="Bilibili"><i class="iconfont icon-bilibili"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title=""><i class="fa fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">暂无公告</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Character-based-convolutional-encoder-for-NMT"><span class="toc-number">1.</span> <span class="toc-text">1. Character-based convolutional encoder for NMT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#a"><span class="toc-number">1.1.</span> <span class="toc-text">(a)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#b"><span class="toc-number">1.2.</span> <span class="toc-text">(b)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#c"><span class="toc-number">1.3.</span> <span class="toc-text">(c)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#d"><span class="toc-number">1.4.</span> <span class="toc-text">(d)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#e"><span class="toc-number">1.5.</span> <span class="toc-text">(e)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#f"><span class="toc-number">1.6.</span> <span class="toc-text">(f)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#g"><span class="toc-number">1.7.</span> <span class="toc-text">(g)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#h"><span class="toc-number">1.8.</span> <span class="toc-text">(h)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#i"><span class="toc-number">1.9.</span> <span class="toc-text">(i)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#j"><span class="toc-number">1.10.</span> <span class="toc-text">(j)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#k"><span class="toc-number">1.11.</span> <span class="toc-text">(k)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#l"><span class="toc-number">1.12.</span> <span class="toc-text">(l)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Character-based-LSTM-decoder-for-NMT"><span class="toc-number">2.</span> <span class="toc-text">2. Character-based LSTM decoder for NMT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#a-1"><span class="toc-number">2.1.</span> <span class="toc-text">(a)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#b-1"><span class="toc-number">2.2.</span> <span class="toc-text">(b)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#c-1"><span class="toc-number">2.3.</span> <span class="toc-text">(c)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#d-1"><span class="toc-number">2.4.</span> <span class="toc-text">(d)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#e-1"><span class="toc-number">2.5.</span> <span class="toc-text">(e)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#f-1"><span class="toc-number">2.6.</span> <span class="toc-text">(f)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Analyzing-NMT-Systems"><span class="toc-number">3.</span> <span class="toc-text">3. Analyzing NMT Systems</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/06/09/2021-6-9-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95%EF%BC%88%E7%AC%AC%E4%B8%83%E7%89%88%EF%BC%89Wireshark-Lab1-Getting-Started/" title="计算机网络——自顶向下方法（第七版）Wireshark Lab1:Getting Started"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机网络——自顶向下方法（第七版）Wireshark Lab1:Getting Started"/></a><div class="content"><a class="title" href="/2021/06/09/2021-6-9-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95%EF%BC%88%E7%AC%AC%E4%B8%83%E7%89%88%EF%BC%89Wireshark-Lab1-Getting-Started/" title="计算机网络——自顶向下方法（第七版）Wireshark Lab1:Getting Started">计算机网络——自顶向下方法（第七版）Wireshark Lab1:Getting Started</a><time datetime="2021-06-09T15:40:00.000Z" title="发表于 2021-06-09 23:40:00">2021-06-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/06/2021-6-6-%E7%AE%97%E6%B3%95%E6%A6%82%E8%AE%BA(DPV)%E4%B9%A0%E9%A2%98%E8%A7%A3%E7%AD%94%E2%80%94%E2%80%94%E7%AC%AC3%E7%AB%A0-%E5%9B%BE%E7%9A%84%E5%88%86%E8%A7%A3/" title="算法概论(DPV)习题解答——第3章 图的分解"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="算法概论(DPV)习题解答——第3章 图的分解"/></a><div class="content"><a class="title" href="/2021/06/06/2021-6-6-%E7%AE%97%E6%B3%95%E6%A6%82%E8%AE%BA(DPV)%E4%B9%A0%E9%A2%98%E8%A7%A3%E7%AD%94%E2%80%94%E2%80%94%E7%AC%AC3%E7%AB%A0-%E5%9B%BE%E7%9A%84%E5%88%86%E8%A7%A3/" title="算法概论(DPV)习题解答——第3章 图的分解">算法概论(DPV)习题解答——第3章 图的分解</a><time datetime="2021-06-06T14:35:00.000Z" title="发表于 2021-06-06 22:35:00">2021-06-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/06/2021-6-6-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F-%E7%AC%AC5%E7%AB%A0-%E4%B9%A0%E9%A2%98%E8%A7%A3%E6%9E%90/" title="深入理解计算机系统 第5章 习题解析"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深入理解计算机系统 第5章 习题解析"/></a><div class="content"><a class="title" href="/2021/06/06/2021-6-6-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F-%E7%AC%AC5%E7%AB%A0-%E4%B9%A0%E9%A2%98%E8%A7%A3%E6%9E%90/" title="深入理解计算机系统 第5章 习题解析">深入理解计算机系统 第5章 习题解析</a><time datetime="2021-06-06T07:23:00.000Z" title="发表于 2021-06-06 15:23:00">2021-06-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/05/2021-6-5-Stanford-Compiler-PA5%E7%BF%BB%E8%AF%91/" title="Stanford Compiler PA5翻译"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Stanford Compiler PA5翻译"/></a><div class="content"><a class="title" href="/2021/06/05/2021-6-5-Stanford-Compiler-PA5%E7%BF%BB%E8%AF%91/" title="Stanford Compiler PA5翻译">Stanford Compiler PA5翻译</a><time datetime="2021-06-05T06:12:00.000Z" title="发表于 2021-06-05 14:12:00">2021-06-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/04/2021-6-4-Stanford-Compiler-PA4%E7%BF%BB%E8%AF%91/" title="Stanford Compiler PA4翻译"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Stanford Compiler PA4翻译"/></a><div class="content"><a class="title" href="/2021/06/04/2021-6-4-Stanford-Compiler-PA4%E7%BF%BB%E8%AF%91/" title="Stanford Compiler PA4翻译">Stanford Compiler PA4翻译</a><time datetime="2021-06-03T16:17:00.000Z" title="发表于 2021-06-04 00:17:00">2021-06-04</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2021 By Doraemonzzz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my blog!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.25
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'IpnmxCW9CvYWIXbol5QXsegX-MdYXbMMI',
      appKey: 'w57DVCdbxcyB1TYYagMIMJIU',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>